{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e70e274",
   "metadata": {},
   "source": [
    "<h1>Bank Marketing Classifier</h1>\n",
    "\n",
    "<h3>Prevendo se o cliente vai adquirir um empréstimo com o uso de PySpark</h3>\n",
    "\n",
    "---\n",
    "\n",
    "Prever se o cliente vai adquirir um determinado serviço é um grande desafio para qualquer empresa. No ramo das instituições financeiras, um desses serviços é o empréstimo à prazo. No qual, o banco empresta dinheiro para o seu cliente e vai recebendo uma parcela desse empréstimo com acréscimo de juros todos os meses. \n",
    "\n",
    "Contudo, a taxa de conversão de clientes com o uso de canais telemarketing é extremamente baixa, tornando o processo operacional custoso e improdutivo para o banco. \n",
    "\n",
    "A aplicação de modelos preditivos permite identificar quais são os clientes mais propensos a adquirir um empréstimo. E dessa forma, é possível otimizar esse serviço para que os canais de telemarketing direcionem os esforços para atingir os clientes com maior propensão de adquirir o empréstimo.\n",
    "\n",
    "Uma forma de construir modelos preditivos é através de programação utilizando a combinação de Python com o framework Spark, denominado PySpark. O uso dessa ferramenta é bastante relevante, dado que o volume de dados em instituições financeiras é bastante alto (Big Data) e o PySpark suporta muito bem a escalabilidade desses dados.\n",
    "\n",
    "Com a utilização do PySpark, este projeto se propõe ao desenvolvimento de uma modelo de Machine Learning que com base nos dados históricos, seja capaz de predizer se o cliente vai ou não adquirir o empréstimo no banco. O projeto percorre por todas as etapas de uma modelagem e no final será desenvolvido um modelo pronto para ser exportando para outros ambientes, como o ambiente em nuvem da Amazon, por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2062442e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2>SUMÁRIO</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc545c",
   "metadata": {},
   "source": [
    "* <a href=\"#id_1\">1) INTRODUÇÃO AO TEMA</a>\n",
    "    * <a href=\"#id_1.1\">1.1) Bank Marketing</a>\n",
    "    * <a href=\"#id_1.2\">1.2) Machine Learning</a>\n",
    "    * <a href=\"#id_1.3\">1.3) Machine Learning e o Bank Marketing</a>\n",
    "    * <a href=\"#id_1.4\">1.4) Objetivos do Projeto</a>\n",
    "    * <a href=\"#id_1.5\">1.5) Sobre o Dataset</a>\n",
    "    * <a href=\"#id_1.6\">1.6) Introdução ao Apache Spark</a>\n",
    "    * <a href=\"#id_1.7\">1.7) PySpark</a>\n",
    "<br/><br/>\n",
    "\n",
    "* <a href=\"#id_2\">2) INÍCIO DO PROJETO</a>\n",
    "    * <a href=\"#id_2.1\">2.1) Carregamento dos Dados e Importações</a>\n",
    "    * <a href=\"#id_2.2\">2.2) Visão Geral</a>\n",
    "    * <a href=\"#id_2.3\">2.3) Tratamento de Valores Ausentes</a>\n",
    "<br/><br/>\n",
    "\n",
    "* <a href=\"#id_3\">3) PRÉ PROCESSAMENTO DOS DADOS</a>\n",
    "    * <a href=\"#id_3.1\">3.1) Encoding dos Dados</a>\n",
    "        * <a href=\"#id_3.2\">3.1.1) Label Encoder</a>\n",
    "        * <a href=\"#id_3.3\">3.1.2) One Hot Encoder</a>\n",
    "    * <a href=\"#id_3.4\">3.2) Alterando o tipo de Dado</a>\n",
    "    * <a href=\"#id_3.5\">3.3) Conversão para Vetor Denso</a>\n",
    "    * <a href=\"#id_3.5\">3.4) Escala dos Dados</a>\n",
    "    * <a href=\"#id_3.6\">3.5) Divisão em Treino, Teste e Validação</a>\n",
    "    * <a href=\"#id_3.7\">3.6) Balanceamento de classes</a>\n",
    "    * <a href=\"#id_3.8\">3.7) Feature Selection</a>\n",
    "<br/><br/>\n",
    "\n",
    "* <a href=\"#id_4\">4) MODELAGEM</a>\n",
    "    * <a href=\"#id_4.1\">4.1) Modelagem Base</a>\n",
    "    * <a href=\"#id_4.2\">4.2) Aplicando GridSearch e Cross Validation</a>\n",
    "<br/><br/>\n",
    "\n",
    "* <a href=\"#id_5\">5) AVALIAÇÃO</a>\n",
    "    * <a href=\"#id_5.1\">5.1) Avaliação do Modelo</a>\n",
    "    * <a href=\"#id_5.2\">5.2) Salvando o modelo</a>\n",
    "    * <a href=\"#id_5.3\">5.3) Carregando o modelo</a>\n",
    "<br/><br/>\n",
    "\n",
    "* <a href=\"#id_6\">6) FINAL</a>\n",
    "    * <a href=\"#id_6.1\">6.1) Considerações Finais</a>\n",
    "    * <a href=\"#id_6.2\">6.2) Agradecimentos</a>\n",
    "    * <a href=\"#id_6.3\">6.3) Referências</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa6e2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 id=\"id_1\">1) INTRODUÇÃO AO TEMA</h2>\n",
    "\n",
    "O marketing está presente nas nossas vidas muito mais do que imaginamos. Faça uma caminhada pelas ruas da cidade, uma busca no seu navegador, ligue a televisão ou o rádio, abra sua rede social e você será impactado por alguma ação de marketing.\n",
    "\n",
    "Mas o que é marketing?\n",
    "Para Philip Kotler, um dos teóricos mais renomados da área, define marketing como:\n",
    "> *Marketing é a ciência e arte de explorar, criar e proporcionar valor para satisfazer necessidades de um público-alvo com rendibilidade.*\n",
    "\n",
    "O campo do marketing é vasto e inclui não apenas o ato de vender um produto ou serviço, mas tudo relacionado ao planejamento, pesquisa e posicionamento de mercado. Em outras palavras, pode-se dizer que o marketing é como uma balança entre o que os clientes desejam e os objetivos da empresa. Afinal, um bom marketing precisa criar valor para ambas as partes: para a empresa e para o consumidor.\n",
    "\n",
    "Vale ressaltar que marketing é uma palavra em inglês, derivada de market (mercado). Portanto, marketing não é apenas vender produtos ou serviços, engloba também outras atividades relacionadas ao mercado.\n",
    "\n",
    "---\n",
    "\n",
    "<h3 id=\"id_1.1\">1.1) Bank Marketing</h3>\n",
    "\n",
    "Um tipo de instituição que aplica marketing no seu dia a dia são os bancos, para isso, damos o nome Bank Marketing (Marketing Bancário). O marketing bancário é a prática de atrair e adquirir novos clientes por meio de estratégias de mídia tradicional e mídia digital. O uso dessas estratégias de mídia ajuda a determinar que tipo de cliente é atraído por uma determinada instituição. Isso também inclui diferentes instituições bancárias que usam propositalmente diferentes estratégias para atrair o tipo de cliente com o qual desejam fazer negócios.\n",
    "\n",
    "E você sabe como as principais empresas fazem atualmente para aplicar estratégias de Marketing?<br/>\n",
    "Se você respondeu, \"elas aplicam técnicas de Inteligência Artificial e Machine Learning para entender e avaliar o comportamento dos seus clientes\". Parabéns, você acertou!\n",
    "Mas antes de explicar com elas fazem isso, vamos começar entendendo um pouco sobre o que é Machine Learning.\n",
    "\n",
    "---\n",
    "\n",
    "<h3 id=\"id_1.2\">1.2) Machine Learning</h3>\n",
    "\n",
    "O Machine Learning, ou aprendizado de máquina. É um subcampo da inteligência artificial que permite dar aos computadores a habilidade de aprender sem que sejam explicitamente programados para isso. Ela permite que computadores tomem decisões e interpretem dados de maneira automática, a partir de algoritmos. Temos vários tipos de aprendizagem, são elas: Supervisionada, não supervisionada, semi supervisionada, aprendizagem por reforço e deep learning.\n",
    "\t\n",
    "Os algoritmos de aprendizagem de máquina, aprendem a induzir uma função ou hipótese capaz de resolver um problema a partir de dados que representam instâncias do problema a ser resolvido.\n",
    "\n",
    "Um algoritmo é uma sequência finita de ações e regras que visam a solucionar um problema. Cada um deles aciona um diferente tipo de operação ao entrar em contato com os dados que o computador recebe. O resultado de todas as operações é o que possibilita o aprendizado da máquina.\n",
    "\n",
    "Dessa forma, as máquinas aperfeiçoam as tarefas executadas, por meio de processamento de dados como imagens e números. Por isso o machine learning depende do Big Data para ser efetivo. O Big Data, por sua vez pode ser entendido de maneira simplória como uma imensa quantidade de dados. Mas calma, ainda irei falar mais em detalhes sobre isso. Por ora, vamos entender como o Machine Learning e a Inteligência Artificial ajudam a benefeciar a área de Bank Marketing.\n",
    "\n",
    "---\n",
    "\n",
    "<h3 id=\"id_1.3\">1.3) Machine Learning e o Bank Marketing</h3>\n",
    "\n",
    "Abaixo irei listar alguns benefícios do Machine Learning (ML) aplicado na área de Bank Marketing:\n",
    "* **Atendimento ao cliente orientado por IA**: Existem muitas maneiras de tornar o atendimento ao cliente realmente orientado por IA ou, melhor dizer, orientado por dados. Por exemplo, com a ajuda da análise de dados, a instituição bancária pode descobrir as intenções de compra do cliente e oferecer um empréstimo flexível. Além disso, os principais bancos criam chatbots inteligentes que ajudam os clientes a interagir melhor com as empresas financeiras. Com a ajuda de aplicativos inteligentes, os clientes podem acompanhar automaticamente seus gastos, planejar seu orçamento e obter sugestões precisas de economia e investimento.\n",
    "* **Segmentação de clientes**: Com ML, é possível encontrar características semelhantes e padrões entre os dados dos clientes. Dessa forma, o algoritmo de ML consegue separar os clientes em grupos, possibilitando que a equipe de Marketing possa direcionar os esforços de maneira individual para cada grupo de clientes.\n",
    "* **Otimização de lances em anúncios**: os anúncios em buscadores funcionam no sistema de leilões de pesquisa. Ou seja, quem der o maior lance aparecerá em primeiro lugar nos resultados de pesquisa para uma determinada palavra-chave. Para fazer o lance perfeito, o marketing se utiliza do machine learning. Ele analisa milhões de dados para ajustar os lances em tempo real.\n",
    "* **Prever os possíveis clientes**: Com base nos dados históricos da empresa, podemos coletar e entender qual é o perfil dos clientes. E com base nisso, prever a probabilidade do indivíduo adquirir determinado serviço. Como por exemplo, contrair um empréstimo, adquirir investimentos, dentre outros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a68bea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_1.4\">1.4) Objetivos do Projeto</h3>\n",
    "\n",
    "Como objetivo específico do problema de negócio, irei aplicar técnicas de Machine Learning para identificar e prever ser se o cliente vai ou não adquirir o empréstimo no banco.\n",
    "\n",
    "Como objetivo de estudo de tecnologia, estarei utilizando do início ao fim do projeto o framework Apache Spark, mais especificamente, o PySpark. PySpark é uma API Python para Apache SPARK que é denominado como o mecanismo de processamento analítico para aplicações de processamento de dados distribuídos em larga escala e aprendizado de máquina em tempo real, ou seja, para grandes volumes de dados, conhecido como Big Data.\n",
    "\n",
    "---\n",
    "\n",
    "<h3 id=\"id_1.5\">1.5) Sobre o Dataset</h3>\n",
    "\n",
    "Este conjunto de dados contém 20 atributos e 41189 registos relevantes para uma campanha de marketing direto de uma instituição bancária portuguesa. A campanha de marketing foi executada por meio de ligações telefônicas. O objetivo da classificação é prever se o cliente irá aderir (yes/no) ao CDB (variável y).<br/>\n",
    "\n",
    "O dataset pode ser obtido no site do Kaggle, clicando [aqui](https://www.kaggle.com/datasets/ruthgn/bank-marketing-data-set)\n",
    "\n",
    "> **Nota**: Eu alterei alguns registros do dataset para forçar a situação de dados missing/nulos. Para que dessa forma eu consiga desenvolver e detalhar a etapa de tratamento de valores NA.\n",
    "\n",
    "Atributos do Dataset:\n",
    "* AGE: Idade do indivíduo.\n",
    "* JOB: Emprego.\n",
    "* MARITAL: Status de estado civil: ('casado', 'solteiro', 'divorciado').\n",
    "* EDUCATION: Nível de escolaridade\n",
    "* DEFAULT: Se possui crédito inadimplente.\n",
    "* HOUSING: Possui credito à habitação.\n",
    "* LOAN: Se possui empréstimo.\n",
    "* CONTACT: Tipo de contato: ('Telefone' ou 'Smartphone').\n",
    "* MONTH: Mês do ano.\n",
    "* DAY OF WEEK: Dia da semana.\n",
    "* CAMPAIGN: Quantidade de contatos realizados durante esta campanha e para este cliente.\n",
    "* PDAYS: Quantidade de dias que se passaram depois que o cliente foi contatado pela última vez em uma campanha anterior.\n",
    "* PREVIOUS: Quantidade de contatos realizados antes desta campanha e para este cliente.\n",
    "* POUTCOME: Resultado da campanha de marketing anterior ('fracasso', 'inexistente', 'sucesso').\n",
    "* EMP.VAR.RATE: Taxa de variação do emprego - indicador trimestral.\n",
    "* CONS.PRICE.IDX: Índice de preços ao consumidor - indicador mensal.\n",
    "* CONS.CONF.IDX: Índice de confiança do consumidor - indicador mensal.\n",
    "* EURIBOR3M: Taxa de 3 meses euribor - indicador diário.\n",
    "* NR.EMPLOYED: Número de funcionários - indicador trimestral.\n",
    "\n",
    "\n",
    "* Y: Variável alvo. Verifica se o cliente adquiriu o empréstimo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53365659",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_1.6\">1.6) Introdução ao Apache Spark</h3>\n",
    "\n",
    "Apache Spark é uma estrutura de código aberto que simplifica o desenvolvimento e a eficiência dos trabalhos de análise de dados. Ele oferece suporte a uma ampla variedade de opções de API e linguagem com mais de 80 operadores de transformação e ação de dados que ocultam a complexidade da computação em cluster.\n",
    "\n",
    "Com velocidades relatadas 100 vezes mais rápidas do que mecanismos de análise semelhantes, o Spark pode acessar fontes de dados variáveis e ser executado em várias plataformas, incluindo Hadoop, Apache Mesos, Kubernetes, de forma independente ou na nuvem. Seja processando dados em lote ou streaming, você verá um desempenho de alto nível devido ao agendador Spark DAG de última geração, um otimizador de consulta e um mecanismo de execução física.\n",
    "\n",
    "> Caso você queira saber mais sobre o Apache Spark, eu recomendo fortemente a leitura do artigo \"Spark: entenda sua função e saiba mais sobre essa ferramenta\", publicado pelo blog XP Educação, que pode ser acessado clicando [aqui](https://blog.xpeducacao.com.br/apache-spark/)\n",
    "\n",
    "---\n",
    "\n",
    "<h3 id=\"id_1.7\">1.7) PySpark</h3>\n",
    "\n",
    "PySpark é a colaboração do Apache Spark e do Python.\n",
    "\n",
    "O Apache Spark é uma estrutura de computação em cluster de código aberto, construída em torno da velocidade, facilidade de uso e análise de streaming, enquanto o Python é uma linguagem de programação de alto nível e de uso geral. Ele fornece uma ampla variedade de bibliotecas e é usado principalmente para Machine Learning e Real-Time Streaming Analytics.\n",
    "\n",
    "Em outras palavras, é uma API Python para Spark que permite aproveitar a simplicidade do Python e o poder do Apache Spark para domar o Big Data. \n",
    "\n",
    "O uso da biblioteca PySpark possui diversas vantagens:\n",
    "* É um mecanismo de processamento distribuído, na memória, que permite o processamento de dados de forma eficiente e utilizando a característica de computação distribuída.\n",
    "* Com o uso do PySpark, é possível o processamento de dados em Hadoop (HDFS), AWS S3 e outros sistemas de arquivos.\n",
    "* Possui quatro grandes funcionalidades: Manipulação e integração SQL, Streaming de Dados, MLlib para Machine Learning e GraphX para manipulação de grafos.\n",
    "* Segundo os desenvolvedores, o Apache Spark é até 100x mais rápido em termos de processamento distribuído quando comparado com o Hadoop. \n",
    "\n",
    "Toda a execução dos scripts são realizados dentro do Apache Spark, que distribui o processamento dentro de um ambiente de cluster que são interligados aos NÓs que realizam a execução e transformação dos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719056b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 id=\"id_2\">2) INÍCIO DO PROJETO</h2>\n",
    "\n",
    "Depois de todas as definições, vamos iniciar o projeto. O conteúdo programático segue a estrutura padrão de projetos desse tipo. Iniciando pelo carregamento dos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f488779",
   "metadata": {},
   "source": [
    "<h3 id=\"id_2.1\">2.1) Carregamento dos Dados e Importações</h3>\n",
    "\n",
    "Irei utilizar funções de dois dos quatro principais módulos do Spark. Funções do módulo de SQL e também do MLLib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786a3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row #Converte RDDs em objetos do tipo Row\n",
    "from pyspark.sql.functions import col, isnan, when, count # Encontra a contagem para valores None, Null, Nan, etc.\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder #Converte strings em valores numéricos\n",
    "from pyspark.ml.linalg import Vectors #Serve para criar um vetor denso\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # Para avaliar o modelo com as métricas de avaliação.\n",
    "from pyspark.ml.feature import RobustScaler, StandardScaler, MinMaxScaler, Normalizer # Métodos para escalas dos dados\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier, LinearSVC # Algoritmos de ML\n",
    "from pyspark.ml import Pipeline # Criação de um Pipeline de execução.\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator # GridSearch e Validação Cruzada\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator # Evaluator para classificação binária\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ea32e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n",
      "Spark Context Version: 3.0.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f'System Version: {sys.version}')\n",
    "print(f'Spark Context Version: {sc.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd63a54",
   "metadata": {},
   "source": [
    "Ao inicializar o Notebook pelo terminal com o comando PySpark, criamos automaticamente um Contexto Spark (SparkContext). Este é um objeto que define como e onde o Spark acessa o Cluster.\n",
    "\n",
    "Para facilitar nossa vida, irei criar uma Sessão Spark. Esta serve para fornecer uma maneira simples para interagir com várias funcionalidades do Spark com um número menor de constructs. Em vez de ter que criar um contexto Spark, contexto Hive, contexto SQL, agora tudo é encapsulado em uma sessão Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53a43136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session - usada quando se trabalha com Dataframes no Spark\n",
    "spSession = SparkSession.builder.master(\"local\").appName(\"PySpark-BankMarketing\").config(\"spark.some.config.option\", \"session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfb2b0",
   "metadata": {},
   "source": [
    "Como citado, quero carregar o dataset que eu propositalmente modifiquei com o intuito de fazer algumas etapas adicionais para tratamento dos dados durante o notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a324bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd = sc.textFile('data/bank-marketing-dataset.csv') # Arquivo original\n",
    "rdd = sc.textFile('data/dataset-with-na.csv') # Arquivo modificado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a31928",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_2.2\">2.2) Visão Geral</h3>\n",
    "\n",
    "Primeiro uma visão geral sobre os dados e algumas explicações sobre o funcionamento básico do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7467a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2430676",
   "metadata": {},
   "source": [
    "**RDD - Resilient Distributed Datasets**\n",
    "\n",
    "É como uma tabela de banco de dados, é a essência do funcionamento do Spark. É uma coleção de objetos distribuída e imutável, é read-only. Cada conjunto de dados no RDD é dividido em partições lógicas, que podem ser computadas em diferentes nodes do cluster. Existem duas formas de criar o RDD:\n",
    "* Paralelizando uma coleção existente (função sc.parallelize);\n",
    "* Referenciando um dataset externo (HDFS, RDBMS, NoSQL, S3);\n",
    "\n",
    "O Spark utiliza o conceito de RDDs para aplicar o MapReduce de maneira rápida. Por padrão, os RDDs são computados cada vez que executamos uma ação. Entretanto, podemos “persistir” o RDD na memória (ou mesmo no disco) de modo que os dados estejam disponíveis ao longo do cluster e possam ser processados de forma muito mais rápida pelas operações de análise de dados.\n",
    "O RDD suporta dois tipos de operações:\n",
    "\n",
    "<img src=\"resources/tabela01.png\"/>\n",
    "\n",
    "Cada transformação gera um novo RDD, pois os RDDs são imutáveis. As ações aplicam as transformações nos RDDs e retornam o resultado.\n",
    "\n",
    "**Características dos RDDs**:\n",
    "\n",
    "* Spark é baseado em RDDs. Criamos, transformamos e armazenamos RDDs em Spark;\n",
    "* RDD representa uma coleção de elementos de dados particionados que podem ser operados em paralelo.\n",
    "* RDDs são objetos imutáveis. Eles não podem ser alterados uma vez criados.\n",
    "* RDDs podem ser colocados em cache e permitem persistência (mesmo objeto usado entre sessões diferentes).\n",
    "* Ao aplicarmos Transformações em RDDs criamos novos RDDs.\n",
    "* Ações aplicam as transformações nos RDDs e geram um resultado.\n",
    "\n",
    "**Existem dois tipos de transformações:**\n",
    "\n",
    "* *Narrow*: Resultado de funções como map() e filter() e os dados vem de uma única partição.\n",
    "* *Wide*: Resultado de funções como groupByKey() e os dados podem vir de diversas partições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d457ebc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41189"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando a quantidade de registros no RDD\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9229b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age,job,marital,education,default,housing,loan,contact,month,day_of_week,campaign,pdays,previous,poutcome,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed,y',\n",
       " '56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0,no',\n",
       " '57,services,married,high.school,unknown,no,no,telephone,may,mon,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0,no',\n",
       " '37,services,married,high.school,no,yes,no,telephone,may,mon,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0,no',\n",
       " '40,admin.,married,basic.6y,no,no,no,telephone,may,mon,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0,no']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listando os 5 primeiros registros\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ab08b",
   "metadata": {},
   "source": [
    "Veja que a primeira linha do RDD se trata do cabeçalho da tabela. Além disso, o RDD carregou os dados como uma \"lista de strings\". \n",
    "\n",
    "Portanto, o que eu irei fazer é: Remover a primeira linha que é o cabeçalho. Em seguida fazer o split dos dados utilizando o vírgula como separador. Assim, terei os dados como uma tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74d6137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AGE',\n",
       " 'JOB',\n",
       " 'MARITAL',\n",
       " 'EDUCATION',\n",
       " 'DEFAULT',\n",
       " 'HOUSING',\n",
       " 'LOAN',\n",
       " 'CONTACT',\n",
       " 'MONTH',\n",
       " 'DAY_OF_WEEK',\n",
       " 'CAMPAIGN',\n",
       " 'PDAYS',\n",
       " 'PREVIOUS',\n",
       " 'POUTCOME',\n",
       " 'EMP_VAR_RATE',\n",
       " 'CONS_PRICE_IDX',\n",
       " 'CONS_CONF_IDX',\n",
       " 'EURIBOR3M',\n",
       " 'NR_EMPLOYED',\n",
       " 'Y']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = rdd.first()\n",
    "rdd_body = rdd.filter(lambda x: header not in x).map(lambda l: l.split(','))\n",
    "\n",
    "list_columns = header.replace('.', '_').upper().split(',')\n",
    "list_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0dfe83",
   "metadata": {},
   "source": [
    "Agora quero criar o conceito de Rows. Isto é, para cada linha e para cada dado irei atribuir uma chave. Esta chave é justamente o nome das colunas do dataset.\n",
    "\n",
    "Alterei o nome do atributo Y para TARGET de modo a facilitar as próximas etapas do processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "711b94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_row = rdd_body.map(lambda p: Row(\n",
    "    AGE = p[0], \n",
    "    JOB = p[1], \n",
    "    MARITAL = p[2],\n",
    "    EDUCATION = p[3],\n",
    "    DEFAULT = p[4],\n",
    "    HOUSING = p[5],\n",
    "    LOAN = p[6],\n",
    "    CONTACT = p[7],\n",
    "    MONTH = p[8],\n",
    "    DAY_OF_WEEK = p[9],\n",
    "    CAMPAIGN = p[10],\n",
    "    PDAYS = p[11],\n",
    "    PREVIOUS = p[12],\n",
    "    POUTCOME = p[13],\n",
    "    EMP_VAR_RATE = p[14],\n",
    "    CONS_PRICE_IDX = p[15],\n",
    "    CONS_CONF_IDX = p[16],\n",
    "    EURIBOR3M = p[17],\n",
    "    EMPLOYED = p[18],\n",
    "    TARGET = p[19]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94467717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AGE: string, JOB: string, MARITAL: string, EDUCATION: string, DEFAULT: string, HOUSING: string, LOAN: string, CONTACT: string, MONTH: string, DAY_OF_WEEK: string, CAMPAIGN: string, PDAYS: string, PREVIOUS: string, POUTCOME: string, EMP_VAR_RATE: string, CONS_PRICE_IDX: string, CONS_CONF_IDX: string, EURIBOR3M: string, EMPLOYED: string, TARGET: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um Dataframe\n",
    "rdd_df = spSession.createDataFrame(rdd_row)\n",
    "rdd_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e12f073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+-----+--------+--------+------------+--------------+-------------+---------+--------+------+\n",
      "|AGE|JOB|MARITAL|EDUCATION|DEFAULT|HOUSING|LOAN|CONTACT|MONTH|DAY_OF_WEEK|CAMPAIGN|PDAYS|PREVIOUS|POUTCOME|EMP_VAR_RATE|CONS_PRICE_IDX|CONS_CONF_IDX|EURIBOR3M|EMPLOYED|TARGET|\n",
      "+---+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+-----+--------+--------+------------+--------------+-------------+---------+--------+------+\n",
      "|  0|  0|      1|        0|      1|      1|   0|      0|    0|          1|       0|    0|       0|       2|           0|             1|            0|        0|       0|     0|\n",
      "+---+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+-----+--------+--------+------------+--------------+-------------+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utilizando as funções importadas para auxiliar na detecção de valores Missing/Ausentes.\n",
    "\n",
    "rdd_na = rdd_df.select([count(when(col(c).contains('None') | col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | col(c).isNull() | isnan(c), c )).alias(c)\n",
    "                    for c in rdd_df.columns])\n",
    "rdd_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b529039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: AGE\tCount: 78\n",
      "Column: JOB\tCount: 12\n",
      "Column: MARITAL\tCount: 5\n",
      "Column: EDUCATION\tCount: 8\n",
      "Column: DEFAULT\tCount: 4\n",
      "Column: HOUSING\tCount: 4\n",
      "Column: LOAN\tCount: 3\n",
      "Column: CONTACT\tCount: 2\n",
      "Column: MONTH\tCount: 10\n",
      "Column: DAY_OF_WEEK\tCount: 6\n",
      "Column: CAMPAIGN\tCount: 42\n",
      "Column: PDAYS\tCount: 27\n",
      "Column: PREVIOUS\tCount: 8\n",
      "Column: POUTCOME\tCount: 4\n",
      "Column: EMP_VAR_RATE\tCount: 10\n",
      "Column: CONS_PRICE_IDX\tCount: 27\n",
      "Column: CONS_CONF_IDX\tCount: 26\n",
      "Column: EURIBOR3M\tCount: 316\n",
      "Column: EMPLOYED\tCount: 11\n",
      "Column: TARGET\tCount: 2\n"
     ]
    }
   ],
   "source": [
    "# Visualizando os valores únicos para cada coluna do dataframe\n",
    "\n",
    "list_columns = rdd_df.columns\n",
    "\n",
    "for column in list_columns:\n",
    "    count = rdd_df.select(column).distinct().count()\n",
    "    print(f'Column: {column}\\tCount: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec41b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_2.3\">2.3) Tratamento de Valores Ausentes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a624625",
   "metadata": {},
   "source": [
    "Colunas que apresentaram valores ausentes:\n",
    "* MARITAL\n",
    "* DEFAULT\n",
    "* HOUSING\n",
    "* DAY_OF_WEEK\n",
    "* POUTCOME\n",
    "* CONS_PRICE_IDX\n",
    "\n",
    "> Vários registros em várias colunas possuem o valor `unknown` em português (desconhecido). Porém, eu irei considerar estes valores como corretos e não vou fazer nenhum tratamento para estes dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0c927e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este método agrupa o dataframe pela coluna passada como parâmetro e pela variável target.\n",
    "def getDfGroup(rdd_df, column):\n",
    "    df_group = spSession.createDataFrame(rdd_df.groupBy(['TARGET', column]).agg({column: 'count'}).collect())\n",
    "\n",
    "    df_group = df_group.orderBy(['TARGET', column, f'count({column})'], ascending=[0, 1, 0])\n",
    "\n",
    "    return df_group\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cdcb5a",
   "metadata": {},
   "source": [
    "**MARITAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ef631aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', MARITAL='divorced', count(MARITAL)=476),\n",
       " Row(TARGET='yes', MARITAL='married', count(MARITAL)=2532),\n",
       " Row(TARGET='yes', MARITAL='single', count(MARITAL)=1620),\n",
       " Row(TARGET='yes', MARITAL='unknown', count(MARITAL)=12),\n",
       " Row(TARGET='no', MARITAL='', count(MARITAL)=1),\n",
       " Row(TARGET='no', MARITAL='divorced', count(MARITAL)=4136),\n",
       " Row(TARGET='no', MARITAL='married', count(MARITAL)=22396),\n",
       " Row(TARGET='no', MARITAL='single', count(MARITAL)=9947),\n",
       " Row(TARGET='no', MARITAL='unknown', count(MARITAL)=68)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'MARITAL')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47a69d",
   "metadata": {},
   "source": [
    "A moda do valor nulo agrupada por target é `married`, contendo 22396 registros, portanto, é com esse valor que irei preencher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04cbbf3",
   "metadata": {},
   "source": [
    "**DEFAULT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2afe39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', DEFAULT='no', count(DEFAULT)=4197),\n",
       " Row(TARGET='yes', DEFAULT='unknown', count(DEFAULT)=443),\n",
       " Row(TARGET='no', DEFAULT='', count(DEFAULT)=1),\n",
       " Row(TARGET='no', DEFAULT='no', count(DEFAULT)=28391),\n",
       " Row(TARGET='no', DEFAULT='unknown', count(DEFAULT)=8153),\n",
       " Row(TARGET='no', DEFAULT='yes', count(DEFAULT)=3)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'DEFAULT')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1fc9a8",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `no`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4fd36",
   "metadata": {},
   "source": [
    "**EDUCATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce394f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', EDUCATION='basic.4y', count(EDUCATION)=428),\n",
       " Row(TARGET='yes', EDUCATION='basic.6y', count(EDUCATION)=188),\n",
       " Row(TARGET='yes', EDUCATION='basic.9y', count(EDUCATION)=473),\n",
       " Row(TARGET='yes', EDUCATION='high.school', count(EDUCATION)=1031),\n",
       " Row(TARGET='yes', EDUCATION='illiterate', count(EDUCATION)=4),\n",
       " Row(TARGET='yes', EDUCATION='professional.course', count(EDUCATION)=595),\n",
       " Row(TARGET='yes', EDUCATION='university.degree', count(EDUCATION)=1670),\n",
       " Row(TARGET='yes', EDUCATION='unknown', count(EDUCATION)=251),\n",
       " Row(TARGET='no', EDUCATION='basic.4y', count(EDUCATION)=3748),\n",
       " Row(TARGET='no', EDUCATION='basic.6y', count(EDUCATION)=2104),\n",
       " Row(TARGET='no', EDUCATION='basic.9y', count(EDUCATION)=5572),\n",
       " Row(TARGET='no', EDUCATION='high.school', count(EDUCATION)=8484),\n",
       " Row(TARGET='no', EDUCATION='illiterate', count(EDUCATION)=14),\n",
       " Row(TARGET='no', EDUCATION='professional.course', count(EDUCATION)=4648),\n",
       " Row(TARGET='no', EDUCATION='university.degree', count(EDUCATION)=10498),\n",
       " Row(TARGET='no', EDUCATION='unknown', count(EDUCATION)=1480)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'EDUCATION')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c262dca",
   "metadata": {},
   "source": [
    "Neste atributo, temos valores `unknown`, porém, não irei considerar como valor nulo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d75b1",
   "metadata": {},
   "source": [
    "**HOUSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f32ed5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', HOUSING='no', count(HOUSING)=2026),\n",
       " Row(TARGET='yes', HOUSING='unknown', count(HOUSING)=107),\n",
       " Row(TARGET='yes', HOUSING='yes', count(HOUSING)=2507),\n",
       " Row(TARGET='no', HOUSING='', count(HOUSING)=1),\n",
       " Row(TARGET='no', HOUSING='no', count(HOUSING)=16596),\n",
       " Row(TARGET='no', HOUSING='unknown', count(HOUSING)=883),\n",
       " Row(TARGET='no', HOUSING='yes', count(HOUSING)=19068)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'HOUSING')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ffe66",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `yes`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74475b7",
   "metadata": {},
   "source": [
    "**DAY_OF_WEEK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e3dd750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', DAY_OF_WEEK='fri', count(DAY_OF_WEEK)=846),\n",
       " Row(TARGET='yes', DAY_OF_WEEK='mon', count(DAY_OF_WEEK)=847),\n",
       " Row(TARGET='yes', DAY_OF_WEEK='thu', count(DAY_OF_WEEK)=1045),\n",
       " Row(TARGET='yes', DAY_OF_WEEK='tue', count(DAY_OF_WEEK)=953),\n",
       " Row(TARGET='yes', DAY_OF_WEEK='wed', count(DAY_OF_WEEK)=949),\n",
       " Row(TARGET='no', DAY_OF_WEEK='', count(DAY_OF_WEEK)=1),\n",
       " Row(TARGET='no', DAY_OF_WEEK='fri', count(DAY_OF_WEEK)=6981),\n",
       " Row(TARGET='no', DAY_OF_WEEK='mon', count(DAY_OF_WEEK)=7666),\n",
       " Row(TARGET='no', DAY_OF_WEEK='thu', count(DAY_OF_WEEK)=7578),\n",
       " Row(TARGET='no', DAY_OF_WEEK='tue', count(DAY_OF_WEEK)=7137),\n",
       " Row(TARGET='no', DAY_OF_WEEK='wed', count(DAY_OF_WEEK)=7185)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'DAY_OF_WEEK')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f5d0f",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `mon`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5483f1",
   "metadata": {},
   "source": [
    "**POUTCOME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a95e55c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', POUTCOME='failure', count(POUTCOME)=605),\n",
       " Row(TARGET='yes', POUTCOME='nonexistent', count(POUTCOME)=3141),\n",
       " Row(TARGET='yes', POUTCOME='success', count(POUTCOME)=894),\n",
       " Row(TARGET='no', POUTCOME='', count(POUTCOME)=2),\n",
       " Row(TARGET='no', POUTCOME='failure', count(POUTCOME)=3647),\n",
       " Row(TARGET='no', POUTCOME='nonexistent', count(POUTCOME)=32420),\n",
       " Row(TARGET='no', POUTCOME='success', count(POUTCOME)=479)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'POUTCOME')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e88318",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `nonexistent`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4ec3c",
   "metadata": {},
   "source": [
    "**CONS_PRICE_IDX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f40f5190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', CONS_PRICE_IDX='92.20100000000001', count(CONS_PRICE_IDX)=264),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.37899999999999', count(CONS_PRICE_IDX)=106),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.431', count(CONS_PRICE_IDX)=180),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.469', count(CONS_PRICE_IDX)=66),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.649', count(CONS_PRICE_IDX)=168),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.713', count(CONS_PRICE_IDX)=88),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.756', count(CONS_PRICE_IDX)=1),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.84299999999999', count(CONS_PRICE_IDX)=126),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.89299999999999', count(CONS_PRICE_IDX)=524),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.963', count(CONS_PRICE_IDX)=264),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.075', count(CONS_PRICE_IDX)=442),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.2', count(CONS_PRICE_IDX)=190),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.369', count(CONS_PRICE_IDX)=150),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.444', count(CONS_PRICE_IDX)=271),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.749', count(CONS_PRICE_IDX)=97),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.79799999999999', count(CONS_PRICE_IDX)=42),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.876', count(CONS_PRICE_IDX)=122),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.91799999999999', count(CONS_PRICE_IDX)=407),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.994', count(CONS_PRICE_IDX)=240),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.027', count(CONS_PRICE_IDX)=120),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.055', count(CONS_PRICE_IDX)=107),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.199', count(CONS_PRICE_IDX)=150),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.215', count(CONS_PRICE_IDX)=176),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.465', count(CONS_PRICE_IDX)=188),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.601', count(CONS_PRICE_IDX)=93),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.76700000000001', count(CONS_PRICE_IDX)=58),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='', count(CONS_PRICE_IDX)=1),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.20100000000001', count(CONS_PRICE_IDX)=506),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.37899999999999', count(CONS_PRICE_IDX)=161),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.431', count(CONS_PRICE_IDX)=267),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.469', count(CONS_PRICE_IDX)=112),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.649', count(CONS_PRICE_IDX)=189),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.713', count(CONS_PRICE_IDX)=84),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.756', count(CONS_PRICE_IDX)=9),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.84299999999999', count(CONS_PRICE_IDX)=156),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.89299999999999', count(CONS_PRICE_IDX)=5270),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.963', count(CONS_PRICE_IDX)=451),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.075', count(CONS_PRICE_IDX)=2016),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.2', count(CONS_PRICE_IDX)=3426),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.369', count(CONS_PRICE_IDX)=114),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.444', count(CONS_PRICE_IDX)=4904),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.749', count(CONS_PRICE_IDX)=77),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.79799999999999', count(CONS_PRICE_IDX)=25),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.876', count(CONS_PRICE_IDX)=90),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.91799999999999', count(CONS_PRICE_IDX)=6278),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.994', count(CONS_PRICE_IDX)=7522),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.027', count(CONS_PRICE_IDX)=113),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.055', count(CONS_PRICE_IDX)=122),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.199', count(CONS_PRICE_IDX)=153),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.215', count(CONS_PRICE_IDX)=135),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.465', count(CONS_PRICE_IDX)=4186),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.601', count(CONS_PRICE_IDX)=111),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.76700000000001', count(CONS_PRICE_IDX)=70)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'CONS_PRICE_IDX')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ffa08",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `93.91799999999999`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dced9c",
   "metadata": {},
   "source": [
    "Após ter encontrado todos os valores nulos e definido com quais valores eu irei preencher, irei criar a função que realizará todo este mamepamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9db5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificarNA(c):\n",
    "    c = c.upper()\n",
    "    if c == 'NONE' or c == 'NULL' or c == '' or c == 'NAN':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def mapNA(x):\n",
    "    AGE = x.AGE\n",
    "    JOB = x.JOB\n",
    "    MARITAL = x.MARITAL\n",
    "    EDUCATION = x.EDUCATION\n",
    "    DEFAULT = x.DEFAULT\n",
    "    HOUSING = x.HOUSING\n",
    "    LOAN = x.LOAN\n",
    "    CONTACT = x.CONTACT\n",
    "    MONTH = x.MONTH\n",
    "    DAY_OF_WEEK = x.DAY_OF_WEEK\n",
    "    CAMPAIGN = x.CAMPAIGN\n",
    "    PDAYS = x.PDAYS\n",
    "    PREVIOUS = x.PREVIOUS\n",
    "    POUTCOME = x.POUTCOME\n",
    "    EMP_VAR_RATE = x.EMP_VAR_RATE\n",
    "    CONS_PRICE_IDX = x.CONS_PRICE_IDX\n",
    "    CONS_CONF_IDX = x.CONS_CONF_IDX\n",
    "    EURIBOR3M = x.EURIBOR3M\n",
    "    EMPLOYED = x.EMPLOYED\n",
    "    TARGET = x.TARGET\n",
    "    \n",
    "    #Corrigindo valores missing\n",
    "    if verificarNA(x.MARITAL):\n",
    "        MARITAL = 'married'\n",
    "        \n",
    "    if verificarNA(x.DEFAULT):\n",
    "        DEFAULT = 'no'\n",
    "        \n",
    "    if verificarNA(x.HOUSING):\n",
    "        HOUSING = 'yes'\n",
    "        \n",
    "    if verificarNA(x.DAY_OF_WEEK):\n",
    "        DAY_OF_WEEK = 'mon'\n",
    "        \n",
    "    if verificarNA(x.POUTCOME):\n",
    "        POUTCOME = 'nonexistent'\n",
    "        \n",
    "    if verificarNA(x.CONS_PRICE_IDX):\n",
    "        CONS_PRICE_IDX = '93.91799999999999'\n",
    "    \n",
    "    \n",
    "    return (AGE, JOB, MARITAL, EDUCATION, DEFAULT, HOUSING, LOAN, CONTACT, MONTH, DAY_OF_WEEK, CAMPAIGN, PDAYS, PREVIOUS, POUTCOME, EMP_VAR_RATE, CONS_PRICE_IDX, CONS_CONF_IDX, EURIBOR3M, EMPLOYED, TARGET)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16aa1d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AGE: string, JOB: string, MARITAL: string, EDUCATION: string, DEFAULT: string, HOUSING: string, LOAN: string, CONTACT: string, MONTH: string, DAY_OF_WEEK: string, CAMPAIGN: string, PDAYS: string, PREVIOUS: string, POUTCOME: string, EMP_VAR_RATE: string, CONS_PRICE_IDX: string, CONS_CONF_IDX: string, EURIBOR3M: string, EMPLOYED: string, TARGET: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_row = rdd_df.rdd.map(lambda x: mapNA(x))\n",
    "\n",
    "# Criando um Dataframe\n",
    "rdd_df = spSession.createDataFrame(rdd_row, schema=list_columns)\n",
    "rdd_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "821dc6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+--------+------+\n",
      "|AGE|      JOB|MARITAL|  EDUCATION|DEFAULT|HOUSING|LOAN|  CONTACT|MONTH|DAY_OF_WEEK|CAMPAIGN|PDAYS|PREVIOUS|   POUTCOME|EMP_VAR_RATE|CONS_PRICE_IDX|CONS_CONF_IDX|EURIBOR3M|EMPLOYED|TARGET|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+--------+------+\n",
      "| 56|housemaid|married|   basic.4y|     no|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|  5191.0|    no|\n",
      "| 57| services|married|high.school|unknown|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|  5191.0|    no|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+--------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef911455",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b11ee0",
   "metadata": {},
   "source": [
    "<h2 id=\"id_3\">3) PRÉ PROCESSAMENTO DOS DADOS</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb83c7a",
   "metadata": {},
   "source": [
    "<h3 id=\"id_3.1\">3.1) Encoding dos Dados</h3>\n",
    "\n",
    "A maioria dos atributos são dados categóricos que representam classes. Porém, da forma que está, é impossível submeter os dados para um modelo de Machine Learning.\n",
    "\n",
    "Logo, temos que converter os dados categóricos para numéricos. E para isso, existem duas abordagens: Label Encoder e o One Hot Encoder. Mas calma, irei explicar o que são e em que situações se aplicam cada um dos dois.\n",
    "\n",
    "* **Label Encoder**: De maneira simples, o Label Encoder serve para converter os dados categóricos para numéricos. Vamos pegar como exemplo os dias da semana. O algoritmo vai converter os dias em numéricos. Ou seja, Segunda-feira será 0, Terça-feira será igual 1, Quarta-Feira igual 2, e assim por diante.\n",
    "\n",
    "    O que devemos nos atentar?<br/>\n",
    "    Veja que o algoritmo substitui valores por números, certo? Pois bem, note que os números são sequenciais e para a maioria dos algoritmos de Machine Learning o fato do numéro ser sequencial implica que o 5 tem mais peso que o 4, o 4 por sua vez, tem mais peso que o 3, e assim por diante.\n",
    "\n",
    "    Usamos o Label Encoder quando temos uma hierarquia nos dados. Por exemplo, grau de escolaridade, patente em cargos militares, etc. \n",
    "\n",
    "> Nota: Em casos onde temos apenas dois valores distintos, podemos utilizar o Label Encoder, mesmo que não haja uma hierárquia entre os dados. Afinal de contas, teremos apenas dois valores: 0 e 1.\n",
    "\n",
    "---\n",
    "\n",
    "* **One Hot Encoder**: O One hot encoder (OHE) também converte dados categóricos para valores numéricos. Mas nesse método, para cada classe diferente, o algoritmo cria uma nova coluna, atribuindo apenas os valores 0 e 1. Com isso, eliminamos a possibilidade de uma classe influenciar a outra em um modelo de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e629873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE: Distinct count: 78\n",
      "+---+-----+\n",
      "|AGE|count|\n",
      "+---+-----+\n",
      "| 51|  754|\n",
      "| 54|  684|\n",
      "| 29| 1453|\n",
      "| 69|   34|\n",
      "| 42| 1142|\n",
      "| 73|   34|\n",
      "| 87|    1|\n",
      "| 64|   57|\n",
      "| 30| 1714|\n",
      "| 34| 1745|\n",
      "| 59|  463|\n",
      "| 28| 1001|\n",
      "| 22|  137|\n",
      "| 85|   15|\n",
      "| 35| 1759|\n",
      "| 52|  779|\n",
      "| 71|   53|\n",
      "| 98|    2|\n",
      "| 47|  928|\n",
      "| 43| 1055|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "JOB: Distinct count: 12\n",
      "+-------------+-----+\n",
      "|          JOB|count|\n",
      "+-------------+-----+\n",
      "|   management| 2924|\n",
      "|      retired| 1720|\n",
      "|      unknown|  330|\n",
      "|self-employed| 1421|\n",
      "|      student|  875|\n",
      "|  blue-collar| 9254|\n",
      "| entrepreneur| 1456|\n",
      "|       admin.|10422|\n",
      "|   technician| 6743|\n",
      "|     services| 3969|\n",
      "|    housemaid| 1060|\n",
      "|   unemployed| 1014|\n",
      "+-------------+-----+\n",
      "\n",
      "None\n",
      "MARITAL: Distinct count: 4\n",
      "+--------+-----+\n",
      "| MARITAL|count|\n",
      "+--------+-----+\n",
      "| unknown|   80|\n",
      "|divorced| 4612|\n",
      "| married|24929|\n",
      "|  single|11567|\n",
      "+--------+-----+\n",
      "\n",
      "None\n",
      "EDUCATION: Distinct count: 8\n",
      "+-------------------+-----+\n",
      "|          EDUCATION|count|\n",
      "+-------------------+-----+\n",
      "|        high.school| 9515|\n",
      "|            unknown| 1731|\n",
      "|           basic.6y| 2292|\n",
      "|professional.course| 5243|\n",
      "|  university.degree|12168|\n",
      "|         illiterate|   18|\n",
      "|           basic.4y| 4176|\n",
      "|           basic.9y| 6045|\n",
      "+-------------------+-----+\n",
      "\n",
      "None\n",
      "DEFAULT: Distinct count: 3\n",
      "+-------+-----+\n",
      "|DEFAULT|count|\n",
      "+-------+-----+\n",
      "|unknown| 8596|\n",
      "|     no|32589|\n",
      "|    yes|    3|\n",
      "+-------+-----+\n",
      "\n",
      "None\n",
      "HOUSING: Distinct count: 3\n",
      "+-------+-----+\n",
      "|HOUSING|count|\n",
      "+-------+-----+\n",
      "|unknown|  990|\n",
      "|     no|18622|\n",
      "|    yes|21576|\n",
      "+-------+-----+\n",
      "\n",
      "None\n",
      "LOAN: Distinct count: 3\n",
      "+-------+-----+\n",
      "|   LOAN|count|\n",
      "+-------+-----+\n",
      "|unknown|  990|\n",
      "|     no|33950|\n",
      "|    yes| 6248|\n",
      "+-------+-----+\n",
      "\n",
      "None\n",
      "CONTACT: Distinct count: 2\n",
      "+---------+-----+\n",
      "|  CONTACT|count|\n",
      "+---------+-----+\n",
      "| cellular|26144|\n",
      "|telephone|15044|\n",
      "+---------+-----+\n",
      "\n",
      "None\n",
      "MONTH: Distinct count: 10\n",
      "+-----+-----+\n",
      "|MONTH|count|\n",
      "+-----+-----+\n",
      "|  jun| 5318|\n",
      "|  aug| 6178|\n",
      "|  may|13769|\n",
      "|  sep|  570|\n",
      "|  mar|  546|\n",
      "|  oct|  718|\n",
      "|  jul| 7174|\n",
      "|  nov| 4101|\n",
      "|  apr| 2632|\n",
      "|  dec|  182|\n",
      "+-----+-----+\n",
      "\n",
      "None\n",
      "DAY_OF_WEEK: Distinct count: 5\n",
      "+-----------+-----+\n",
      "|DAY_OF_WEEK|count|\n",
      "+-----------+-----+\n",
      "|        fri| 7827|\n",
      "|        thu| 8623|\n",
      "|        tue| 8090|\n",
      "|        wed| 8134|\n",
      "|        mon| 8514|\n",
      "+-----------+-----+\n",
      "\n",
      "None\n",
      "CAMPAIGN: Distinct count: 42\n",
      "+--------+-----+\n",
      "|CAMPAIGN|count|\n",
      "+--------+-----+\n",
      "|       7|  629|\n",
      "|      15|   51|\n",
      "|      11|  177|\n",
      "|      29|   10|\n",
      "|      42|    2|\n",
      "|       3| 5341|\n",
      "|      30|    7|\n",
      "|      34|    3|\n",
      "|       8|  400|\n",
      "|      22|   17|\n",
      "|      28|    8|\n",
      "|      16|   51|\n",
      "|      35|    5|\n",
      "|      43|    2|\n",
      "|       5| 1599|\n",
      "|      31|    7|\n",
      "|      18|   33|\n",
      "|      27|   11|\n",
      "|      17|   58|\n",
      "|      26|    8|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "PDAYS: Distinct count: 27\n",
      "+-----+-----+\n",
      "|PDAYS|count|\n",
      "+-----+-----+\n",
      "|    7|   60|\n",
      "|   15|   24|\n",
      "|   11|   28|\n",
      "|    3|  439|\n",
      "|    8|   18|\n",
      "|   22|    3|\n",
      "|   16|   11|\n",
      "|    0|   15|\n",
      "|    5|   46|\n",
      "|   18|    7|\n",
      "|   27|    1|\n",
      "|  999|39673|\n",
      "|   17|    8|\n",
      "|   26|    1|\n",
      "|    6|  412|\n",
      "|   19|    3|\n",
      "|   25|    1|\n",
      "|    9|   64|\n",
      "|    1|   26|\n",
      "|   20|    1|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "PREVIOUS: Distinct count: 8\n",
      "+--------+-----+\n",
      "|PREVIOUS|count|\n",
      "+--------+-----+\n",
      "|       7|    1|\n",
      "|       3|  216|\n",
      "|       0|35563|\n",
      "|       5|   18|\n",
      "|       6|    5|\n",
      "|       1| 4561|\n",
      "|       4|   70|\n",
      "|       2|  754|\n",
      "+--------+-----+\n",
      "\n",
      "None\n",
      "POUTCOME: Distinct count: 3\n",
      "+-----------+-----+\n",
      "|   POUTCOME|count|\n",
      "+-----------+-----+\n",
      "|    success| 1373|\n",
      "|    failure| 4252|\n",
      "|nonexistent|35563|\n",
      "+-----------+-----+\n",
      "\n",
      "None\n",
      "EMP_VAR_RATE: Distinct count: 10\n",
      "+------------+-----+\n",
      "|EMP_VAR_RATE|count|\n",
      "+------------+-----+\n",
      "|        -0.1| 3683|\n",
      "|        -1.8| 9184|\n",
      "|        -1.7|  773|\n",
      "|        -0.2|   10|\n",
      "|        -3.0|  172|\n",
      "|         1.4|16234|\n",
      "|        -2.9| 1663|\n",
      "|        -1.1|  635|\n",
      "|        -3.4| 1071|\n",
      "|         1.1| 7763|\n",
      "+------------+-----+\n",
      "\n",
      "None\n",
      "CONS_PRICE_IDX: Distinct count: 26\n",
      "+-----------------+-----+\n",
      "|   CONS_PRICE_IDX|count|\n",
      "+-----------------+-----+\n",
      "|           94.027|  233|\n",
      "|93.91799999999999| 6686|\n",
      "|           94.465| 4374|\n",
      "|94.76700000000001|  128|\n",
      "|92.37899999999999|  267|\n",
      "|           94.055|  229|\n",
      "|           93.444| 5175|\n",
      "|           93.369|  264|\n",
      "|93.79799999999999|   67|\n",
      "|           94.199|  303|\n",
      "|           92.756|   10|\n",
      "|92.84299999999999|  282|\n",
      "|92.20100000000001|  770|\n",
      "|           92.963|  715|\n",
      "|           92.649|  357|\n",
      "|92.89299999999999| 5794|\n",
      "|           92.469|  178|\n",
      "|           93.994| 7762|\n",
      "|             93.2| 3616|\n",
      "|           94.601|  204|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "CONS_CONF_IDX: Distinct count: 26\n",
      "+-------------+-----+\n",
      "|CONS_CONF_IDX|count|\n",
      "+-------------+-----+\n",
      "|        -34.8|  264|\n",
      "|        -36.1| 5175|\n",
      "|        -40.4|   67|\n",
      "|        -31.4|  770|\n",
      "|        -41.8| 4374|\n",
      "|        -36.4| 7763|\n",
      "|        -42.7| 6685|\n",
      "|        -50.0|  282|\n",
      "|        -30.1|  357|\n",
      "|        -49.5|  204|\n",
      "|        -45.9|   10|\n",
      "|        -47.1| 2458|\n",
      "|        -34.6|  174|\n",
      "|        -26.9|  447|\n",
      "|        -39.8|  229|\n",
      "|        -29.8|  267|\n",
      "|        -40.3|  311|\n",
      "|        -33.0|  172|\n",
      "|        -42.0| 3616|\n",
      "|        -37.5|  303|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "EURIBOR3M: Distinct count: 316\n",
      "+------------------+-----+\n",
      "|         EURIBOR3M|count|\n",
      "+------------------+-----+\n",
      "|             1.072|   34|\n",
      "|             0.899|   50|\n",
      "|             0.937|    2|\n",
      "|             0.762|    4|\n",
      "|               1.0|   18|\n",
      "|              4.96| 1013|\n",
      "|             1.045|    1|\n",
      "|             1.327|  538|\n",
      "|             1.334|  482|\n",
      "|             4.857| 2868|\n",
      "|3.6689999999999996|    1|\n",
      "|              0.75|    7|\n",
      "|             4.957|  537|\n",
      "|             1.614|   13|\n",
      "|1.0490000000000002|   13|\n",
      "|0.8029999999999999|   31|\n",
      "|             0.835|   20|\n",
      "|             0.883|  124|\n",
      "|4.0760000000000005|  822|\n",
      "|             1.405| 1169|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "EMPLOYED: Distinct count: 11\n",
      "+--------+-----+\n",
      "|EMPLOYED|count|\n",
      "+--------+-----+\n",
      "|  5176.3|   10|\n",
      "|  5228.1|16234|\n",
      "|  5195.8| 3683|\n",
      "|  5023.5|  172|\n",
      "|  5099.1| 8534|\n",
      "|  4991.6|  773|\n",
      "|  4963.6|  635|\n",
      "|  5076.2| 1663|\n",
      "|  5008.7|  650|\n",
      "|  5017.5| 1071|\n",
      "|  5191.0| 7763|\n",
      "+--------+-----+\n",
      "\n",
      "None\n",
      "TARGET: Distinct count: 2\n",
      "+------+-----+\n",
      "|TARGET|count|\n",
      "+------+-----+\n",
      "|    no|36548|\n",
      "|   yes| 4640|\n",
      "+------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Voltando a programação.. Vejamos todos os valores únicos e a quantidade de registros para cada coluna\n",
    "\n",
    "for column in list_columns:\n",
    "    print(f'{column}: Distinct count: {rdd_df.select(column).distinct().count()}')\n",
    "    print(rdd_df.groupBy([column]).count().show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b42ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_3.2\">3.1.1) Label Encoder</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba7fe90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas são as colunas que eu decidi selecionar para aplicar o Label Encoder\n",
    "columns_labelencoder = ['EDUCATION', 'DEFAULT', 'CONTACT', 'MONTH', 'DAY_OF_WEEK', 'CAMPAIGN', 'PDAYS', 'PREVIOUS', 'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79dcefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crio uma função para mapear cada atributo\n",
    "\n",
    "def map_education(x):\n",
    "    return -1 if x == 'unknown' else \\\n",
    "            0 if x == 'illiterate' else \\\n",
    "            1 if x == 'basic.4y' else \\\n",
    "            2 if x == 'basic.6y' else \\\n",
    "            3 if x == 'basic.9y' else \\\n",
    "            4 if x == 'high.school' else \\\n",
    "            5 if x == 'professional.course' else \\\n",
    "            6 if x == 'university.degree' else \\\n",
    "            7\n",
    "\n",
    "def map_housing(x):\n",
    "    return  0 if x == 'yes' else \\\n",
    "            1 if x == 'no' else \\\n",
    "            1 if x == 'unknown' else\\\n",
    "           -1\n",
    "\n",
    "def map_month(x):\n",
    "    return  0 if x == 'jan' else \\\n",
    "            1 if x == 'feb' else \\\n",
    "            2 if x == 'mar' else \\\n",
    "            3 if x == 'apr' else \\\n",
    "            4 if x == 'may' else \\\n",
    "            5 if x == 'jun' else \\\n",
    "            6 if x == 'jul' else \\\n",
    "            7 if x == 'aug' else \\\n",
    "            8 if x == 'sep' else \\\n",
    "            9 if x == 'oct' else \\\n",
    "            10 if x == 'nov' else \\\n",
    "            11 if x == 'dec' else \\\n",
    "            -1\n",
    "\n",
    "def map_day_of_week(x):\n",
    "    return  0 if x == 'mon' else \\\n",
    "            1 if x == 'tue' else \\\n",
    "            2 if x == 'wed' else \\\n",
    "            3 if x == 'thu' else \\\n",
    "            4 if x == 'fri' else \\\n",
    "            -1\n",
    "\n",
    "def map_contact(x):\n",
    "    return  0 if x == 'cellular' else \\\n",
    "            1 if x == 'telephone' else \\\n",
    "            -1\n",
    "\n",
    "def map_target(x):\n",
    "    return  0 if x == 'no' else \\\n",
    "            1 if x == 'yes' else \\\n",
    "            -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "633fc2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para aplicar efetivamente o LabelEncoder \n",
    "\n",
    "def manualEncoder(x):\n",
    "    AGE = x.AGE\n",
    "    JOB = x.JOB\n",
    "    MARITAL = x.MARITAL\n",
    "    EDUCATION = map_education(x.EDUCATION)\n",
    "    DEFAULT = x.DEFAULT\n",
    "    HOUSING = map_housing(x.HOUSING)\n",
    "    LOAN = x.LOAN\n",
    "    CONTACT = map_contact(x.CONTACT)\n",
    "    MONTH = map_month(x.MONTH)\n",
    "    DAY_OF_WEEK = map_day_of_week(x.DAY_OF_WEEK)\n",
    "    CAMPAIGN = x.CAMPAIGN\n",
    "    PDAYS = x.PDAYS\n",
    "    PREVIOUS = x.PREVIOUS\n",
    "    POUTCOME = x.POUTCOME\n",
    "    EMP_VAR_RATE = x.EMP_VAR_RATE\n",
    "    CONS_PRICE_IDX = x.CONS_PRICE_IDX\n",
    "    CONS_CONF_IDX = x.CONS_CONF_IDX\n",
    "    EURIBOR3M = x.EURIBOR3M\n",
    "    EMPLOYED = x.EMPLOYED\n",
    "    TARGET = map_target(x.TARGET)\n",
    "  \n",
    "    \n",
    "    \n",
    "    return (AGE, JOB, MARITAL, EDUCATION, DEFAULT, HOUSING, LOAN, CONTACT, \n",
    "            MONTH, DAY_OF_WEEK, CAMPAIGN, PDAYS, PREVIOUS, POUTCOME, EMP_VAR_RATE, \n",
    "            CONS_PRICE_IDX, CONS_CONF_IDX, EURIBOR3M, EMPLOYED, TARGET)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f79efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_row = rdd_df.rdd.map(lambda x: manualEncoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3961a13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AGE: string, JOB: string, MARITAL: string, EDUCATION: bigint, DEFAULT: string, HOUSING: bigint, LOAN: string, CONTACT: bigint, MONTH: bigint, DAY_OF_WEEK: bigint, CAMPAIGN: string, PDAYS: string, PREVIOUS: string, POUTCOME: string, EMP_VAR_RATE: string, CONS_PRICE_IDX: string, CONS_CONF_IDX: string, EURIBOR3M: string, EMPLOYED: string, TARGET: bigint]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um Dataframe\n",
    "rdd_df_encoded = spSession.createDataFrame(rdd_row, schema=list_columns)\n",
    "rdd_df_encoded.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04e89bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+---------+-------+-------+----+-------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+--------+------+\n",
      "|AGE|      JOB|MARITAL|EDUCATION|DEFAULT|HOUSING|LOAN|CONTACT|MONTH|DAY_OF_WEEK|CAMPAIGN|PDAYS|PREVIOUS|   POUTCOME|EMP_VAR_RATE|CONS_PRICE_IDX|CONS_CONF_IDX|EURIBOR3M|EMPLOYED|TARGET|\n",
      "+---+---------+-------+---------+-------+-------+----+-------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+--------+------+\n",
      "| 56|housemaid|married|        1|     no|      1|  no|      1|    4|          0|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|  5191.0|     0|\n",
      "| 57| services|married|        4|unknown|      1|  no|      1|    4|          0|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|  5191.0|     0|\n",
      "+---+---------+-------+---------+-------+-------+----+-------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+--------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_df_encoded.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be7ae3",
   "metadata": {},
   "source": [
    "> Nota: Eu poderia ter utilizado um método para fazer tudo isso de maneira automática para mim como o StringIndexer. Porém, eu quis ter um controle para indexar os dados ordenados de maneira lógica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8c657",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_3.3\">3.1.2) One Hot Encoder</h3>\n",
    "\n",
    "Agora o One Hot Encoder. Estas são as colunas que eu decidi selecionar para aplicar o algoritmo. Para esta etapa eu quis criar uma função que faça o OHE para mim. Eu não gostei da versão do One Hot Encoder do pacote ml.feature do Pyspark. \n",
    "\n",
    "Por isso decidi incrementar o algoritmo com a minha própria função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7c2c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_ohe = ['JOB', 'MARITAL', 'DEFAULT', 'LOAN', 'POUTCOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38949e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_others = ['AGE','EDUCATION', 'HOUSING', 'CONTACT', 'MONTH', 'DAY_OF_WEEK', 'CAMPAIGN', \n",
    "                  'PDAYS', 'PREVIOUS', 'EMP_VAR_RATE', 'CONS_PRICE_IDX', 'CONS_CONF_IDX', 'EURIBOR3M', \n",
    "                  'EMPLOYED', 'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0358c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função expandir as colunas que foram geradas pelo OneHotEncoding.\n",
    "def __applyOHE(df, column):\n",
    "    \n",
    "    alias = f'_{column}'\n",
    "    df_col_onehot = df.select('*', vector_to_array(column).alias(alias))\n",
    "\n",
    "    num_categories = len(df_col_onehot.first()[alias])\n",
    "    cols_expanded = [(col(alias)[i]) for i in range(num_categories)]\n",
    "    df_cols_onehot = df_col_onehot.select('*', *cols_expanded)\n",
    "    \n",
    "    return df_cols_onehot, cols_expanded\n",
    "\n",
    "\n",
    "\n",
    "def myOneHotEncoder(df, list_columns, list_others_columns):\n",
    "    # LabelEncoder\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"OHE_{0}\".format(c)) for c in list_columns]\n",
    "\n",
    "    # Criação do array de Encoders\n",
    "    encoders = [OneHotEncoder(\n",
    "            inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"_{0}\".format(indexer.getOutputCol())) \n",
    "        for indexer in indexers]\n",
    "    \n",
    "    # Aplica os dois métodos: LabelEncoder e OneHotEncoder\n",
    "    pipeline = Pipeline(stages=indexers + encoders)\n",
    "    df_temp = pipeline.fit(df).transform(rdd_df_encoded)\n",
    "\n",
    "    # Obtendo todas as colunas de saída do Encoder\n",
    "    encoder_columns = [encoder.getOutputCol() for encoder in encoders]\n",
    "\n",
    "    # Expandindo o array gerado pelo OneHotEncoding para Colunas individuais\n",
    "    ohe_columns = []\n",
    "    for column in encoder_columns: \n",
    "        df_temp, _ohe = __applyOHE(df_temp, column)\n",
    "\n",
    "        ohe_columns.extend(_ohe)\n",
    "    \n",
    "    df_res = df_temp.select(*list_others_columns, *ohe_columns)\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9382ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = myOneHotEncoder(rdd_df_encoded, columns_ohe, columns_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27385436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-------+-----+-----------+--------+-----+--------+------------+--------------+-------------+---------+--------+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+-------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-----------------+-----------------+\n",
      "|AGE|EDUCATION|HOUSING|CONTACT|MONTH|DAY_OF_WEEK|CAMPAIGN|PDAYS|PREVIOUS|EMP_VAR_RATE|CONS_PRICE_IDX|CONS_CONF_IDX|EURIBOR3M|EMPLOYED|TARGET|__OHE_JOB[0]|__OHE_JOB[1]|__OHE_JOB[2]|__OHE_JOB[3]|__OHE_JOB[4]|__OHE_JOB[5]|__OHE_JOB[6]|__OHE_JOB[7]|__OHE_JOB[8]|__OHE_JOB[9]|__OHE_JOB[10]|__OHE_MARITAL[0]|__OHE_MARITAL[1]|__OHE_MARITAL[2]|__OHE_DEFAULT[0]|__OHE_DEFAULT[1]|__OHE_LOAN[0]|__OHE_LOAN[1]|__OHE_POUTCOME[0]|__OHE_POUTCOME[1]|\n",
      "+---+---------+-------+-------+-----+-----------+--------+-----+--------+------------+--------------+-------------+---------+--------+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+-------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-----------------+-----------------+\n",
      "| 56|        1|      1|      1|    4|          0|       1|  999|       0|         1.1|        93.994|        -36.4|    4.857|  5191.0|     0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         1.0|         0.0|          0.0|             1.0|             0.0|             0.0|             1.0|             0.0|          1.0|          0.0|              1.0|              0.0|\n",
      "+---+---------+-------+-------+-----+-----------+--------+-----+--------+------------+--------------+-------------+---------+--------+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+-------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-----------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_res.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219670c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_3.4\">3.2) Alterando o tipo de Dado</h3>\n",
    "\n",
    "Lembra que quando carregamos os dados eles vieram como uma lista de Strings? Pois bem, quando eu fiz o split dos dados e separei cada valor para seu respectivo atributo, o que aconteceu foi que os dados continuaram como sendo do tipo string.\n",
    "\n",
    "Por isso irei aplicar o cast para converter os dados para os tipos `inteiro` e `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c0a08cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AGE', 'string'),\n",
       " ('EDUCATION', 'bigint'),\n",
       " ('HOUSING', 'bigint'),\n",
       " ('CONTACT', 'bigint'),\n",
       " ('MONTH', 'bigint'),\n",
       " ('DAY_OF_WEEK', 'bigint'),\n",
       " ('CAMPAIGN', 'string'),\n",
       " ('PDAYS', 'string'),\n",
       " ('PREVIOUS', 'string'),\n",
       " ('EMP_VAR_RATE', 'string'),\n",
       " ('CONS_PRICE_IDX', 'string'),\n",
       " ('CONS_CONF_IDX', 'string'),\n",
       " ('EURIBOR3M', 'string'),\n",
       " ('EMPLOYED', 'string'),\n",
       " ('TARGET', 'bigint'),\n",
       " ('__OHE_JOB[0]', 'double'),\n",
       " ('__OHE_JOB[1]', 'double'),\n",
       " ('__OHE_JOB[2]', 'double'),\n",
       " ('__OHE_JOB[3]', 'double'),\n",
       " ('__OHE_JOB[4]', 'double'),\n",
       " ('__OHE_JOB[5]', 'double'),\n",
       " ('__OHE_JOB[6]', 'double'),\n",
       " ('__OHE_JOB[7]', 'double'),\n",
       " ('__OHE_JOB[8]', 'double'),\n",
       " ('__OHE_JOB[9]', 'double'),\n",
       " ('__OHE_JOB[10]', 'double'),\n",
       " ('__OHE_MARITAL[0]', 'double'),\n",
       " ('__OHE_MARITAL[1]', 'double'),\n",
       " ('__OHE_MARITAL[2]', 'double'),\n",
       " ('__OHE_DEFAULT[0]', 'double'),\n",
       " ('__OHE_DEFAULT[1]', 'double'),\n",
       " ('__OHE_LOAN[0]', 'double'),\n",
       " ('__OHE_LOAN[1]', 'double'),\n",
       " ('__OHE_POUTCOME[0]', 'double'),\n",
       " ('__OHE_POUTCOME[1]', 'double')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e62e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_res.withColumn('TARGET', col('TARGET').cast(IntegerType()))\n",
    "df_res = df_res.withColumn('EDUCATION', col('EDUCATION').cast(IntegerType()))\n",
    "df_res = df_res.withColumn('HOUSING', col('HOUSING').cast(IntegerType()))\n",
    "df_res = df_res.withColumn('CONTACT', col('CONTACT').cast(IntegerType()))\n",
    "df_res = df_res.withColumn('MONTH', col('MONTH').cast(IntegerType()))\n",
    "df_res = df_res.withColumn('DAY_OF_WEEK', col('DAY_OF_WEEK').cast(IntegerType()))\n",
    "\n",
    "df_res = df_res.withColumn('AGE', col('AGE').cast(IntegerType()))\n",
    "df_res = df_res.withColumn('CAMPAIGN', col('CAMPAIGN').cast(IntegerType()))\n",
    "df_res = df_res.withColumn('PDAYS', col('PDAYS').cast(IntegerType()))\n",
    "df_res = df_res.withColumn('PREVIOUS', col('PREVIOUS').cast(IntegerType()))\n",
    "\n",
    "df_res = df_res.withColumn('EMP_VAR_RATE', col('EMP_VAR_RATE').cast(FloatType()))\n",
    "df_res = df_res.withColumn('CONS_PRICE_IDX', col('CONS_PRICE_IDX').cast(FloatType()))\n",
    "df_res = df_res.withColumn('CONS_CONF_IDX', col('CONS_CONF_IDX').cast(FloatType()))\n",
    "df_res = df_res.withColumn('EURIBOR3M', col('EURIBOR3M').cast(FloatType()))\n",
    "df_res = df_res.withColumn('EMPLOYED', col('EMPLOYED').cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d565af",
   "metadata": {},
   "source": [
    "Agora que todos os dados são numéricos, quero visualizar a correlação de cada atributo com a variável TARGET. Em seguida, selecionar somente os atributos cuja correlação seja maior que o valor definido (threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d03ec7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column AGE Corr : 0.030398803040002937\n",
      "Column EDUCATION Corr : 0.03180033903332563\n",
      "Column HOUSING Corr : -0.011742938367144051\n",
      "Column CONTACT Corr : -0.14477305571199917\n",
      "Column MONTH Corr : 0.037186838842473466\n",
      "Column DAY_OF_WEEK Corr : 0.010050675806852068\n",
      "Column CAMPAIGN Corr : -0.0663574147546701\n",
      "Column PDAYS Corr : -0.3249144776166333\n",
      "Column PREVIOUS Corr : 0.23018100321659105\n",
      "Column EMP_VAR_RATE Corr : -0.29833442990618103\n",
      "Column CONS_PRICE_IDX Corr : -0.13621048389858517\n",
      "Column CONS_CONF_IDX Corr : 0.054877973090697106\n",
      "Column EURIBOR3M Corr : -0.3077714036786201\n",
      "Column EMPLOYED Corr : -0.3546782342326573\n",
      "Column TARGET Corr : 1.0\n",
      "Column __OHE_JOB[0] Corr : 0.0314260139826668\n",
      "Column __OHE_JOB[1] Corr : -0.07442328716829617\n",
      "Column __OHE_JOB[2] Corr : -0.006148639356561313\n",
      "Column __OHE_JOB[3] Corr : -0.03230086750341449\n",
      "Column __OHE_JOB[4] Corr : -0.0004188620737695703\n",
      "Column __OHE_JOB[5] Corr : 0.09222084296125596\n",
      "Column __OHE_JOB[6] Corr : -0.016643882021650503\n",
      "Column __OHE_JOB[7] Corr : -0.004662544896685427\n",
      "Column __OHE_JOB[8] Corr : -0.006504932342859821\n",
      "Column __OHE_JOB[9] Corr : 0.01475189557285274\n",
      "Column __OHE_JOB[10] Corr : 0.09395498918093849\n",
      "Column __OHE_MARITAL[0] Corr : -0.04341593866718437\n",
      "Column __OHE_MARITAL[1] Corr : 0.054154174228272664\n",
      "Column __OHE_MARITAL[2] Corr : -0.010608045255554966\n",
      "Column __OHE_DEFAULT[0] Corr : 0.09932745551302603\n",
      "Column __OHE_DEFAULT[1] Corr : -0.09927635198722823\n",
      "Column __OHE_LOAN[0] Corr : 0.005123050777053014\n",
      "Column __OHE_LOAN[1] Corr : -0.00446611744048821\n",
      "Column __OHE_POUTCOME[0] Corr : -0.19350684552056044\n",
      "Column __OHE_POUTCOME[1] Corr : 0.031798732674044095\n"
     ]
    }
   ],
   "source": [
    "selected_columns = []\n",
    "\n",
    "for column in df_res.columns:\n",
    "    corr = df_res.corr('TARGET', column)\n",
    "    print(f'Column {column} Corr : {corr}')\n",
    "    \n",
    "    if abs(corr) > 0.2 and column != 'TARGET':\n",
    "        selected_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "225ba672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------+---------+--------+------+\n",
      "|PDAYS|PREVIOUS|EMP_VAR_RATE|EURIBOR3M|EMPLOYED|TARGET|\n",
      "+-----+--------+------------+---------+--------+------+\n",
      "|  999|       0|         1.1|    4.857|  5191.0|     0|\n",
      "|  999|       0|         1.1|    4.857|  5191.0|     0|\n",
      "|  999|       0|         1.1|    4.857|  5191.0|     0|\n",
      "|  999|       0|         1.1|    4.857|  5191.0|     0|\n",
      "|  999|       0|         1.1|    4.857|  5191.0|     0|\n",
      "+-----+--------+------------+---------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_res1 = df_res.select(*selected_columns, 'TARGET')\n",
    "df_res1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cb72ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PDAYS', 'PREVIOUS', 'EMP_VAR_RATE', 'EURIBOR3M', 'EMPLOYED']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415d203",
   "metadata": {},
   "source": [
    "Eu poderia começar a modelagem dos dados utilizando somente essas colunas, mas quero testar uma versão considerandos todos os atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c9093",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_3.5\">3.3) Conversão para Vetor Denso</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b9267c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo uma lista com todas as colunas, exceto a coluna target\n",
    "list_columns = [column for column in df_res.columns if column != 'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5249c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformaVar(row, list_columns) :\n",
    "    # Criando uma lista com o valor de cada atributo do Row\n",
    "    list_row = [row[i] for i in list_columns]\n",
    "    \n",
    "    # Criação de uma Tupla com dois valores\n",
    "    # O primeiro é a variável alvo: TARGET;\n",
    "    # Em seguida, a criação de um Vetor Denso com todos os valores da Row\n",
    "    obj = (float(row['TARGET']), Vectors.dense(list_row))\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b74dc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0,\n",
       "  DenseVector([56.0, 1.0, 1.0, 1.0, 4.0, 0.0, 1.0, 999.0, 0.0, 1.1, 93.994, -36.4, 4.857, 5191.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_processing = df_res.rdd.map(lambda x: transformaVar(x, list_columns))\n",
    "\n",
    "rdd_processing.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "790af685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|TARGET|            FEATURES|\n",
      "+------+--------------------+\n",
      "|   0.0|[56.0,1.0,1.0,1.0...|\n",
      "|   0.0|[57.0,4.0,1.0,1.0...|\n",
      "|   0.0|[37.0,4.0,0.0,1.0...|\n",
      "|   0.0|[40.0,2.0,1.0,1.0...|\n",
      "|   0.0|[56.0,4.0,1.0,1.0...|\n",
      "|   0.0|[45.0,3.0,1.0,1.0...|\n",
      "|   0.0|[59.0,5.0,1.0,1.0...|\n",
      "|   0.0|[41.0,-1.0,1.0,1....|\n",
      "|   0.0|[24.0,5.0,0.0,1.0...|\n",
      "|   0.0|[25.0,4.0,0.0,1.0...|\n",
      "+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[TARGET: double, FEATURES: vector]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processing = spSession.createDataFrame(rdd_processing, [\"TARGET\",\"FEATURES\"])\n",
    "df_processing.show(10)\n",
    "df_processing.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad928a1a",
   "metadata": {},
   "source": [
    "Agora temos a primeira versão dos dados que serão submetidos ao algoritmo de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1800f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_3.5\">3.4) Escala dos Dados</h3>\n",
    "\n",
    "Temos que deixar os dados todos em uma mesma escala, se não o modelo vai ficar doido, tendo que encontrar a relação entre atributos com valores 0s e 1s e outros atributos com valores 99999 e 100000.\n",
    "\n",
    "O modelo não vai apresentar erro (exception), mas ele vai aprender errado sobre os dados, dando muito mais importância para valores muito altos do que os valores baixos, principalmente os modelos lineares.\n",
    "\n",
    "Existem vários algoritmos para fazer a escala dos dados: MinMax, Standard, Robust, Quantile, etc.. É uma verdadeira ciência! Para esse projeto eu selecionei o RobustScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0647734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler(inputCol='FEATURES', outputCol='FEATURES_SCALED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ff85f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_3.6\">3.5) Divisão em Treino, Teste e Validação</h3>\n",
    "\n",
    "Irei dividir o conjunto de dados em três partes:\n",
    "1. Dados para treinamento: 78%;\n",
    "2. Dados para testes: 20%;\n",
    "3. Dados para validações: 2%;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e74f207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32281, 8124, 783)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dados de Treino e de Teste\n",
    "(dados_treino, dados_teste, dados_valid) = df_processing.randomSplit([0.78, 0.2, 0.02])\n",
    "\n",
    "dados_treino.count(), dados_teste.count(), dados_valid.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59449c0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_3.7\">3.6) Balanceamento de classes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aed67e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|TARGET|count|\n",
      "+------+-----+\n",
      "|   0.0|28596|\n",
      "|   1.0| 3685|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dados_treino.groupBy('TARGET').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a753e",
   "metadata": {},
   "source": [
    "Veja que a diferença entre a quantidade para cada saída da variável target é muito grande. Isso reflete a realidade. Afinal de contas, a quantidade de pessoas que realmente fazem um empréstimo após uma chamada de telefone do setor de Marketing do banco é pequena.\n",
    "\n",
    "Mas o que isso impacta no nosso modelo?<br/>\n",
    "Simples, o algoritmo vai aprender muito sobre os dados do grupo majoritário e pouco sobre os dados do grupo minoritário. Por isto, devemos aplicar alguma técnica para tratar este problema.\n",
    "\n",
    "1. Podemos simplesmente remover os dados do grupo majoritário de modo a balancear as classes. Mas como a diferença é enorme, neste caso perderíamos muita informação relevante.\n",
    "2. Podemos preencher o grupo minoritário com dados sintéticos. Existem algumas formas de fazer isso. Eu irei aplicar a mais simples de todas, multiplicar os registros existentes de modo que as classes fiquem balanceadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a483f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanceClasses(df, target):\n",
    "    # Separando os dataframes por classe target\n",
    "    df_target_1 = df[df[target] == 1]\n",
    "    df_target_0 = df[df[target] == 0]\n",
    "    \n",
    "    # Obtendo a proporção entre os datasets. \n",
    "    # Arredondei essa proporção para baixo para que a classe minoritária não ultrapasse a classe majoritária.\n",
    "    fraction = float(floor(df_target_0.count() / df_target_1.count()))\n",
    "\n",
    "    # Multiplicando os valores utilizando o Sample (Amostragem) com reposição\n",
    "    # Nota: O 123 é somente um seed qualquer para a aleatoriedade possa ser replicada\n",
    "    df_target_1_sample = df_target_1.sample(True, fraction, 123)\n",
    "    \n",
    "    # A amostragem pode ter desconsiderado de maneira aleatória algum registro real\n",
    "    # Por isso eu quero incluir além dos valores sintéticos gerados, os valores reais\n",
    "    # mas sem ultrapassar a quantidade de dados do grupo majoritário\n",
    "    diff = df_target_1_sample.count() - df_target_1.count()\n",
    "\n",
    "    # Convertendo os dados um dataframe\n",
    "    df_temp = spSession.createDataFrame(df_target_1_sample.collect()[0:diff])\n",
    "    # Unindo os dados sintéticos com os dados reais\n",
    "    df_temp = df_temp.unionAll(df_target_1)\n",
    "    \n",
    "    # Unindo os dois dataframes finais\n",
    "    df =  df_temp.unionAll(df_target_0)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7e27e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino = balanceClasses(dados_treino, 'TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4b21b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54245\n",
      "+------+-----+\n",
      "|TARGET|count|\n",
      "+------+-----+\n",
      "|   0.0|28596|\n",
      "|   1.0|25649|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dados_treino.count())\n",
    "\n",
    "dados_treino.groupBy('TARGET').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080a83b",
   "metadata": {},
   "source": [
    "Agora sim. Temos as classes muito mais balanceadas. Com isso, o modelo vai aprender igualmente sobre os dados. Você pode se perguntar... Mas o modelo não vai aprender muito mais sobre os dados duplicados do que os demais?\n",
    "\n",
    "A resposta é sim. Em computação não é possível somente ganhar, sempre quando cobrimos um lado, o outro é descoberto. Contudo, o modelo ainda sim vai aprender melhor sobre os dados como um todo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c40cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_3.8\">3.7) Feature Selection</h3>\n",
    "\n",
    "O processo de Feature Selection é o processo de seleção de variáveis que são relevantes para o modelo de machine learning. A seleção de variáveis tem como objetivo reduzir a dimensão dos dados de entrada, através da identificação das variáveis que são mais interessantes para o modelo e descartando as menos interessantes. Isso é importante por vários aspectos:\n",
    "* Simplificação do modelo: Aquele ditado \"Menos é mais\" é muito verdadeiro quando falamos de modelos de Machine Learning. Afinal, quanto mais simples for o modelo, melhor será a performance, manutenção e entendimento.\n",
    "* Economia de tempo e recursos: Imagina treinar um modelo com centenas de variáveis. Isso pode se tornar algo extramamente inviável, dado o tempo que poderia levar, além dos recursos computacionais demandados para tal tarefa.\n",
    "\n",
    "Para selecionar os atributos mais relevantes eu optei por utilizar o modelo de Random Forest. Nesse caso eu não estou preocupado com realizar previsões, mas sim, em selecionar os atributos que obtiverem maior pontuação. E com isso, selecioná-los para o modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01d0a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando a classe RandomForest\n",
    "dtClassifer = RandomForestClassifier(labelCol = \"TARGET\", featuresCol = \"FEATURES_SCALED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e73eef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação do pipeline de execução com os seguintes passos: Escala dos dados e em seguida o treina o classificador.\n",
    "pipeline = Pipeline(stages=[scaler, dtClassifer])\n",
    "# Executando o pipeline\n",
    "model = pipeline.fit(dados_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eaabc504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMPLOYED',\n",
       " 'EMP_VAR_RATE',\n",
       " 'EURIBOR3M',\n",
       " 'CONTACT',\n",
       " 'CONS_CONF_IDX',\n",
       " 'CONS_PRICE_IDX',\n",
       " 'AGE',\n",
       " 'MONTH']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pegando todas as pontuações para cada atributo do modelo\n",
    "features_importances = model.stages[-1].featureImportances\n",
    "\n",
    "# Criação de uma lista linkando todas as pontuações com os respectivos nomes de atributos\n",
    "features_importances = list(sorted(zip(list_columns, features_importances), key= lambda x: x[1], reverse=True))\n",
    "\n",
    "# Selecionando somente os atributos cujo o score de pontuação seja maior que 0.03\n",
    "cols_importants = [column[0] for column in features_importances if column[1] > 0.03]\n",
    "\n",
    "# Exibindo os atributos mais importantes\n",
    "cols_importants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81e352",
   "metadata": {},
   "source": [
    "Com os atributos mais relevantes definidos, eu irei aplicar novamente o passo de pré-processamento dos dados, mas dessa vez somente com os atributos mais relevantes para o modelo.\n",
    "\n",
    "Além disso, irei incluir os atributos que obtiveram uma boa correlação com a variável TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8834f061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREVIOUS',\n",
       " 'AGE',\n",
       " 'EURIBOR3M',\n",
       " 'PDAYS',\n",
       " 'CONS_CONF_IDX',\n",
       " 'EMPLOYED',\n",
       " 'CONS_PRICE_IDX',\n",
       " 'EMP_VAR_RATE',\n",
       " 'CONTACT',\n",
       " 'MONTH']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_importants.extend(selected_columns)\n",
    "\n",
    "cols_importants = list(set(cols_importants))\n",
    "cols_importants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8af39c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0,\n",
       "  DenseVector([0.0, 56.0, 4.857, 999.0, -36.4, 5191.0, 93.994, 1.1, 1.0, 4.0]))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_processing = df_res.rdd.map(lambda x: transformaVar(x, cols_importants))\n",
    "\n",
    "rdd_processing.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4f422be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processing = spSession.createDataFrame(rdd_processing, [\"TARGET\",\"FEATURES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c182f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de Treino e de Teste\n",
    "(dados_treino, dados_teste, dados_valid) = df_processing.randomSplit([0.78, 0.2, 0.02])\n",
    "\n",
    "# Realiza o balaceamento das classes\n",
    "dados_treino = balanceClasses(dados_treino, 'TARGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3da36",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 id=\"id_4\">4) MODELAGEM</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91171b6f",
   "metadata": {},
   "source": [
    "<h3 id=\"id_4.1\">4.1) Modelagem Base</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46e2fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models = [\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'model': RandomForestClassifier(labelCol = \"TARGET\", featuresCol = \"FEATURES_SCALED\")\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'name': 'Gradient Boosting',\n",
    "        'model': GBTClassifier(labelCol = \"TARGET\", featuresCol = \"FEATURES_SCALED\",\n",
    "                              maxDepth=10, maxBins=32, maxMemoryInMB=512)\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'name': 'Linear SVM',\n",
    "        'model': LinearSVC(labelCol = \"TARGET\", featuresCol = \"FEATURES_SCALED\")\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'name': 'Logist Regression',\n",
    "        'model': LogisticRegression(labelCol = \"TARGET\", featuresCol = \"FEATURES_SCALED\")\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9b0af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando o modelo: Random Forest\n",
      "O modelo obteve pontuação na métrica F1 Score de: 0.8504962213419764\n",
      "Treinando o modelo: Gradient Boosting\n",
      "O modelo obteve pontuação na métrica F1 Score de: 0.8366225760633074\n",
      "Treinando o modelo: Linear SVM\n",
      "O modelo obteve pontuação na métrica F1 Score de: 0.7674619791103622\n",
      "Treinando o modelo: Logist Regression\n",
      "O modelo obteve pontuação na métrica F1 Score de: 0.7776538296210929\n"
     ]
    }
   ],
   "source": [
    "for m in list_models:\n",
    "    # Criação do pipeline com o modelo atual\n",
    "    pipeline = Pipeline(stages=[scaler, m['model']])\n",
    "    \n",
    "    # Realizando o treinamento\n",
    "    print('Treinando o modelo: {}'.format(m['name']))\n",
    "    model = pipeline.fit(dados_treino)\n",
    "    \n",
    "    \n",
    "    # Previsões com dados de teste\n",
    "    previsoes = model.transform(dados_teste)\n",
    "    \n",
    "    # Avaliando a acurácia\n",
    "    avaliador = MulticlassClassificationEvaluator(predictionCol = \"prediction\", labelCol = \"TARGET\", metricName = \"f1\")\n",
    "    score = avaliador.evaluate(previsoes) \n",
    "    print(f'O modelo obteve pontuação na métrica F1 Score de: {score}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05dfb2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_4.2\">4.2) Aplicando Grid Search e Cross Validation</h3>\n",
    "\n",
    "Agora que sabemos qual foi o melhor algoritmo para a construção do modelo, vamos aplicar técnicas de GridSearch para testar várias combinações de parâmetros. Com isso, selecionar a melhor combinação de parâmetros para o melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1871573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando a class Grandient Boosting Classifier\n",
    "rf = RandomForestClassifier(labelCol=\"TARGET\", featuresCol=\"FEATURES_SCALED\")\n",
    "\n",
    "# Criando o objeto Grid com todos os parâmetros que serão testados\n",
    "# Irei definir poucos parâmetros para que a execução não demore horas\n",
    "rfparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 5, 10])\n",
    "             .addGrid(rf.maxBins, [10, 20, 40])\n",
    "               \n",
    "             .build())\n",
    "\n",
    "# Instanciando o avaliador\n",
    "rfevaluator = BinaryClassificationEvaluator(labelCol=\"TARGET\")\n",
    "\n",
    "# Criação de 5-fold para o Cross Validator\n",
    "rfcv = CrossValidator(estimator = rf,\n",
    "                      estimatorParamMaps = rfparamGrid,\n",
    "                      evaluator = rfevaluator,\n",
    "                      numFolds = 5)\n",
    "\n",
    "\n",
    "# Criação do Pipeline de Execução e realização do treinamento do modelo.\n",
    "pipeline = Pipeline(stages=[scaler, rfcv])\n",
    "model = pipeline.fit(dados_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b073f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 id=\"id_5\">5) AVALIAÇÃO</h2>\n",
    "\n",
    "<h3 id=\"id_5.1\">5.1) Avaliação do Modelo</h3>\n",
    "\n",
    "Para avaliar o modelo eu selecionei a métrica F1 Score, porque em geral, é uma métrica melhor que a Acurácia. Além disso eu quero ter uma noção de como os Falsos Positivos e Falsos Negativos estão impactando o modelo.\n",
    "\n",
    "Para entender mais sobre as métricas de avaliação em modelos de classificação, eu recomendo a leitura do artigo: [Machine Learning: métricas para Modelos de Classificação](https://imasters.com.br/desenvolvimento/machine-learning-metricas-para-modelos-de-classificacao) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7afc904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7776538296210929"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avaliando a acurácia\n",
    "avaliador = MulticlassClassificationEvaluator(predictionCol = \"prediction\", labelCol = \"TARGET\", metricName = \"f1\")\n",
    "avaliador.evaluate(previsoes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9f0e5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+\n",
      "|TARGET|PREDICTION|count|\n",
      "+------+----------+-----+\n",
      "|   1.0|       1.0|  672|\n",
      "|   0.0|       1.0| 1938|\n",
      "|   1.0|       0.0|  285|\n",
      "|   0.0|       0.0| 5419|\n",
      "+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resumindo as previsões - Confusion Matrix\n",
    "previsoes.groupBy(\"TARGET\",\"PREDICTION\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf88c89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_5.2\">5.2) Salvando o modelo</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "409970b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando o modelo no diretório local\n",
    "model.write().overwrite().save('models/modelo_v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b539f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_5.3\">5.3) Carregando o modelo</h3>\n",
    "\n",
    "Para finalizar, vamos carregar novamente o modelo e testar com os dados que foram separados para validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef695a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0c52765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o modelo do diretório.\n",
    "model_load = PipelineModel.load('models/modelo_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4abccc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando previsões\n",
    "preds_valid = model_load.transform(dados_valid)\n",
    "\n",
    "# Avaliando resultados\n",
    "avaliador = MulticlassClassificationEvaluator(predictionCol = \"prediction\", labelCol = \"TARGET\", metricName = \"f1\")\n",
    "score = avaliador.evaluate(preds_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "62081395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisão do modelo com dados de validação 83.43%\n"
     ]
    }
   ],
   "source": [
    "print(f'Precisão do modelo com dados de validação {round(score * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd08f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+\n",
      "|TARGET|PREDICTION|count|\n",
      "+------+----------+-----+\n",
      "|   1.0|       1.0|   71|\n",
      "|   0.0|       1.0|  109|\n",
      "|   1.0|       0.0|   38|\n",
      "|   0.0|       0.0|  584|\n",
      "+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resumindo as previsões - Confusion Matrix\n",
    "preds_valid.groupBy(\"TARGET\",\"PREDICTION\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e044d0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 id=\"id_6\">6) FINAL</h2>\n",
    "\n",
    "<h3 id=\"id_6.1\">6.1) Considerações Finais</h3>\n",
    "\n",
    "Voltando ao objetivo central do projeto. É possível prever se o cliente vai adquirir um empréstimo?\n",
    "\n",
    "**Resposta**: Com o modelo desenvolvido, conseguimos prever se o cliente vai ou não adquirir um empréstimo em 86% dos casos. Agora a equipe de telemarketing pode direcionar os esforços principalmente nos clientes cujo modelo realizar a previsão igual a 1. Isto é, os clientes que tem a maior probabilidade de contrairem um empréstimo.\n",
    "\n",
    "Além disso, a equipe de marketing pode traçar estratégias para os clientes que têm pouca probabilidade de contrairem um empréstimo, com o intuito de atrair novos clientes. Mas isso é tema para outro projeto. E se você quiser ter uma referência de como seria um projetos desses na prática, você pode visitar este outro projeto que eu desenvolvi: [Marketing Analytics](https://github.com/Krupique/challenges-kaggle/tree/main/challenges/13_MarketingAnalytics)\n",
    "\n",
    "Lá eu mostro como realizar uma segmentação estatística e hierárquica para equipe de marketing, segmentação gerencial para equipe de vendas, previsão de faturamento, cálculo do Life Time Value (LTV) dos clientes e muito mais utilizando linguagem de programação Python.\n",
    "\n",
    "---\n",
    "\n",
    "Ao longo deste projeto eu apliquei diversas técnicas em todas as etapas de um projeto de Data Science. Tentei ao máximo utilizar somente os recursos disponíveis no Framework do Spark, tornando o projeto independente de outras bibliotecas.\n",
    "\n",
    "Fiz questão de detalhar ao máximo todas as técnicas e conceitos envolvidos para que possa servir como fonte de estudos e referência para quem deseja trabalhar com este framework. Vale ressaltar que o PySpark é mais complexo que os outros pacotes da linguagem Python, portanto, a curva de aprendizado nessa biblioteca é um pouco maior. Mas é natural, afinal, o propósito desse framework é aplicar a computação em larga escala para ambientes distribuídos e lidar com o Big Data.\n",
    "\n",
    "---\n",
    "\n",
    "<h3 id=\"id_6.2\">6.2) Agradecimentos</h3>\n",
    "\n",
    "Para quem chegou até aqui, muito obrigado por acompanhar este conteúdo. Me coloco a disposição para esclarecer eventuais dúvidas. Segue o contato das minhas redes:\n",
    "\n",
    "* Email: <a href=\"mailto:krupck@outlook.com\">krupck@outlook.com</a>\n",
    "* Linkedin: <a href=\"https://www.linkedin.com/in/henrique-krupck/\">henrique-krupck</a>\n",
    "\n",
    "At.te,\n",
    "\n",
    "Henrique K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd20d16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 id=\"id_6.3\">6.3) Referências</h3>\n",
    "\n",
    "* O que é marketing?. *Resultados Digitais*. 2020. Disponível em: [o que é marketing](https://resultadosdigitais.com.br/o-que-e-marketing/)\n",
    "\n",
    "\n",
    "* BALDISSERA, Olívia. *4 benefícios do machine learning no marketing. Quantos você conhece?*. 2021. Disponível em: [4 benefícios de ML no marketing](https://posdigital.pucpr.br/blog/machine-learning-no-marketing)\n",
    "\n",
    "\n",
    "* KRUPCK, Henrique. *Marketing Analytics em Python*. 2022. Disponível em: [Marketing Analytics](https://github.com/Krupique/challenges-kaggle/tree/main/challenges/13_MarketingAnalytics)\n",
    "\n",
    "\n",
    "* LIV. *Machine learning in banking and marketing applications for 2021 and beyond*. 2020. Disponível em [Machine learning in banking and marketing](https://roboticsandautomationnews.com/2020/11/16/machine-learning-in-banking-and-marketing-applications-for-2021-and-beyond/38220/)\n",
    "\n",
    "\n",
    "* XP EDUCAÇÃO. *Spark: entenda sua função e saiba mais sobre essa ferramenta*. 2022. Disponível em: [Saiba como funciona o Apache Spark](https://blog.xpeducacao.com.br/apache-spark/)\n",
    "\n",
    "\n",
    "* DePINO, Frank. *Modern Bank Marketing – A Comprehensive Guide*. 2021. Disponível em: [Modern Bank Marketing](https://mediaboom.com/news/bank-marketing/#:~:text=Bank%20marketing%20is%20the%20practice,attracted%20to%20a%20certain%20institutions.)\n",
    "\n",
    "\n",
    "* SCHADE, Gabriel. *Machine Learning: métricas para Modelos de Classificação*. 2019. Disponível em: [Métricas de Avaliação](https://imasters.com.br/desenvolvimento/machine-learning-metricas-para-modelos-de-classificacao)\n",
    "\n",
    "\n",
    "* ML Pipelines. *Machine Learning Pipelines with MLlib*. Disponível em: (ML Pipelines)[https://spark.apache.org/docs/latest/ml-pipeline.html]\n",
    "\n",
    "\n",
    "* WAN, Jun. *Oversampling and Undersampling with PySpark*. 2020. Disponível em: [Oversampling and Undersampling](https://medium.com/@junwan01/oversampling-and-undersampling-with-pyspark-5dbc25cdf253)\n",
    "\n",
    "\n",
    "* ERSOY, Pinar. *Types of Samplings in PySpark 3*. 2020. Disponível em: [Types of Samplings](https://towardsdatascience.com/types-of-samplings-in-pyspark-3-16afc64c95d4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94342fd7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Fim."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
