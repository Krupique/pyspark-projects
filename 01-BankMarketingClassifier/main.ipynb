{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e70e274",
   "metadata": {},
   "source": [
    "# Bank Marketing Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa6e2e",
   "metadata": {},
   "source": [
    "### Introdução ao tema\n",
    "\n",
    "O marketing está presente nas nossas vidas muito mais do que imaginamos. Faça uma caminhada pelas ruas da cidade, uma busca no seu navegador, ligue a televisão ou o rádio, abra sua rede social e você será impactado por alguma ação de marketing.\n",
    "\n",
    "Mas o que é marketing?\n",
    "Para Philip Kotler, um dos teóricos mais renomados da área, define marketing como:\n",
    "> *Marketing é a ciência e arte de explorar, criar e proporcionar valor para satisfazer necessidades de um público-alvo com rendibilidade.*\n",
    "\n",
    "O campo do marketing é vasto e inclui não apenas o ato de vender um produto ou serviço, mas tudo relacionado ao planejamento, pesquisa e posicionamento de mercado. Em outras palavras, pode-se dizer que o marketing é como uma balança entre o que os clientes desejam e os objetivos da empresa. Afinal, um bom marketing precisa criar valor para ambas as partes: para a empresa e para o consumidor.\n",
    "\n",
    "Vale ressaltar que marketing é uma palavra em inglês, derivada de market (mercado). Portanto, marketing não é apenas vender produtos ou serviços, engloba também outras atividades relacionadas ao mercado.\n",
    "\n",
    "**Bank Marketing**<br/>\n",
    "\n",
    "Um tipo de instituição que aplica marketing no seu dia a dia são os bancos, para isso, damos o nome Bank Marketing (Marketing Bancário). O marketing bancário é a prática de atrair e adquirir novos clientes por meio de estratégias de mídia tradicional e mídia digital. O uso dessas estratégias de mídia ajuda a determinar que tipo de cliente é atraído por uma determinada instituição. Isso também inclui diferentes instituições bancárias que usam propositalmente diferentes estratégias para atrair o tipo de cliente com o qual desejam fazer negócios.\n",
    "\n",
    "E você sabe como as principais empresas fazem atualmente para aplicar estratégias de Marketing?<br/>\n",
    "Se você respondeu, \"elas aplicam técnicas de Inteligência Artificial e Machine Learning para entender e avaliar o comportamento dos seus clientes\". Parabéns, você acertou!\n",
    "Mas antes de explicar com elas fazem isso, vamos começar entendendo um pouco sobre o que é Machine Learning.\n",
    "\n",
    "**Machine Learning**<br/>\n",
    "O Machine Learning, ou aprendizado de máquina. É um subcampo da inteligência artificial que permite dar aos computadores a habilidade de aprender sem que sejam explicitamente programados para isso. Ela permite que computadores tomem decisões e interpretem dados de maneira automática, a partir de algoritmos. Temos vários tipos de aprendizagem, são elas: Supervisionada, não supervisionada, semi supervisionada, aprendizagem por reforço e deep learning.\n",
    "\t\n",
    "Os algoritmos de aprendizagem de máquina, aprendem a induzir uma função ou hipótese capaz de resolver um problema a partir de dados que representam instâncias do problema a ser resolvido.\n",
    "\n",
    "Um algoritmo é uma sequência finita de ações e regras que visam a solucionar um problema. Cada um deles aciona um diferente tipo de operação ao entrar em contato com os dados que o computador recebe. O resultado de todas as operações é o que possibilita o aprendizado da máquina.\n",
    "\n",
    "Dessa forma, as máquinas aperfeiçoam as tarefas executadas, por meio de processamento de dados como imagens e números. Por isso o machine learning depende do Big Data para ser efetivo. O Big Data, por sua vez pode ser entendido de maneira simplória como uma imensa quantidade de dados. Mas calma, ainda irei falar mais em detalhes sobre isso. Por ora, vamos entender como o Machine Learning e a Inteligência Artificial ajudam a benefeciar a área de Bank Marketing.\n",
    "\n",
    "**Machine Learning e o Bank Marketing**<br/>\n",
    "Abaixo irei listar alguns benefícios do Machine Learning (ML) aplicado na área de Bank Marketing:\n",
    "* **Atendimento ao cliente orientado por IA**: Existem muitas maneiras de tornar o atendimento ao cliente realmente orientado por IA ou, melhor dizer, orientado por dados. Por exemplo, com a ajuda da análise de dados, a instituição bancária pode descobrir as intenções de compra do cliente e oferecer um empréstimo flexível. Além disso, os principais bancos criam chatbots inteligentes que ajudam os clientes a interagir melhor com as empresas financeiras. Com a ajuda de aplicativos inteligentes, os clientes podem acompanhar automaticamente seus gastos, planejar seu orçamento e obter sugestões precisas de economia e investimento.\n",
    "* **Segmentação de clientes**: Com ML, é possível encontrar características semelhantes e padrões entre os dados dos clientes. Dessa forma, o algoritmo de ML consegue separar os clientes em grupos, possibilitando que a equipe de Marketing possa direcionar os esforços de maneira individual para cada grupo de clientes.\n",
    "* **Otimização de lances em anúncios**: os anúncios em buscadores funcionam no sistema de leilões de pesquisa. Ou seja, quem der o maior lance aparecerá em primeiro lugar nos resultados de pesquisa para uma determinada palavra-chave. Para fazer o lance perfeito, o marketing se utiliza do machine learning. Ele analisa milhões de dados para ajustar os lances em tempo real.\n",
    "* **Prever os possíveis clientes**: Com base nos dados históricos da empresa, podemos coletar e entender qual é o perfil dos clientes. E com base nisso, prever a probabilidade do indivíduo adquirir determinado serviço. Como por exemplo, contrair um empréstimo, adquirir investimentos, dentre outros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a68bea",
   "metadata": {},
   "source": [
    "**Objetivo do Projeto**<br/>\n",
    "Como objetivo específico do problema de negócio, irei aplicar técnicas de Machine Learning para identificar e prever ser se o cliente vai ou não adquirir o empréstimo no banco.\n",
    "\n",
    "Como objetivo de estudo de tecnologia, estarei utilizando do início ao fim do projeto o framework Apache Spark, mais especificamente, o PySpark. PySpark é uma API Python para Apache SPARK que é denominado como o mecanismo de processamento analítico para aplicações de processamento de dados distribuídos em larga escala e aprendizado de máquina em tempo real, ou seja, para grandes volumes de dados, conhecido como Big Data.\n",
    "\n",
    "**Sobre o Dataset**<br/>\n",
    "Este conjunto de dados contém 20 atributos e 41189 registos relevantes para uma campanha de marketing direto de uma instituição bancária portuguesa. A campanha de marketing foi executada por meio de ligações telefônicas. O objetivo da classificação é prever se o cliente irá aderir (yes/no) ao CDB (variável y).<br/>\n",
    "\n",
    "O dataset pode ser obtido no site do Kaggle, clicando [aqui](https://www.kaggle.com/datasets/ruthgn/bank-marketing-data-set)\n",
    "\n",
    "> **Nota**: Eu alterei alguns registros do dataset para forçar a situação de dados missing/nulos. Para que dessa forma eu consiga desenvolver e detalhar a etapa de tratamento de valores NA.\n",
    "\n",
    "Atributos do Dataset:\n",
    "* AGE: Idade do indivíduo.\n",
    "* JOB: Emprego.\n",
    "* MARITAL: Status de estado civil: ('casado', 'solteiro', 'divorciado').\n",
    "* EDUCATION: Nível de escolaridade\n",
    "* DEFAULT: Se possui crédito inadimplente.\n",
    "* HOUSING: Possui credito à habitação.\n",
    "* LOAN: Se possui empréstimo.\n",
    "* CONTACT: Tipo de contato: ('Telefone' ou 'Smartphone').\n",
    "* MONTH: Mês do ano.\n",
    "* DAY OF WEEK: Dia da semana.\n",
    "* CAMPAIGN: Quantidade de contatos realizados durante esta campanha e para este cliente.\n",
    "* PDAYS: Quantidade de dias que se passaram depois que o cliente foi contatado pela última vez em uma campanha anterior.\n",
    "* PREVIOUS: Quantidade de contatos realizados antes desta campanha e para este cliente.\n",
    "* POUTCOME: Resultado da campanha de marketing anterior ('fracasso', 'inexistente', 'sucesso').\n",
    "* EMP.VAR.RATE: Taxa de variação do emprego - indicador trimestral.\n",
    "* CONS.PRICE.IDX: Índice de preços ao consumidor - indicador mensal.\n",
    "* CONS.CONF.IDX: Índice de confiança do consumidor - indicador mensal.\n",
    "* EURIBOR3M: Taxa de 3 meses euribor - indicador diário.\n",
    "* NR.EMPLOYED: Número de funcionários - indicador trimestral.\n",
    "\n",
    "\n",
    "* Y: Variável alvo. Verifica se o cliente adquiriu o empréstimo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53365659",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Introdução ao Apache Spark\n",
    "\n",
    "Apache Spark é uma estrutura de código aberto que simplifica o desenvolvimento e a eficiência dos trabalhos de análise de dados. Ele oferece suporte a uma ampla variedade de opções de API e linguagem com mais de 80 operadores de transformação e ação de dados que ocultam a complexidade da computação em cluster.\n",
    "\n",
    "Com velocidades relatadas 100 vezes mais rápidas do que mecanismos de análise semelhantes, o Spark pode acessar fontes de dados variáveis e ser executado em várias plataformas, incluindo Hadoop, Apache Mesos, Kubernetes, de forma independente ou na nuvem. Seja processando dados em lote ou streaming, você verá um desempenho de alto nível devido ao agendador Spark DAG de última geração, um otimizador de consulta e um mecanismo de execução física.\n",
    "\n",
    "> Caso você queira saber mais sobre o Apache Spark, eu recomendo fortemente a leitura do artigo \"Spark: entenda sua função e saiba mais sobre essa ferramenta\", publicado pelo blog XP Educação, que pode ser acessado clicando [aqui](https://blog.xpeducacao.com.br/apache-spark/)\n",
    "\n",
    "### PySpark\n",
    "\n",
    "PySpark é a colaboração do Apache Spark e do Python.\n",
    "\n",
    "O Apache Spark é uma estrutura de computação em cluster de código aberto, construída em torno da velocidade, facilidade de uso e análise de streaming, enquanto o Python é uma linguagem de programação de alto nível e de uso geral. Ele fornece uma ampla variedade de bibliotecas e é usado principalmente para Machine Learning e Real-Time Streaming Analytics.\n",
    "\n",
    "Em outras palavras, é uma API Python para Spark que permite aproveitar a simplicidade do Python e o poder do Apache Spark para domar o Big Data. \n",
    "\n",
    "O uso da biblioteca PySpark possui diversas vantagens:\n",
    "* É um mecanismo de processamento distribuído, na memória, que permite o processamento de dados de forma eficiente e utilizando a característica de computação distribuída.\n",
    "* Com o uso do PySpark, é possível o processamento de dados em Hadoop (HDFS), AWS S3 e outros sistemas de arquivos.\n",
    "* Possui quatro grandes funcionalidades: Manipulação e integração SQL, Streaming de Dados, MLlib para Machine Learning e GraphX para manipulação de grafos.\n",
    "* Segundo os desenvolvedores, o Apache Spark é até 100x mais rápido em termos de processamento distribuído quando comparado com o Hadoop. \n",
    "\n",
    "Toda a execução dos scripts são realizados dentro do Apache Spark, que distribui o processamento dentro de um ambiente de cluster que são interligados aos NÓs que realizam a execução e transformação dos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719056b",
   "metadata": {},
   "source": [
    "### Início do Projeto\n",
    "\n",
    "Depois de todas as definições, vamos iniciar o projeto. O conteúdo programático segue a estrutura padrão de projetos desse tipo. Iniciando pelo carregamento dos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f488779",
   "metadata": {},
   "source": [
    "### Data Load and Packages Imports\n",
    "\n",
    "Irei utilizar funções de dois dos quatro principais módulos do Spark. Funções do módulo de SQL e também do MLLib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row #Converte RDDs em objetos do tipo Row\n",
    "from pyspark.sql.functions import col, isnan, when, count # Encontra a contagem para valores None, Null, Nan, etc.\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder #Converte strings em valores numéricos\n",
    "from pyspark.ml.linalg import Vectors #Serve para criar um vetor denso\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # Para avaliar o modelo com as métricas de avaliação.\n",
    "from pyspark.ml.feature import RobustScaler, StandardScaler, MinMaxScaler, Normalizer # Métodos para escalas dos dados\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier, LinearSVC # Algoritmos de ML\n",
    "from pyspark.ml import Pipeline # Criação de um Pipeline de execução.\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f'System Version: {sys.version}')\n",
    "print(f'Spark Context Version: {sc.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd63a54",
   "metadata": {},
   "source": [
    "Ao inicializar o Notebook pelo terminal com o comando PySpark, criamos automaticamente um Contexto Spark (SparkContext). Este é um objeto que define como e onde o Spark acessa o Cluster.\n",
    "\n",
    "Para facilitar nossa vida, irei criar uma Sessão Spark. Esta serve para fornecer uma maneira simples para interagir com várias funcionalidades do Spark com um número menor de constructs. Em vez de ter que criar um contexto Spark, contexto Hive, contexto SQL, agora tudo é encapsulado em uma sessão Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a43136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session - usada quando se trabalha com Dataframes no Spark\n",
    "spSession = SparkSession.builder.master(\"local\").appName(\"PySpark-BankMarketing\").config(\"spark.some.config.option\", \"session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfb2b0",
   "metadata": {},
   "source": [
    "Como citado, quero carregar o dataset que eu propositalmente modifiquei com o intuito de fazer algumas etapas adicionais para tratamento dos dados durante o notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd = sc.textFile('data/bank-marketing-dataset.csv') # Arquivo original\n",
    "rdd = sc.textFile('data/dataset-with-na.csv') # Arquivo modificado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a31928",
   "metadata": {},
   "source": [
    "### Visão Geral sobre os Dados\n",
    "\n",
    "Primeiro uma visão geral sobre os dados e algumas explicações sobre o funcionamento básico do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7467a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2430676",
   "metadata": {},
   "source": [
    "**RDD - Resilient Distributed Datasets**\n",
    "\n",
    "É como uma tabela de banco de dados, é a essência do funcionamento do Spark. É uma coleção de objetos distribuída e imutável, é read-only. Cada conjunto de dados no RDD é dividido em partições lógicas, que podem ser computadas em diferentes nodes do cluster. Existem duas formas de criar o RDD:\n",
    "* Paralelizando uma coleção existente (função sc.parallelize);\n",
    "* Referenciando um dataset externo (HDFS, RDBMS, NoSQL, S3);\n",
    "\n",
    "O Spark utiliza o conceito de RDDs para aplicar o MapReduce de maneira rápida. Por padrão, os RDDs são computados cada vez que executamos uma ação. Entretanto, podemos “persistir” o RDD na memória (ou mesmo no disco) de modo que os dados estejam disponíveis ao longo do cluster e possam ser processados de forma muito mais rápida pelas operações de análise de dados.\n",
    "O RDD suporta dois tipos de operações:\n",
    "\n",
    "<img src=\"resources/tabela01.png\"/>\n",
    "\n",
    "Cada transformação gera um novo RDD, pois os RDDs são imutáveis. As ações aplicam as transformações nos RDDs e retornam o resultado.\n",
    "\n",
    "**Características dos RDDs**:\n",
    "\n",
    "* Spark é baseado em RDDs. Criamos, transformamos e armazenamos RDDs em Spark;\n",
    "* RDD representa uma coleção de elementos de dados particionados que podem ser operados em paralelo.\n",
    "* RDDs são objetos imutáveis. Eles não podem ser alterados uma vez criados.\n",
    "* RDDs podem ser colocados em cache e permitem persistência (mesmo objeto usado entre sessões diferentes).\n",
    "* Ao aplicarmos Transformações em RDDs criamos novos RDDs.\n",
    "* Ações aplicam as transformações nos RDDs e geram um resultado.\n",
    "\n",
    "**Existem dois tipos de transformações:**\n",
    "\n",
    "* *Narrow*: Resultado de funções como map() e filter() e os dados vem de uma única partição.\n",
    "* *Wide*: Resultado de funções como groupByKey() e os dados podem vir de diversas partições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando a quantidade de registros no RDD\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9229b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listando os 5 primeiros registros\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ab08b",
   "metadata": {},
   "source": [
    "Veja que a primeira linha do RDD se trata do cabeçalho da tabela. Além disso, o RDD carregou os dados como uma \"lista de strings\". \n",
    "\n",
    "Portanto, o que eu irei fazer é: Remover a primeira linha que é o cabeçalho. Em seguida fazer o split dos dados utilizando o vírgula como separador. Assim, terei os dados como uma tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = rdd.first()\n",
    "rdd_body = rdd.filter(lambda x: header not in x).map(lambda l: l.split(','))\n",
    "\n",
    "list_columns = header.replace('.', '_').upper().split(',')\n",
    "list_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0dfe83",
   "metadata": {},
   "source": [
    "Agora quero criar o conceito de Rows. Isto é, para cada linha e para cada dado irei atribuir uma chave. Esta chave é justamente o nome das colunas do dataset.\n",
    "\n",
    "Alterei o nome do atributo Y para TARGET de modo a facilitar as próximas etapas do processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_row = rdd_body.map(lambda p: Row(\n",
    "    AGE = p[0], \n",
    "    JOB = p[1], \n",
    "    MARITAL = p[2],\n",
    "    EDUCATION = p[3],\n",
    "    DEFAULT = p[4],\n",
    "    HOUSING = p[5],\n",
    "    LOAN = p[6],\n",
    "    CONTACT = p[7],\n",
    "    MONTH = p[8],\n",
    "    DAY_OF_WEEK = p[9],\n",
    "    CAMPAIGN = p[10],\n",
    "    PDAYS = p[11],\n",
    "    PREVIOUS = p[12],\n",
    "    POUTCOME = p[13],\n",
    "    EMP_VAR_RATE = p[14],\n",
    "    CONS_PRICE_IDX = p[15],\n",
    "    CONS_CONF_IDX = p[16],\n",
    "    EURIBOR3M = p[17],\n",
    "    EMPLOYED = p[18],\n",
    "    TARGET = p[19]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94467717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um Dataframe\n",
    "rdd_df = spSession.createDataFrame(rdd_row)\n",
    "rdd_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando as funções importadas para auxiliar na detecção de valores Missing/Ausentes.\n",
    "\n",
    "rdd_na = rdd_df.select([count(when(col(c).contains('None') | col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | col(c).isNull() | isnan(c), c )).alias(c)\n",
    "                    for c in rdd_df.columns])\n",
    "rdd_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b529039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando os valores únicos para cada coluna do dataframe\n",
    "\n",
    "list_columns = rdd_df.columns\n",
    "\n",
    "for column in list_columns:\n",
    "    count = rdd_df.select(column).distinct().count()\n",
    "    print(f'Column: {column}\\tCount: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec41b3",
   "metadata": {},
   "source": [
    "### Tratando valores Ausentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a624625",
   "metadata": {},
   "source": [
    "Colunas que apresentaram valores ausentes:\n",
    "* MARITAL\n",
    "* DEFAULT\n",
    "* HOUSING\n",
    "* DAY_OF_WEEK\n",
    "* POUTCOME\n",
    "* CONS_PRICE_IDX\n",
    "\n",
    "> Vários registros em várias colunas possuem o valor `unknown` em português (desconhecido). Porém, eu irei considerar estes valores como corretos e não vou fazer nenhum tratamento para estes dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c927e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este método agrupa o dataframe pela coluna passada como parâmetro e pela variável target.\n",
    "def getDfGroup(rdd_df, column):\n",
    "    df_group = spSession.createDataFrame(rdd_df.groupBy(['TARGET', column]).agg({column: 'count'}).collect())\n",
    "\n",
    "    df_group = df_group.orderBy(['TARGET', column, f'count({column})'], ascending=[0, 1, 0])\n",
    "\n",
    "    return df_group\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cdcb5a",
   "metadata": {},
   "source": [
    "**MARITAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef631aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = getDfGroup(rdd_df, 'MARITAL')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47a69d",
   "metadata": {},
   "source": [
    "A moda do valor nulo agrupada por target é `married`, contendo 22396 registros, portanto, é com esse valor que irei preencher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04cbbf3",
   "metadata": {},
   "source": [
    "**DEFAULT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2afe39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = getDfGroup(rdd_df, 'DEFAULT')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1fc9a8",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `no`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4fd36",
   "metadata": {},
   "source": [
    "**EDUCATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce394f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = getDfGroup(rdd_df, 'EDUCATION')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c262dca",
   "metadata": {},
   "source": [
    "Neste atributo, temos valores `unknown`, porém, não irei considerar como valor nulo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d75b1",
   "metadata": {},
   "source": [
    "**HOUSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ed5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = getDfGroup(rdd_df, 'HOUSING')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ffe66",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `yes`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74475b7",
   "metadata": {},
   "source": [
    "**DAY_OF_WEEK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3dd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = getDfGroup(rdd_df, 'DAY_OF_WEEK')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f5d0f",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `mon`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5483f1",
   "metadata": {},
   "source": [
    "**POUTCOME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = getDfGroup(rdd_df, 'POUTCOME')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e88318",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `nonexistent`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4ec3c",
   "metadata": {},
   "source": [
    "**CONS_PRICE_IDX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = getDfGroup(rdd_df, 'CONS_PRICE_IDX')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ffa08",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `93.91799999999999`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dced9c",
   "metadata": {},
   "source": [
    "Após ter encontrado todos os valores nulos e definido com quais valores eu irei preencher, irei criar a função que realizará todo este mamepamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificarNA(c):\n",
    "    c = c.upper()\n",
    "    if c == 'NONE' or c == 'NULL' or c == '' or c == 'NAN':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def mapNA(x):\n",
    "    AGE = x.AGE\n",
    "    JOB = x.JOB\n",
    "    MARITAL = x.MARITAL\n",
    "    EDUCATION = x.EDUCATION\n",
    "    DEFAULT = x.DEFAULT\n",
    "    HOUSING = x.HOUSING\n",
    "    LOAN = x.LOAN\n",
    "    CONTACT = x.CONTACT\n",
    "    MONTH = x.MONTH\n",
    "    DAY_OF_WEEK = x.DAY_OF_WEEK\n",
    "    CAMPAIGN = x.CAMPAIGN\n",
    "    PDAYS = x.PDAYS\n",
    "    PREVIOUS = x.PREVIOUS\n",
    "    POUTCOME = x.POUTCOME\n",
    "    EMP_VAR_RATE = x.EMP_VAR_RATE\n",
    "    CONS_PRICE_IDX = x.CONS_PRICE_IDX\n",
    "    CONS_CONF_IDX = x.CONS_CONF_IDX\n",
    "    EURIBOR3M = x.EURIBOR3M\n",
    "    EMPLOYED = x.EMPLOYED\n",
    "    TARGET = x.TARGET\n",
    "    \n",
    "    #Corrigindo valores missing\n",
    "    if verificarNA(x.MARITAL):\n",
    "        MARITAL = 'married'\n",
    "        \n",
    "    if verificarNA(x.DEFAULT):\n",
    "        DEFAULT = 'no'\n",
    "        \n",
    "    if verificarNA(x.HOUSING):\n",
    "        HOUSING = 'yes'\n",
    "        \n",
    "    if verificarNA(x.DAY_OF_WEEK):\n",
    "        DAY_OF_WEEK = 'mon'\n",
    "        \n",
    "    if verificarNA(x.POUTCOME):\n",
    "        POUTCOME = 'nonexistent'\n",
    "        \n",
    "    if verificarNA(x.CONS_PRICE_IDX):\n",
    "        CONS_PRICE_IDX = '93.91799999999999'\n",
    "    \n",
    "    \n",
    "    return (AGE, JOB, MARITAL, EDUCATION, DEFAULT, HOUSING, LOAN, CONTACT, MONTH, DAY_OF_WEEK, CAMPAIGN, PDAYS, PREVIOUS, POUTCOME, EMP_VAR_RATE, CONS_PRICE_IDX, CONS_CONF_IDX, EURIBOR3M, EMPLOYED, TARGET)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_row = rdd_df.rdd.map(lambda x: mapNA(x))\n",
    "\n",
    "# Criando um Dataframe\n",
    "rdd_df = spSession.createDataFrame(rdd_row, schema=list_columns)\n",
    "rdd_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821dc6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef911455",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb83c7a",
   "metadata": {},
   "source": [
    "### Encoding dos dados\n",
    "\n",
    "A maioria dos atributos são dados categóricos que representam classes. Porém, da forma que está, é impossível submeter os dados para um modelo de Machine Learning.\n",
    "\n",
    "Logo, temos que converter os dados categóricos para numéricos. E para isso, existem duas abordagens: Label Encoder e o One Hot Encoder. Mas calma, irei explicar o que são e em que situações se aplicam cada um dos dois.\n",
    "\n",
    "* **Label Encoder**: De maneira simples, o Label Encoder serve para converter os dados categóricos para numéricos. Vamos pegar como exemplo os dias da semana. O algoritmo vai converter os dias em numéricos. Ou seja, Segunda-feira será 0, Terça-feira será igual 1, Quarta-Feira igual 2, e assim por diante.\n",
    "\n",
    "    O que devemos nos atentar?<br/>\n",
    "    Veja que o algoritmo substitui valores por números, certo? Pois bem, note que os números são sequenciais e para a maioria dos algoritmos de Machine Learning o fato do numéro ser sequencial implica que o 5 tem mais peso que o 4, o 4 por sua vez, tem mais peso que o 3, e assim por diante.\n",
    "\n",
    "    Usamos o Label Encoder quando temos uma hierarquia nos dados. Por exemplo, grau de escolaridade, patente em cargos militares, etc. \n",
    "\n",
    "> Nota: Em casos onde temos apenas dois valores distintos, podemos utilizar o Label Encoder, mesmo que não haja uma hierárquia entre os dados. Afinal de contas, teremos apenas dois valores: 0 e 1.\n",
    "\n",
    "---\n",
    "\n",
    "* **One Hot Encoder**: O One hot encoder (OHE) também converte dados categóricos para valores numéricos. Mas nesse método, para cada classe diferente, o algoritmo cria uma nova coluna, atribuindo apenas os valores 0 e 1. Com isso, eliminamos a possibilidade de uma classe influenciar a outra em um modelo de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af67d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voltando a programação.. Vejamos todos os valores únicos para cada coluna\n",
    "\n",
    "\n",
    "\n",
    "for column in list_columns:\n",
    "    print(f'{column}: Distinct count: {rdd_df.select(column).distinct().count()}')\n",
    "    rdd_df.select(column).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e629873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voltando a programação.. Vejamos todos os valores únicos e a quantidade de registros para cada coluna\n",
    "\n",
    "for column in list_columns:\n",
    "    print(f'{column}: Distinct count: {rdd_df.select(column).distinct().count()}')\n",
    "    print(rdd_df.groupBy([column]).count().show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b42ef",
   "metadata": {},
   "source": [
    "### LABEL ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7fe90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas são as colunas que eu decidi selecionar para aplicar o Label Encoder\n",
    "columns_labelencoder = ['EDUCATION', 'DEFAULT', 'CONTACT', 'MONTH', 'DAY_OF_WEEK', 'CAMPAIGN', 'PDAYS', 'PREVIOUS', 'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dcefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crio uma função para mapear cada atributo\n",
    "\n",
    "def map_education(x):\n",
    "    return -1 if x == 'unknown' else \\\n",
    "            0 if x == 'illiterate' else \\\n",
    "            1 if x == 'basic.4y' else \\\n",
    "            2 if x == 'basic.6y' else \\\n",
    "            3 if x == 'basic.9y' else \\\n",
    "            4 if x == 'high.school' else \\\n",
    "            5 if x == 'professional.course' else \\\n",
    "            6 if x == 'university.degree' else \\\n",
    "            7\n",
    "\n",
    "def map_housing(x):\n",
    "    return  0 if x == 'yes' else \\\n",
    "            1 if x == 'no' else \\\n",
    "            1 if x == 'unknown' else\\\n",
    "           -1\n",
    "\n",
    "def map_month(x):\n",
    "    return  0 if x == 'jan' else \\\n",
    "            1 if x == 'feb' else \\\n",
    "            2 if x == 'mar' else \\\n",
    "            3 if x == 'apr' else \\\n",
    "            4 if x == 'may' else \\\n",
    "            5 if x == 'jun' else \\\n",
    "            6 if x == 'jul' else \\\n",
    "            7 if x == 'aug' else \\\n",
    "            8 if x == 'sep' else \\\n",
    "            9 if x == 'oct' else \\\n",
    "            10 if x == 'nov' else \\\n",
    "            11 if x == 'dec' else \\\n",
    "            -1\n",
    "\n",
    "def map_day_of_week(x):\n",
    "    return  0 if x == 'mon' else \\\n",
    "            1 if x == 'tue' else \\\n",
    "            2 if x == 'wed' else \\\n",
    "            3 if x == 'thu' else \\\n",
    "            4 if x == 'fri' else \\\n",
    "            -1\n",
    "\n",
    "def map_contact(x):\n",
    "    return  0 if x == 'cellular' else \\\n",
    "            1 if x == 'telephone' else \\\n",
    "            -1\n",
    "\n",
    "def map_target(x):\n",
    "    return  0 if x == 'no' else \\\n",
    "            1 if x == 'yes' else \\\n",
    "            -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633fc2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para aplicar efetivamente o LabelEncoder \n",
    "\n",
    "def manualEncoder(x):\n",
    "    AGE = x.AGE\n",
    "    JOB = x.JOB\n",
    "    MARITAL = x.MARITAL\n",
    "    EDUCATION = map_education(x.EDUCATION)\n",
    "    DEFAULT = x.DEFAULT\n",
    "    HOUSING = map_housing(x.HOUSING)\n",
    "    LOAN = x.LOAN\n",
    "    CONTACT = map_contact(x.CONTACT)\n",
    "    MONTH = map_month(x.MONTH)\n",
    "    DAY_OF_WEEK = map_day_of_week(x.DAY_OF_WEEK)\n",
    "    CAMPAIGN = x.CAMPAIGN\n",
    "    PDAYS = x.PDAYS\n",
    "    PREVIOUS = x.PREVIOUS\n",
    "    POUTCOME = x.POUTCOME\n",
    "    EMP_VAR_RATE = x.EMP_VAR_RATE\n",
    "    CONS_PRICE_IDX = x.CONS_PRICE_IDX\n",
    "    CONS_CONF_IDX = x.CONS_CONF_IDX\n",
    "    EURIBOR3M = x.EURIBOR3M\n",
    "    EMPLOYED = x.EMPLOYED\n",
    "    TARGET = map_target(x.TARGET)\n",
    "  \n",
    "    \n",
    "    \n",
    "    return (AGE, JOB, MARITAL, EDUCATION, DEFAULT, HOUSING, LOAN, CONTACT, MONTH, DAY_OF_WEEK, CAMPAIGN, PDAYS, PREVIOUS, POUTCOME, EMP_VAR_RATE, CONS_PRICE_IDX, CONS_CONF_IDX, EURIBOR3M, EMPLOYED, TARGET)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_row = rdd_df.rdd.map(lambda x: manualEncoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um Dataframe\n",
    "rdd_df_encoded = spSession.createDataFrame(rdd_row, schema=list_columns)\n",
    "rdd_df_encoded.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e89bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_df_encoded.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be7ae3",
   "metadata": {},
   "source": [
    "> Nota: Eu poderia ter utilizado um método para fazer tudo isso de maneira automática para mim como o StringIndexer. Porém, eu quis ter um controle para indexar os dados ordenados de maneira lógica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8c657",
   "metadata": {},
   "source": [
    "### ONE HOT ENCODER\n",
    "\n",
    "Agora o One Hot Encoder. Estas são as colunas que eu decidi selecionar para aplicar o algoritmo. Para esta etapa eu quis criar uma função que faça o OHE para mim. Eu não gostei da versão do One Hot Encoder do pacote ml.feature do Pyspark. \n",
    "\n",
    "Por isso decidi incrementar o algoritmo com a minha própria função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_ohe = ['JOB', 'MARITAL', 'DEFAULT', 'LOAN', 'POUTCOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38949e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_others = ['AGE','EDUCATION', 'HOUSING', 'CONTACT', 'MONTH', 'DAY_OF_WEEK', 'CAMPAIGN', \n",
    "                  'PDAYS', 'PREVIOUS', 'EMP_VAR_RATE', 'CONS_PRICE_IDX', 'CONS_CONF_IDX', 'EURIBOR3M', \n",
    "                  'EMPLOYED', 'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função expandir as colunas que foram geradas pelo OneHotEncoding.\n",
    "def __applyOHE(df, column):\n",
    "    \n",
    "    alias = f'_{column}'\n",
    "    df_col_onehot = df.select('*', vector_to_array(column).alias(alias))\n",
    "\n",
    "    num_categories = len(df_col_onehot.first()[alias])\n",
    "    cols_expanded = [(col(alias)[i]) for i in range(num_categories)]\n",
    "    df_cols_onehot = df_col_onehot.select('*', *cols_expanded)\n",
    "    \n",
    "    return df_cols_onehot, cols_expanded\n",
    "\n",
    "\n",
    "\n",
    "def myOneHotEncoder(df, list_columns, list_others_columns):\n",
    "    # LabelEncoder\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"OHE_{0}\".format(c)) for c in list_columns]\n",
    "\n",
    "    # Criação do array de Encoders\n",
    "    encoders = [OneHotEncoder(\n",
    "            inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"_{0}\".format(indexer.getOutputCol())) \n",
    "        for indexer in indexers]\n",
    "    \n",
    "    # Aplica os dois métodos: LabelEncoder e OneHotEncoder\n",
    "    pipeline = Pipeline(stages=indexers + encoders)\n",
    "    df_temp = pipeline.fit(df).transform(rdd_df_encoded)\n",
    "\n",
    "    # Obtendo todas as colunas de saída do Encoder\n",
    "    encoder_columns = [encoder.getOutputCol() for encoder in encoders]\n",
    "\n",
    "    # Expandindo o array gerado pelo OneHotEncoding para Colunas individuais\n",
    "    ohe_columns = []\n",
    "    for column in encoder_columns: \n",
    "        df_temp, _ohe = __applyOHE(df_temp, column)\n",
    "\n",
    "        ohe_columns.extend(_ohe)\n",
    "    \n",
    "    df_res = df_temp.select(*list_others_columns, *ohe_columns)\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = myOneHotEncoder(rdd_df_encoded, columns_ohe, columns_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27385436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219670c",
   "metadata": {},
   "source": [
    "### Alterando o tipo de dado\n",
    "\n",
    "Lembra que quando carregamos os dados eles vieram como uma lista de Strings? Pois bem, quando eu fiz o split dos dados e separei cada valor para seu respectivo atributo, o que aconteceu foi que os dados continuaram como sendo do tipo string.\n",
    "\n",
    "Por isso irei aplicar o cast para converter os dados para os tipos `inteiro` e `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e62e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_res.withColumn('AGE', col('AGE').cast(IntegerType()))\n",
    "df_res = df_res.withColumn('CAMPAIGN', col('CAMPAIGN').cast(IntegerType()))\n",
    "df_res = df_res.withColumn('PDAYS', col('PDAYS').cast(IntegerType()))\n",
    "df_res = df_res.withColumn('PREVIOUS', col('PREVIOUS').cast(IntegerType()))\n",
    "\n",
    "df_res = df_res.withColumn('EMP_VAR_RATE', col('EMP_VAR_RATE').cast(FloatType()))\n",
    "df_res = df_res.withColumn('CONS_PRICE_IDX', col('CONS_PRICE_IDX').cast(FloatType()))\n",
    "df_res = df_res.withColumn('CONS_CONF_IDX', col('CONS_CONF_IDX').cast(FloatType()))\n",
    "df_res = df_res.withColumn('EURIBOR3M', col('EURIBOR3M').cast(FloatType()))\n",
    "df_res = df_res.withColumn('EMPLOYED', col('EMPLOYED').cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d565af",
   "metadata": {},
   "source": [
    "Agora que todos os dados são numéricos, quero visualizar a correlação de cada atributo com a variável TARGET. Em seguida, selecionar somente os atributos cuja correlação é maior do que 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = []\n",
    "\n",
    "for column in df_res.columns:\n",
    "    corr = df_res.corr('TARGET', column)\n",
    "    print(f'Column {column} Corr : {corr}')\n",
    "    \n",
    "    if abs(corr) > 0.05 and column != 'TARGET':\n",
    "        selected_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ba672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res1 = df_res.select(*selected_columns, 'TARGET')\n",
    "df_res1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb72ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415d203",
   "metadata": {},
   "source": [
    "Eu poderia começar a modelagem dos dados utilizando somente essas colunas, mas quero testar uma versão considerandos todos os atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c9093",
   "metadata": {},
   "source": [
    "### Pré Processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rows = ['AGE', 'EDUCATION', 'HOUSING', 'CONTACT', 'MONTH', 'DAY_OF_WEEK', 'CAMPAIGN', 'PDAYS', 'PREVIOUS', 'EMP_VAR_RATE', \n",
    " 'CONS_PRICE_IDX', 'CONS_CONF_IDX', 'EURIBOR3M', 'EMPLOYED', '__OHE_JOB[0]', '__OHE_JOB[1]', '__OHE_JOB[2]', '__OHE_JOB[3]', \n",
    " '__OHE_JOB[4]', '__OHE_JOB[5]', '__OHE_JOB[6]', '__OHE_JOB[7]', '__OHE_JOB[8]', '__OHE_JOB[9]', '__OHE_JOB[10]', \n",
    " '__OHE_MARITAL[0]', '__OHE_MARITAL[1]', '__OHE_MARITAL[2]', '__OHE_DEFAULT[0]', '__OHE_DEFAULT[1]', '__OHE_LOAN[0]', \n",
    " '__OHE_LOAN[1]', '__OHE_POUTCOME[0]', '__OHE_POUTCOME[1]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformaVar(row, list_columns) :\n",
    "    # Criando uma lista com o valor de cada atributo do Row\n",
    "    list_row = [row[i] for i in list_columns]\n",
    "    \n",
    "    # Criação de uma Tupla com dois valores\n",
    "    # O primeiro é a variável alvo: TARGET;\n",
    "    # Em seguida, a criação de um Vetor Denso com todos os valores da Row\n",
    "    obj = (row['TARGET'], Vectors.dense(list_row))\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_processing = df_res.rdd.map(lambda x: transformaVar(x, list_rows))\n",
    "\n",
    "rdd_processing.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790af685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processing = spSession.createDataFrame(rdd_processing, [\"TARGET\",\"FEATURES\"])\n",
    "df_processing.show(10)\n",
    "df_processing.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad928a1a",
   "metadata": {},
   "source": [
    "Agora temos a primeira versão dos dados que serão submetidos ao algoritmo de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1800f2",
   "metadata": {},
   "source": [
    "### Escala dos Dados\n",
    "\n",
    "Temos que deixar os dados todos em uma mesma escala, se não o modelo vai ficar doido, tendo que encontrar a relação entre atributos com valores 0s e 1s e outros atributos com valores 99999 e 100000.\n",
    "\n",
    "O modelo não vai apresentar erro (exception), mas ele vai aprender errado sobre os dados, dando muito mais importância para valores muito altos do que os valores baixos, principalmente os modelos lineares.\n",
    "\n",
    "Existem vários algoritmos para fazer a escala dos dados: MinMax, Standard, Robust, Quantile, etc.. É uma verdadeira ciência! Para esse projeto eu selecionei o RobustScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0647734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler(inputCol='FEATURES', outputCol='FEATURES_SCALED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ff85f",
   "metadata": {},
   "source": [
    "### Divisão em treino e teste\n",
    "\n",
    "Irei dividir o conjunto de dados em três partes:\n",
    "1. Dados para treinamento: 78%;\n",
    "2. Dados para testes: 20%;\n",
    "3. Dados para validações: 2%;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74f207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de Treino e de Teste\n",
    "(dados_treino, dados_teste, dados_valid) = df_processing.randomSplit([0.78, 0.2, 0.02])\n",
    "\n",
    "dados_treino.count(), dados_teste.count(), dados_valid.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59449c0f",
   "metadata": {},
   "source": [
    "### Tratamento das classes desbalanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed67e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino.groupBy('TARGET').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a753e",
   "metadata": {},
   "source": [
    "Veja que a diferença entre a quantidade para cada saída da variável target é muito grande. Isso reflete a realidade. Afinal de contas, a quantidade de pessoas que realmente fazem um empréstimo após uma chamada de telefone do setor de Marketing do banco é pequena.\n",
    "\n",
    "Mas o que isso impacta no nosso modelo?<br/>\n",
    "Simples, o algoritmo vai aprender muito sobre os dados do grupo majoritário e pouco sobre os dados do grupo minoritário. Por isto, devemos aplicar alguma técnica para tratar este problema.\n",
    "\n",
    "1. Podemos simplesmente remover os dados do grupo majoritário de modo a balancear as classes. Mas como a diferença é enorme, neste caso perderíamos muita informação relevante.\n",
    "2. Podemos preencher o grupo minoritário com dados sintéticos. Existem algumas formas de fazer isso. Eu irei aplicar a mais simples de todas, multiplicar os registros existentes de modo que as classes fiquem balanceadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a483f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanceClasses(df, target):\n",
    "    # Separando os dataframes por classe target\n",
    "    df_target_1 = df[df[target] == 1]\n",
    "    df_target_0 = df[df[target] == 0]\n",
    "    \n",
    "    # Obtendo a proporção entre os datasets. \n",
    "    # Arredondei essa proporção para baixo para que a classe minoritária não ultrapasse a classe majoritária.\n",
    "    fraction = float(floor(df_target_0.count() / df_target_1.count()))\n",
    "\n",
    "    # Multiplicando os valores utilizando o Sample (Amostragem) com reposição\n",
    "    # Nota: O 123 é somente um seed qualquer para a aleatoriedade possa ser replicada\n",
    "    df_target_1_sample = df_target_1.sample(True, fraction, 123)\n",
    "    \n",
    "    # A amostragem pode ter desconsiderado de maneira aleatória algum registro real\n",
    "    # Por isso eu quero incluir além dos valores sintéticos gerados, os valores reais\n",
    "    # mas sem ultrapassar a quantidade de dados do grupo majoritário\n",
    "    diff = df_target_1_sample.count() - df_target_1.count()\n",
    "\n",
    "    # Convertendo os dados um dataframe\n",
    "    df_temp = spSession.createDataFrame(df_target_1_sample.collect()[0:diff])\n",
    "    # Unindo os dados sintéticos com os dados reais\n",
    "    df_temp = df_temp.unionAll(df_target_1)\n",
    "    \n",
    "    # Unindo os dois dataframes finais\n",
    "    df =  df_temp.unionAll(df_target_0)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e27e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino = balanceClasses(dados_treino, 'TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b21b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dados_treino.count())\n",
    "\n",
    "dados_treino.groupBy('TARGET').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080a83b",
   "metadata": {},
   "source": [
    "Agora sim. Temos as classes muito mais balanceadas. Com isso, o modelo vai aprender igualmente sobre os dados. Você pode se perguntar... Mas o modelo não vai aprender muito mais sobre os dados duplicados do que os demais?\n",
    "\n",
    "A resposta é sim. Em computação não dá só para ganhar, sempre quando cobrimos um lado, o outro é descoberto. Contudo, o modelo ainda sim vai aprender melhor sobre os atributos TARGET como um todo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90a1e7",
   "metadata": {},
   "source": [
    "### Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713fa762",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtClassifer = RandomForestClassifier(labelCol = \"TARGET\", featuresCol = \"FEATURES_SCALED\")\n",
    "#dtClassifer = GBTClassifier(labelCol = \"TARGET\", featuresCol = \"FEATURES_SCALED\")\n",
    "#dtClassifer = LinearSVC(labelCol = \"TARGET\", featuresCol = \"FEATURES_SCALED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c40cc",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73eef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[scaler, dtClassifer])\n",
    "\n",
    "model = pipeline.fit(dados_treino)\n",
    "\n",
    "features_importances = model.stages[-1].featureImportances\n",
    "\n",
    "features_importances = list(sorted(zip(list_rows, features_importances), key= lambda x: x[1], reverse=True))\n",
    "\n",
    "cols_importants = [column[0] for column in features_importances if column[1] > 0.03]\n",
    "\n",
    "cols_importants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af39c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_processing = df_res.rdd.map(lambda x: transformaVar(x, cols_importants))\n",
    "\n",
    "rdd_processing.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f422be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processing = spSession.createDataFrame(rdd_processing, [\"TARGET\",\"FEATURES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de Treino e de Teste\n",
    "(dados_treino, dados_teste, dados_valid) = df_processing.randomSplit([0.78, 0.2, 0.02])\n",
    "\n",
    "dados_treino = balanceClasses(dados_treino, 'TARGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91171b6f",
   "metadata": {},
   "source": [
    "### Aplicando Pipeline, Treinando e Prevendo o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fcf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtClassifer = RandomForestClassifier(labelCol = \"TARGET\", featuresCol = \"FEATURES_SCALED\")\n",
    "dtClassifer = GBTClassifier(labelCol = \"TARGET\", featuresCol = \"FEATURES_SCALED\")\n",
    "#dtClassifer = LinearSVC(labelCol = \"TARGET\", featuresCol = \"FEATURES_SCALED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110fac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[scaler, dtClassifer])\n",
    "\n",
    "model = pipeline.fit(dados_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões com dados de teste\n",
    "previsoes = model.transform(dados_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b073f",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando a acurácia\n",
    "avaliador = MulticlassClassificationEvaluator(predictionCol = \"prediction\", labelCol = \"TARGET\", metricName = \"f1\")\n",
    "avaliador.evaluate(previsoes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumindo as previsões - Confusion Matrix\n",
    "previsoes.groupBy(\"TARGET\",\"PREDICTION\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf88c89",
   "metadata": {},
   "source": [
    "### Salvando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409970b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "model.write().overwrite().save('models/modelo_v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b539f",
   "metadata": {},
   "source": [
    "### Carregando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef695a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c52765",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load = PipelineModel.load('models/modelo_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abccc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = model_load.transform(dados_valid)\n",
    "\n",
    "avaliador = MulticlassClassificationEvaluator(predictionCol = \"prediction\", labelCol = \"TARGET\", metricName = \"f1\")\n",
    "avaliador.evaluate(preds_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumindo as previsões - Confusion Matrix\n",
    "preds_valid.groupBy(\"TARGET\",\"PREDICTION\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e044d0d",
   "metadata": {},
   "source": [
    "### Considerações Finais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
