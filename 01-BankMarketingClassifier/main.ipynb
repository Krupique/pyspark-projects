{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e70e274",
   "metadata": {},
   "source": [
    "# Bank Marketing Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa6e2e",
   "metadata": {},
   "source": [
    "### Introdução ao tema\n",
    "\n",
    "O marketing está presente nas nossas vidas muito mais do que imaginamos. Faça uma caminhada pelas ruas da cidade, uma busca no seu navegador, ligue a televisão ou o rádio, abra sua rede social e você será impactado por alguma ação de marketing.\n",
    "\n",
    "Mas o que é marketing?\n",
    "Para Philip Kotler, um dos teóricos mais renomados da área, define marketing como:\n",
    "> *Marketing é a ciência e arte de explorar, criar e proporcionar valor para satisfazer necessidades de um público-alvo com rendibilidade.*\n",
    "\n",
    "O campo do marketing é vasto e inclui não apenas o ato de vender um produto ou serviço, mas tudo relacionado ao planejamento, pesquisa e posicionamento de mercado. Em outras palavras, pode-se dizer que o marketing é como uma balança entre o que os clientes desejam e os objetivos da empresa. Afinal, um bom marketing precisa criar valor para ambas as partes: para a empresa e para o consumidor.\n",
    "\n",
    "Vale ressaltar que marketing é uma palavra em inglês, derivada de market (mercado). Portanto, marketing não é apenas vender produtos ou serviços, engloba também outras atividades relacionadas ao mercado.\n",
    "\n",
    "**Bank Marketing**<br/>\n",
    "\n",
    "Um tipo de instituição que aplica marketing no seu dia a dia são os bancos, para isso, damos o nome Bank Marketing (Marketing Bancário). O marketing bancário é a prática de atrair e adquirir novos clientes por meio de estratégias de mídia tradicional e mídia digital. O uso dessas estratégias de mídia ajuda a determinar que tipo de cliente é atraído por uma determinada instituição. Isso também inclui diferentes instituições bancárias que usam propositalmente diferentes estratégias para atrair o tipo de cliente com o qual desejam fazer negócios.\n",
    "\n",
    "E você sabe como as principais empresas fazem atualmente para aplicar estratégias de Marketing?<br/>\n",
    "Se você respondeu, \"elas aplicam técnicas de Inteligência Artificial e Machine Learning para entender e avaliar o comportamento dos seus clientes\". Parabéns, você acertou!\n",
    "Mas antes de explicar com elas fazem isso, vamos começar entendendo um pouco sobre o que é Machine Learning.\n",
    "\n",
    "**Machine Learning**<br/>\n",
    "O Machine Learning, ou aprendizado de máquina. É um subcampo da inteligência artificial que permite dar aos computadores a habilidade de aprender sem que sejam explicitamente programados para isso. Ela permite que computadores tomem decisões e interpretem dados de maneira automática, a partir de algoritmos. Temos vários tipos de aprendizagem, são elas: Supervisionada, não supervisionada, semi supervisionada, aprendizagem por reforço e deep learning.\n",
    "\t\n",
    "Os algoritmos de aprendizagem de máquina, aprendem a induzir uma função ou hipótese capaz de resolver um problema a partir de dados que representam instâncias do problema a ser resolvido.\n",
    "\n",
    "Um algoritmo é uma sequência finita de ações e regras que visam a solucionar um problema. Cada um deles aciona um diferente tipo de operação ao entrar em contato com os dados que o computador recebe. O resultado de todas as operações é o que possibilita o aprendizado da máquina.\n",
    "\n",
    "Dessa forma, as máquinas aperfeiçoam as tarefas executadas, por meio de processamento de dados como imagens e números. Por isso o machine learning depende do Big Data para ser efetivo. O Big Data, por sua vez pode ser entendido de maneira simplória como uma imensa quantidade de dados. Mas calma, ainda irei falar mais em detalhes sobre isso. Por ora, vamos entender como o Machine Learning e a Inteligência Artificial ajudam a benefeciar a área de Bank Marketing.\n",
    "\n",
    "**Machine Learning e o Bank Marketing**<br/>\n",
    "Abaixo irei listar alguns benefícios do Machine Learning (ML) aplicado na área de Bank Marketing:\n",
    "* **Atendimento ao cliente orientado por IA**: Existem muitas maneiras de tornar o atendimento ao cliente realmente orientado por IA ou, melhor dizer, orientado por dados. Por exemplo, com a ajuda da análise de dados, a instituição bancária pode descobrir as intenções de compra do cliente e oferecer um empréstimo flexível. Além disso, os principais bancos criam chatbots inteligentes que ajudam os clientes a interagir melhor com as empresas financeiras. Com a ajuda de aplicativos inteligentes, os clientes podem acompanhar automaticamente seus gastos, planejar seu orçamento e obter sugestões precisas de economia e investimento.\n",
    "* **Segmentação de clientes**: Com ML, é possível encontrar características semelhantes e padrões entre os dados dos clientes. Dessa forma, o algoritmo de ML consegue separar os clientes em grupos, possibilitando que a equipe de Marketing possa direcionar os esforços de maneira individual para cada grupo de clientes.\n",
    "* **Otimização de lances em anúncios**: os anúncios em buscadores funcionam no sistema de leilões de pesquisa. Ou seja, quem der o maior lance aparecerá em primeiro lugar nos resultados de pesquisa para uma determinada palavra-chave. Para fazer o lance perfeito, o marketing se utiliza do machine learning. Ele analisa milhões de dados para ajustar os lances em tempo real.\n",
    "* **Prever os possíveis clientes**: Com base nos dados históricos da empresa, podemos coletar e entender qual é o perfil dos clientes. E com base nisso, prever a probabilidade do indivíduo adquirir determinado serviço. Como por exemplo, contrair um empréstimo, adquirir investimentos, dentre outros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a68bea",
   "metadata": {},
   "source": [
    "**Objetivo do Projeto**<br/>\n",
    "Como objetivo específico do problema de negócio, irei aplicar técnicas de Machine Learning para identificar e prever ser se o cliente vai ou não adquirir CDBs (Certificado de Depósito Bancário). Este é um tipo de investimento muito comum e bastante conservador. De maneira rápida e simples, basicamente o cliente empresta o dinheiro para o banco e após de um prazo pré-estabelecido, ele resgata o dinheiro com o acréscimo de juros.\n",
    "\n",
    "Como objetivo de estudo de tecnologia, estarei utilizando do início ao fim do projeto o framework Apache Spark, mais especificamente, o PySpark. PySpark é uma API Python para Apache SPARK que é denominado como o mecanismo de processamento analítico para aplicações de processamento de dados distribuídos em larga escala e aprendizado de máquina em tempo real, ou seja, para grandes volumes de dados, conhecido como Big Data.\n",
    "\n",
    "**Sobre o Dataset**<br/>\n",
    "Este conjunto de dados contém 20 atributos e 41189 registos relevantes para uma campanha de marketing direto de uma instituição bancária portuguesa. A campanha de marketing foi executada por meio de ligações telefônicas. O objetivo da classificação é prever se o cliente irá aderir (yes/no) ao CDB (variável y).<br/>\n",
    "\n",
    "O dataset pode ser obtido clicando [aqui](https://www.kaggle.com/datasets/ruthgn/bank-marketing-data-set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53365659",
   "metadata": {},
   "source": [
    "### Introdução ao Apache Spark\n",
    "\n",
    "Apache Spark é uma estrutura de código aberto que simplifica o desenvolvimento e a eficiência dos trabalhos de análise de dados. Ele oferece suporte a uma ampla variedade de opções de API e linguagem com mais de 80 operadores de transformação e ação de dados que ocultam a complexidade da computação em cluster.\n",
    "\n",
    "Com velocidades relatadas 100 vezes mais rápidas do que mecanismos de análise semelhantes, o Spark pode acessar fontes de dados variáveis e ser executado em várias plataformas, incluindo Hadoop, Apache Mesos, Kubernetes, de forma independente ou na nuvem. Seja processando dados em lote ou streaming, você verá um desempenho de alto nível devido ao agendador Spark DAG de última geração, um otimizador de consulta e um mecanismo de execução física.\n",
    "\n",
    "> Caso você queira saber mais sobre o Apache Spark, eu recomendo fortemente a leitura do artigo \"Spark: entenda sua função e saiba mais sobre essa ferramenta\", publicado pelo blog XP Educação, que pode ser acessado clicando [aqui](https://blog.xpeducacao.com.br/apache-spark/)\n",
    "\n",
    "### PySpark\n",
    "\n",
    "PySpark é a colaboração do Apache Spark e do Python.\n",
    "\n",
    "O Apache Spark é uma estrutura de computação em cluster de código aberto, construída em torno da velocidade, facilidade de uso e análise de streaming, enquanto o Python é uma linguagem de programação de alto nível e de uso geral. Ele fornece uma ampla variedade de bibliotecas e é usado principalmente para Machine Learning e Real-Time Streaming Analytics.\n",
    "\n",
    "Em outras palavras, é uma API Python para Spark que permite aproveitar a simplicidade do Python e o poder do Apache Spark para domar o Big Data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a4b47",
   "metadata": {},
   "source": [
    "**Links de Referência**:\n",
    "\n",
    "* https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html\n",
    "* https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/\n",
    "* https://sparkbyexamples.com/pyspark/pyspark-map-transformation/\n",
    "\n",
    "---\n",
    "\n",
    "* https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "* https://github.com/Krupique/Explorating-Data/blob/main/PreProcessamento-Medium/PreProcessamento.ipynb\n",
    "* https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html\n",
    "\n",
    "* https://www.databricks.com/wp-content/uploads/hubfs/notebooks/spark2.0/ML%20persistence%20in%202.0.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719056b",
   "metadata": {},
   "source": [
    "### Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f488779",
   "metadata": {},
   "source": [
    "### Data Load and Packages Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "786a3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row #Converte RDDs em objetos do tipo Row\n",
    "from pyspark.ml.feature import StringIndexer #Converte strings em valores numéricos\n",
    "from pyspark.ml.linalg import Vectors #Serve para criar um vetor denso\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a6ea32e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n",
      "Spark Context Version: 3.0.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f'System Version: {sys.version}')\n",
    "print(f'Spark Context Version: {sc.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "53a43136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session - usada quando se trabalha com Dataframes no Spark\n",
    "spSession = SparkSession.builder.master(\"local\").appName(\"DSA-SparkMLLib\").config(\"spark.some.config.option\", \"session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a324bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd = sc.textFile('data/bank-marketing-dataset.csv')\n",
    "rdd = sc.textFile('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a31928",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a7467a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d457ebc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41189"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c9229b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age,job,marital,education,default,housing,loan,contact,month,day_of_week,campaign,pdays,previous,poutcome,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed,y',\n",
       " '56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0,no',\n",
       " '57,services,married,high.school,unknown,no,no,telephone,may,mon,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0,no',\n",
       " '37,services,married,high.school,no,yes,no,telephone,may,mon,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0,no',\n",
       " '40,admin.,married,basic.6y,no,no,no,telephone,may,mon,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0,no']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listando os 5 primeiros registros\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c74d6137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AGE',\n",
       " 'JOB',\n",
       " 'MARITAL',\n",
       " 'EDUCATION',\n",
       " 'DEFAULT',\n",
       " 'HOUSING',\n",
       " 'LOAN',\n",
       " 'CONTACT',\n",
       " 'MONTH',\n",
       " 'DAY_OF_WEEK',\n",
       " 'CAMPAIGN',\n",
       " 'PDAYS',\n",
       " 'PREVIOUS',\n",
       " 'POUTCOME',\n",
       " 'EMP_VAR_RATE',\n",
       " 'CONS_PRICE_IDX',\n",
       " 'CONS_CONF_IDX',\n",
       " 'EURIBOR3M',\n",
       " 'NR_EMPLOYED',\n",
       " 'Y']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = rdd.first()\n",
    "rdd_body = rdd.filter(lambda x: header not in x).map(lambda l: l.split(','))\n",
    "\n",
    "list_columns = header.replace('.', '_').upper().split(',')\n",
    "list_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "711b94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_row = rdd_body.map(lambda p: Row(\n",
    "    AGE = p[0], \n",
    "    JOB = p[1], \n",
    "    MARITAL = p[2],\n",
    "    EDUCATION = p[3],\n",
    "    DEFAULT = p[4],\n",
    "    HOUSING = p[5],\n",
    "    LOAN = p[6],\n",
    "    CONTACT = p[7],\n",
    "    MONTH = p[8],\n",
    "    DAY_OF_WEEK = p[9],\n",
    "    CAMPAIGN = p[10],\n",
    "    PDAYS = p[11],\n",
    "    PREVIOUS = p[12],\n",
    "    POUTCOME = p[13],\n",
    "    EMP_VAR_RATE = p[14],\n",
    "    CONS_PRICE_IDX = p[15],\n",
    "    CONS_CONF_IDX = p[16],\n",
    "    EURIBOR3M = p[17],\n",
    "    EMPLOYED = p[18],\n",
    "    TARGET = p[19]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "94467717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AGE: string, JOB: string, MARITAL: string, EDUCATION: string, DEFAULT: string, HOUSING: string, LOAN: string, CONTACT: string, MONTH: string, DAY_OF_WEEK: string, CAMPAIGN: string, PDAYS: string, PREVIOUS: string, POUTCOME: string, EMP_VAR_RATE: string, CONS_PRICE_IDX: string, CONS_CONF_IDX: string, EURIBOR3M: string, EMPLOYED: string, TARGET: string]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um Dataframe\n",
    "rdd_df = spSession.createDataFrame(rdd_row)\n",
    "rdd_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0e12f073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+-----+--------+--------+------------+--------------+-------------+---------+--------+------+\n",
      "|AGE|JOB|MARITAL|EDUCATION|DEFAULT|HOUSING|LOAN|CONTACT|MONTH|DAY_OF_WEEK|CAMPAIGN|PDAYS|PREVIOUS|POUTCOME|EMP_VAR_RATE|CONS_PRICE_IDX|CONS_CONF_IDX|EURIBOR3M|EMPLOYED|TARGET|\n",
      "+---+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+-----+--------+--------+------------+--------------+-------------+---------+--------+------+\n",
      "|  0|  0|      1|        0|      1|      1|   0|      0|    0|          1|       0|    0|       0|       2|           0|             1|            0|        0|       0|     0|\n",
      "+---+---+-------+---------+-------+-------+----+-------+-----+-----------+--------+-----+--------+--------+------------+--------------+-------------+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find count for empty, None, Null, Nan with string literals.\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "\n",
    "rdd_na = rdd_df.select([count(when(col(c).contains('None') | col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | col(c).isNull() | isnan(c), c )).alias(c)\n",
    "                    for c in rdd_df.columns])\n",
    "rdd_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4b529039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: AGE\tCount: 78\n",
      "Column: JOB\tCount: 12\n",
      "Column: MARITAL\tCount: 5\n",
      "Column: EDUCATION\tCount: 8\n",
      "Column: DEFAULT\tCount: 4\n",
      "Column: HOUSING\tCount: 4\n",
      "Column: LOAN\tCount: 3\n",
      "Column: CONTACT\tCount: 2\n",
      "Column: MONTH\tCount: 10\n",
      "Column: DAY_OF_WEEK\tCount: 6\n",
      "Column: CAMPAIGN\tCount: 42\n",
      "Column: PDAYS\tCount: 27\n",
      "Column: PREVIOUS\tCount: 8\n",
      "Column: POUTCOME\tCount: 4\n",
      "Column: EMP_VAR_RATE\tCount: 10\n",
      "Column: CONS_PRICE_IDX\tCount: 27\n",
      "Column: CONS_CONF_IDX\tCount: 26\n",
      "Column: EURIBOR3M\tCount: 316\n",
      "Column: EMPLOYED\tCount: 11\n",
      "Column: TARGET\tCount: 2\n"
     ]
    }
   ],
   "source": [
    "list_columns = rdd_df.columns\n",
    "\n",
    "for column in list_columns:\n",
    "    count = rdd_df.select(column).distinct().count()\n",
    "    print(f'Column: {column}\\tCount: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec41b3",
   "metadata": {},
   "source": [
    "### Handling Data Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a624625",
   "metadata": {},
   "source": [
    "Columns with missing values:\n",
    "* MARITAL\n",
    "* DEFAULT\n",
    "* HOUSING\n",
    "* DAY_OF_WEEK\n",
    "* POUTCOME\n",
    "* CONS_PRICE_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d0c927e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDfGroup(rdd_df, column):\n",
    "    df_group = spSession.createDataFrame(rdd_df.groupBy(['TARGET', column]).agg({column: 'count'}).collect())\n",
    "\n",
    "    df_group = df_group.orderBy(['TARGET', column, f'count({column})'], ascending=[0, 1, 0])\n",
    "\n",
    "    return df_group\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cdcb5a",
   "metadata": {},
   "source": [
    "**MARITAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4ef631aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', MARITAL='divorced', count(MARITAL)=476),\n",
       " Row(TARGET='yes', MARITAL='married', count(MARITAL)=2532),\n",
       " Row(TARGET='yes', MARITAL='single', count(MARITAL)=1620),\n",
       " Row(TARGET='yes', MARITAL='unknown', count(MARITAL)=12),\n",
       " Row(TARGET='no', MARITAL='', count(MARITAL)=1),\n",
       " Row(TARGET='no', MARITAL='divorced', count(MARITAL)=4136),\n",
       " Row(TARGET='no', MARITAL='married', count(MARITAL)=22396),\n",
       " Row(TARGET='no', MARITAL='single', count(MARITAL)=9947),\n",
       " Row(TARGET='no', MARITAL='unknown', count(MARITAL)=68)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'MARITAL')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47a69d",
   "metadata": {},
   "source": [
    "A moda do valor nulo agrupada por target é `married`, contendo 22396 registros, portanto, é com esse valor que irei preencher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04cbbf3",
   "metadata": {},
   "source": [
    "**DEFAULT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d2afe39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', DEFAULT='no', count(DEFAULT)=4197),\n",
       " Row(TARGET='yes', DEFAULT='unknown', count(DEFAULT)=443),\n",
       " Row(TARGET='no', DEFAULT='', count(DEFAULT)=1),\n",
       " Row(TARGET='no', DEFAULT='no', count(DEFAULT)=28391),\n",
       " Row(TARGET='no', DEFAULT='unknown', count(DEFAULT)=8153),\n",
       " Row(TARGET='no', DEFAULT='yes', count(DEFAULT)=3)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'DEFAULT')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1fc9a8",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `no`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4fd36",
   "metadata": {},
   "source": [
    "**EDUCATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dce394f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', EDUCATION='basic.4y', count(EDUCATION)=428),\n",
       " Row(TARGET='yes', EDUCATION='basic.6y', count(EDUCATION)=188),\n",
       " Row(TARGET='yes', EDUCATION='basic.9y', count(EDUCATION)=473),\n",
       " Row(TARGET='yes', EDUCATION='high.school', count(EDUCATION)=1031),\n",
       " Row(TARGET='yes', EDUCATION='illiterate', count(EDUCATION)=4),\n",
       " Row(TARGET='yes', EDUCATION='professional.course', count(EDUCATION)=595),\n",
       " Row(TARGET='yes', EDUCATION='university.degree', count(EDUCATION)=1670),\n",
       " Row(TARGET='yes', EDUCATION='unknown', count(EDUCATION)=251),\n",
       " Row(TARGET='no', EDUCATION='basic.4y', count(EDUCATION)=3748),\n",
       " Row(TARGET='no', EDUCATION='basic.6y', count(EDUCATION)=2104),\n",
       " Row(TARGET='no', EDUCATION='basic.9y', count(EDUCATION)=5572),\n",
       " Row(TARGET='no', EDUCATION='high.school', count(EDUCATION)=8484),\n",
       " Row(TARGET='no', EDUCATION='illiterate', count(EDUCATION)=14),\n",
       " Row(TARGET='no', EDUCATION='professional.course', count(EDUCATION)=4648),\n",
       " Row(TARGET='no', EDUCATION='university.degree', count(EDUCATION)=10498),\n",
       " Row(TARGET='no', EDUCATION='unknown', count(EDUCATION)=1480)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'EDUCATION')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c262dca",
   "metadata": {},
   "source": [
    "Neste atributo, temos valores `unknown`, porém, não irei considerar como valor nulo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d75b1",
   "metadata": {},
   "source": [
    "**HOUSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f32ed5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', HOUSING='no', count(HOUSING)=2026),\n",
       " Row(TARGET='yes', HOUSING='unknown', count(HOUSING)=107),\n",
       " Row(TARGET='yes', HOUSING='yes', count(HOUSING)=2507),\n",
       " Row(TARGET='no', HOUSING='', count(HOUSING)=1),\n",
       " Row(TARGET='no', HOUSING='no', count(HOUSING)=16596),\n",
       " Row(TARGET='no', HOUSING='unknown', count(HOUSING)=883),\n",
       " Row(TARGET='no', HOUSING='yes', count(HOUSING)=19068)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'HOUSING')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ffe66",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `yes`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74475b7",
   "metadata": {},
   "source": [
    "**DAY_OF_WEEK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9e3dd750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', DAY_OF_WEEK='fri', count(DAY_OF_WEEK)=846),\n",
       " Row(TARGET='yes', DAY_OF_WEEK='mon', count(DAY_OF_WEEK)=847),\n",
       " Row(TARGET='yes', DAY_OF_WEEK='thu', count(DAY_OF_WEEK)=1045),\n",
       " Row(TARGET='yes', DAY_OF_WEEK='tue', count(DAY_OF_WEEK)=953),\n",
       " Row(TARGET='yes', DAY_OF_WEEK='wed', count(DAY_OF_WEEK)=949),\n",
       " Row(TARGET='no', DAY_OF_WEEK='', count(DAY_OF_WEEK)=1),\n",
       " Row(TARGET='no', DAY_OF_WEEK='fri', count(DAY_OF_WEEK)=6981),\n",
       " Row(TARGET='no', DAY_OF_WEEK='mon', count(DAY_OF_WEEK)=7666),\n",
       " Row(TARGET='no', DAY_OF_WEEK='thu', count(DAY_OF_WEEK)=7578),\n",
       " Row(TARGET='no', DAY_OF_WEEK='tue', count(DAY_OF_WEEK)=7137),\n",
       " Row(TARGET='no', DAY_OF_WEEK='wed', count(DAY_OF_WEEK)=7185)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'DAY_OF_WEEK')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f5d0f",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `mon`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5483f1",
   "metadata": {},
   "source": [
    "**POUTCOME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a95e55c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', POUTCOME='failure', count(POUTCOME)=605),\n",
       " Row(TARGET='yes', POUTCOME='nonexistent', count(POUTCOME)=3141),\n",
       " Row(TARGET='yes', POUTCOME='success', count(POUTCOME)=894),\n",
       " Row(TARGET='no', POUTCOME='', count(POUTCOME)=2),\n",
       " Row(TARGET='no', POUTCOME='failure', count(POUTCOME)=3647),\n",
       " Row(TARGET='no', POUTCOME='nonexistent', count(POUTCOME)=32420),\n",
       " Row(TARGET='no', POUTCOME='success', count(POUTCOME)=479)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'POUTCOME')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e88318",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `nonexistent`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4ec3c",
   "metadata": {},
   "source": [
    "**CONS_PRICE_IDX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f40f5190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TARGET='yes', CONS_PRICE_IDX='92.20100000000001', count(CONS_PRICE_IDX)=264),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.37899999999999', count(CONS_PRICE_IDX)=106),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.431', count(CONS_PRICE_IDX)=180),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.469', count(CONS_PRICE_IDX)=66),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.649', count(CONS_PRICE_IDX)=168),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.713', count(CONS_PRICE_IDX)=88),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.756', count(CONS_PRICE_IDX)=1),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.84299999999999', count(CONS_PRICE_IDX)=126),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.89299999999999', count(CONS_PRICE_IDX)=524),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='92.963', count(CONS_PRICE_IDX)=264),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.075', count(CONS_PRICE_IDX)=442),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.2', count(CONS_PRICE_IDX)=190),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.369', count(CONS_PRICE_IDX)=150),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.444', count(CONS_PRICE_IDX)=271),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.749', count(CONS_PRICE_IDX)=97),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.79799999999999', count(CONS_PRICE_IDX)=42),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.876', count(CONS_PRICE_IDX)=122),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.91799999999999', count(CONS_PRICE_IDX)=407),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='93.994', count(CONS_PRICE_IDX)=240),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.027', count(CONS_PRICE_IDX)=120),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.055', count(CONS_PRICE_IDX)=107),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.199', count(CONS_PRICE_IDX)=150),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.215', count(CONS_PRICE_IDX)=176),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.465', count(CONS_PRICE_IDX)=188),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.601', count(CONS_PRICE_IDX)=93),\n",
       " Row(TARGET='yes', CONS_PRICE_IDX='94.76700000000001', count(CONS_PRICE_IDX)=58),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='', count(CONS_PRICE_IDX)=1),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.20100000000001', count(CONS_PRICE_IDX)=506),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.37899999999999', count(CONS_PRICE_IDX)=161),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.431', count(CONS_PRICE_IDX)=267),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.469', count(CONS_PRICE_IDX)=112),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.649', count(CONS_PRICE_IDX)=189),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.713', count(CONS_PRICE_IDX)=84),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.756', count(CONS_PRICE_IDX)=9),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.84299999999999', count(CONS_PRICE_IDX)=156),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.89299999999999', count(CONS_PRICE_IDX)=5270),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='92.963', count(CONS_PRICE_IDX)=451),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.075', count(CONS_PRICE_IDX)=2016),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.2', count(CONS_PRICE_IDX)=3426),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.369', count(CONS_PRICE_IDX)=114),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.444', count(CONS_PRICE_IDX)=4904),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.749', count(CONS_PRICE_IDX)=77),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.79799999999999', count(CONS_PRICE_IDX)=25),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.876', count(CONS_PRICE_IDX)=90),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.91799999999999', count(CONS_PRICE_IDX)=6278),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='93.994', count(CONS_PRICE_IDX)=7522),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.027', count(CONS_PRICE_IDX)=113),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.055', count(CONS_PRICE_IDX)=122),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.199', count(CONS_PRICE_IDX)=153),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.215', count(CONS_PRICE_IDX)=135),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.465', count(CONS_PRICE_IDX)=4186),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.601', count(CONS_PRICE_IDX)=111),\n",
       " Row(TARGET='no', CONS_PRICE_IDX='94.76700000000001', count(CONS_PRICE_IDX)=70)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = getDfGroup(rdd_df, 'CONS_PRICE_IDX')\n",
    "\n",
    "df_group.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ffa08",
   "metadata": {},
   "source": [
    "A moda do atributo no valor nulo agrupado por target é `93.91799999999999`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b9db5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificarNA(c):\n",
    "    c = c.upper()\n",
    "    if c == 'NONE' or c == 'NULL' or c == '' or c == 'NAN':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def mapNA(x):\n",
    "    AGE = x.AGE\n",
    "    JOB = x.JOB\n",
    "    MARITAL = x.MARITAL\n",
    "    EDUCATION = x.EDUCATION\n",
    "    DEFAULT = x.DEFAULT\n",
    "    HOUSING = x.HOUSING\n",
    "    LOAN = x.LOAN\n",
    "    CONTACT = x.CONTACT\n",
    "    MONTH = x.MONTH\n",
    "    DAY_OF_WEEK = x.DAY_OF_WEEK\n",
    "    CAMPAIGN = x.CAMPAIGN\n",
    "    PDAYS = x.PDAYS\n",
    "    PREVIOUS = x.PREVIOUS\n",
    "    POUTCOME = x.POUTCOME\n",
    "    EMP_VAR_RATE = x.EMP_VAR_RATE\n",
    "    CONS_PRICE_IDX = x.CONS_PRICE_IDX\n",
    "    CONS_CONF_IDX = x.CONS_CONF_IDX\n",
    "    EURIBOR3M = x.EURIBOR3M\n",
    "    EMPLOYED = x.EMPLOYED\n",
    "    TARGET = x.TARGET\n",
    "    \n",
    "    #Corrigindo valores missing\n",
    "    if verificarNA(x.MARITAL):\n",
    "        MARITAL = 'married'\n",
    "        \n",
    "    if verificarNA(x.DEFAULT):\n",
    "        DEFAULT = 'no'\n",
    "        \n",
    "    if verificarNA(x.HOUSING):\n",
    "        HOUSING = 'yes'\n",
    "        \n",
    "    if verificarNA(x.DAY_OF_WEEK):\n",
    "        DAY_OF_WEEK = 'mon'\n",
    "        \n",
    "    if verificarNA(x.POUTCOME):\n",
    "        POUTCOME = 'nonexistent'\n",
    "        \n",
    "    if verificarNA(x.CONS_PRICE_IDX):\n",
    "        CONS_PRICE_IDX = '93.91799999999999'\n",
    "    \n",
    "    \n",
    "    return (AGE, JOB, MARITAL, EDUCATION, DEFAULT, HOUSING, LOAN, CONTACT, MONTH, DAY_OF_WEEK, CAMPAIGN, PDAYS, PREVIOUS, POUTCOME, EMP_VAR_RATE, CONS_PRICE_IDX, CONS_CONF_IDX, EURIBOR3M, EMPLOYED, TARGET)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "16aa1d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AGE: string, JOB: string, MARITAL: string, EDUCATION: string, DEFAULT: string, HOUSING: string, LOAN: string, CONTACT: string, MONTH: string, DAY_OF_WEEK: string, CAMPAIGN: string, PDAYS: string, PREVIOUS: string, POUTCOME: string, EMP_VAR_RATE: string, CONS_PRICE_IDX: string, CONS_CONF_IDX: string, EURIBOR3M: string, EMPLOYED: string, TARGET: string]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_row = rdd_df.rdd.map(lambda x: mapNA(x))\n",
    "\n",
    "# Criando um Dataframe\n",
    "rdd_df = spSession.createDataFrame(rdd_row, schema=list_columns)\n",
    "rdd_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "821dc6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+--------+------+\n",
      "|AGE|      JOB|MARITAL|  EDUCATION|DEFAULT|HOUSING|LOAN|  CONTACT|MONTH|DAY_OF_WEEK|CAMPAIGN|PDAYS|PREVIOUS|   POUTCOME|EMP_VAR_RATE|CONS_PRICE_IDX|CONS_CONF_IDX|EURIBOR3M|EMPLOYED|TARGET|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+--------+------+\n",
      "| 56|housemaid|married|   basic.4y|     no|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|  5191.0|    no|\n",
      "| 57| services|married|high.school|unknown|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|  5191.0|    no|\n",
      "| 37| services|married|high.school|     no|    yes|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|  5191.0|    no|\n",
      "| 40|   admin.|married|   basic.6y|     no|     no|  no|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|  5191.0|    no|\n",
      "| 56| services|married|high.school|     no|     no| yes|telephone|  may|        mon|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|  5191.0|    no|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+-----+--------+-----------+------------+--------------+-------------+---------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa2333",
   "metadata": {},
   "source": [
    "### Alterando o tipo de dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fdc59965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE: Distinct count: 78\n",
      "+---+\n",
      "|AGE|\n",
      "+---+\n",
      "| 56|\n",
      "| 57|\n",
      "| 37|\n",
      "| 40|\n",
      "| 56|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "JOB: Distinct count: 12\n",
      "+---------+\n",
      "|      JOB|\n",
      "+---------+\n",
      "|housemaid|\n",
      "| services|\n",
      "| services|\n",
      "|   admin.|\n",
      "| services|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "MARITAL: Distinct count: 4\n",
      "+-------+\n",
      "|MARITAL|\n",
      "+-------+\n",
      "|married|\n",
      "|married|\n",
      "|married|\n",
      "|married|\n",
      "|married|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "EDUCATION: Distinct count: 8\n",
      "+-----------+\n",
      "|  EDUCATION|\n",
      "+-----------+\n",
      "|   basic.4y|\n",
      "|high.school|\n",
      "|high.school|\n",
      "|   basic.6y|\n",
      "|high.school|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "DEFAULT: Distinct count: 3\n",
      "+-------+\n",
      "|DEFAULT|\n",
      "+-------+\n",
      "|     no|\n",
      "|unknown|\n",
      "|     no|\n",
      "|     no|\n",
      "|     no|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "HOUSING: Distinct count: 3\n",
      "+-------+\n",
      "|HOUSING|\n",
      "+-------+\n",
      "|     no|\n",
      "|     no|\n",
      "|    yes|\n",
      "|     no|\n",
      "|     no|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "LOAN: Distinct count: 3\n",
      "+----+\n",
      "|LOAN|\n",
      "+----+\n",
      "|  no|\n",
      "|  no|\n",
      "|  no|\n",
      "|  no|\n",
      "| yes|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "CONTACT: Distinct count: 2\n",
      "+---------+\n",
      "|  CONTACT|\n",
      "+---------+\n",
      "|telephone|\n",
      "|telephone|\n",
      "|telephone|\n",
      "|telephone|\n",
      "|telephone|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "MONTH: Distinct count: 10\n",
      "+-----+\n",
      "|MONTH|\n",
      "+-----+\n",
      "|  may|\n",
      "|  may|\n",
      "|  may|\n",
      "|  may|\n",
      "|  may|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "DAY_OF_WEEK: Distinct count: 5\n",
      "+-----------+\n",
      "|DAY_OF_WEEK|\n",
      "+-----------+\n",
      "|        mon|\n",
      "|        mon|\n",
      "|        mon|\n",
      "|        mon|\n",
      "|        mon|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CAMPAIGN: Distinct count: 42\n",
      "+--------+\n",
      "|CAMPAIGN|\n",
      "+--------+\n",
      "|       1|\n",
      "|       1|\n",
      "|       1|\n",
      "|       1|\n",
      "|       1|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "PDAYS: Distinct count: 27\n",
      "+-----+\n",
      "|PDAYS|\n",
      "+-----+\n",
      "|  999|\n",
      "|  999|\n",
      "|  999|\n",
      "|  999|\n",
      "|  999|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "PREVIOUS: Distinct count: 8\n",
      "+--------+\n",
      "|PREVIOUS|\n",
      "+--------+\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "POUTCOME: Distinct count: 3\n",
      "+-----------+\n",
      "|   POUTCOME|\n",
      "+-----------+\n",
      "|nonexistent|\n",
      "|nonexistent|\n",
      "|nonexistent|\n",
      "|nonexistent|\n",
      "|nonexistent|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "EMP_VAR_RATE: Distinct count: 10\n",
      "+------------+\n",
      "|EMP_VAR_RATE|\n",
      "+------------+\n",
      "|         1.1|\n",
      "|         1.1|\n",
      "|         1.1|\n",
      "|         1.1|\n",
      "|         1.1|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CONS_PRICE_IDX: Distinct count: 26\n",
      "+--------------+\n",
      "|CONS_PRICE_IDX|\n",
      "+--------------+\n",
      "|        93.994|\n",
      "|        93.994|\n",
      "|        93.994|\n",
      "|        93.994|\n",
      "|        93.994|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CONS_CONF_IDX: Distinct count: 26\n",
      "+-------------+\n",
      "|CONS_CONF_IDX|\n",
      "+-------------+\n",
      "|        -36.4|\n",
      "|        -36.4|\n",
      "|        -36.4|\n",
      "|        -36.4|\n",
      "|        -36.4|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "EURIBOR3M: Distinct count: 316\n",
      "+---------+\n",
      "|EURIBOR3M|\n",
      "+---------+\n",
      "|    4.857|\n",
      "|    4.857|\n",
      "|    4.857|\n",
      "|    4.857|\n",
      "|    4.857|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "EMPLOYED: Distinct count: 11\n",
      "+--------+\n",
      "|EMPLOYED|\n",
      "+--------+\n",
      "|  5191.0|\n",
      "|  5191.0|\n",
      "|  5191.0|\n",
      "|  5191.0|\n",
      "|  5191.0|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "TARGET: Distinct count: 2\n",
      "+------+\n",
      "|TARGET|\n",
      "+------+\n",
      "|    no|\n",
      "|    no|\n",
      "|    no|\n",
      "|    no|\n",
      "|    no|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in list_columns:\n",
    "    print(f'{column}: Distinct count: {rdd_df.select(column).distinct().count()}')\n",
    "    rdd_df.select(column).show(5)\n",
    "    rdd_df.select(column).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef911455",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "64e898a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, BooleanType, DateType, StringType, FloatType\n",
    "\n",
    "rdd_df = rdd_df.withColumn('AGE', col('AGE').cast(IntegerType()))\n",
    "rdd_df = rdd_df.withColumn('CAMPAIGN ', col('CAMPAIGN').cast(IntegerType()))\n",
    "rdd_df = rdd_df.withColumn('AGE', col('AGE').cast(IntegerType()))\n",
    "rdd_df = rdd_df.withColumn('PDAYS', col('PDAYS').cast(IntegerType()))\n",
    "rdd_df = rdd_df.withColumn('PREVIOUS', col('PREVIOUS').cast(IntegerType()))\n",
    "\n",
    "rdd_df = rdd_df.withColumn('EMP_VAR_RATE', col('EMP_VAR_RATE').cast(FloatType()))\n",
    "rdd_df = rdd_df.withColumn('CONS_PRICE_IDX', col('CONS_PRICE_IDX').cast(FloatType()))\n",
    "rdd_df = rdd_df.withColumn('CONS_CONF_IDX', col('CONS_CONF_IDX').cast(FloatType()))\n",
    "rdd_df = rdd_df.withColumn('EURIBOR3M', col('EURIBOR3M').cast(FloatType()))\n",
    "rdd_df = rdd_df.withColumn('EMPLOYED', col('EMPLOYED').cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bf6b7aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AGE', 'int'),\n",
       " ('JOB', 'string'),\n",
       " ('MARITAL', 'string'),\n",
       " ('EDUCATION', 'string'),\n",
       " ('DEFAULT', 'string'),\n",
       " ('HOUSING', 'string'),\n",
       " ('LOAN', 'string'),\n",
       " ('CONTACT', 'string'),\n",
       " ('MONTH', 'string'),\n",
       " ('DAY_OF_WEEK', 'string'),\n",
       " ('CAMPAIGN', 'string'),\n",
       " ('PDAYS', 'int'),\n",
       " ('PREVIOUS', 'int'),\n",
       " ('POUTCOME', 'string'),\n",
       " ('EMP_VAR_RATE', 'float'),\n",
       " ('CONS_PRICE_IDX', 'float'),\n",
       " ('CONS_CONF_IDX', 'float'),\n",
       " ('EURIBOR3M', 'float'),\n",
       " ('EMPLOYED', 'float'),\n",
       " ('TARGET', 'string'),\n",
       " ('CAMPAIGN ', 'int')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb83c7a",
   "metadata": {},
   "source": [
    "### Normalização e Encoding dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9316d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a7c2c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_str = ['JOB', 'MARITAL', 'EDUCATION', 'DEFAULT', 'HOUSING', \n",
    "               'LOAN', 'CONTACT', 'MONTH', 'DAY_OF_WEEK', 'POUTCOME', 'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "318271b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função expandir as colunas que foram geradas pelo OneHotEncoding.\n",
    "def _oneHotEncoder(df, column):\n",
    "    \n",
    "    alias = f'_{column}'\n",
    "    df_col_onehot = df.select('*', vector_to_array(column).alias(alias))\n",
    "\n",
    "\n",
    "    num_categories = len(df_col_onehot.first()[alias])\n",
    "    cols_expanded = [(F.col(alias)[i]) for i in range(num_categories)]\n",
    "    df_cols_onehot = df_col_onehot.select('*', *cols_expanded)\n",
    "    \n",
    "    return df_cols_onehot, cols_expanded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c1a5c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder\n",
    "indexers = [ StringIndexer(inputCol=c, outputCol=\"OHE_{0}\".format(c)) for c in columns_str]\n",
    "\n",
    "# Criação do array de Encoders\n",
    "encoders = [\n",
    "    OneHotEncoder(\n",
    "        inputCol=indexer.getOutputCol(),\n",
    "        outputCol=\"_{0}\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "32d7633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica os dois métodos: LabelEncoder e OneHotEncoder\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "df_temp = pipeline.fit(rdd_df).transform(rdd_df)\n",
    "\n",
    "# Obtendo todas as colunas de saída do Encoder\n",
    "encoder_columns = [encoder.getOutputCol() for encoder in encoders]\n",
    "\n",
    "# Expandindo o array gerado pelo OneHotEncoding para Colunas individuais\n",
    "ohe_columns = []\n",
    "for column in encoder_columns: \n",
    "    df_temp, _ohe = _oneHotEncoder(df_temp, column)\n",
    "    \n",
    "    ohe_columns.extend(_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "27385436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+------------+--------------+-------------+---------+--------+------+--------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+-------------+----------------+----------------+----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+----------------+----------------+----------------+----------------+-------------+-------------+----------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------+---------------+\n",
      "|AGE|PDAYS|PREVIOUS|EMP_VAR_RATE|CONS_PRICE_IDX|CONS_CONF_IDX|EURIBOR3M|EMPLOYED|TARGET|CAMPAIGN|__OHE_JOB[0]|__OHE_JOB[1]|__OHE_JOB[2]|__OHE_JOB[3]|__OHE_JOB[4]|__OHE_JOB[5]|__OHE_JOB[6]|__OHE_JOB[7]|__OHE_JOB[8]|__OHE_JOB[9]|__OHE_JOB[10]|__OHE_MARITAL[0]|__OHE_MARITAL[1]|__OHE_MARITAL[2]|__OHE_EDUCATION[0]|__OHE_EDUCATION[1]|__OHE_EDUCATION[2]|__OHE_EDUCATION[3]|__OHE_EDUCATION[4]|__OHE_EDUCATION[5]|__OHE_EDUCATION[6]|__OHE_DEFAULT[0]|__OHE_DEFAULT[1]|__OHE_HOUSING[0]|__OHE_HOUSING[1]|__OHE_LOAN[0]|__OHE_LOAN[1]|__OHE_CONTACT[0]|__OHE_MONTH[0]|__OHE_MONTH[1]|__OHE_MONTH[2]|__OHE_MONTH[3]|__OHE_MONTH[4]|__OHE_MONTH[5]|__OHE_MONTH[6]|__OHE_MONTH[7]|__OHE_MONTH[8]|__OHE_DAY_OF_WEEK[0]|__OHE_DAY_OF_WEEK[1]|__OHE_DAY_OF_WEEK[2]|__OHE_DAY_OF_WEEK[3]|__OHE_POUTCOME[0]|__OHE_POUTCOME[1]|__OHE_TARGET[0]|\n",
      "+---+-----+--------+------------+--------------+-------------+---------+--------+------+--------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+-------------+----------------+----------------+----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+----------------+----------------+----------------+----------------+-------------+-------------+----------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------+---------------+\n",
      "| 56|  999|       0|         1.1|        93.994|        -36.4|    4.857|  5191.0|    no|       1|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         0.0|         1.0|         0.0|          0.0|             1.0|             0.0|             0.0|               0.0|               0.0|               0.0|               0.0|               1.0|               0.0|               0.0|             1.0|             0.0|             0.0|             1.0|          1.0|          0.0|             0.0|           1.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|           0.0|                 0.0|                 1.0|                 0.0|                 0.0|              1.0|              0.0|            1.0|\n",
      "+---+-----+--------+------------+--------------+-------------+---------+--------+------+--------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+-------------+----------------+----------------+----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+----------------+----------------+----------------+----------------+-------------+-------------+----------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------------+--------------------+--------------------+--------------------+-----------------+-----------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_res = df_temp.select('AGE', 'PDAYS', 'PREVIOUS', \n",
    "                         'EMP_VAR_RATE', 'CONS_PRICE_IDX', \n",
    "                         'CONS_CONF_IDX', 'EURIBOR3M', 'EMPLOYED', \n",
    "                         'TARGET', 'CAMPAIGN', *ohe_columns)\n",
    "\n",
    "df_res.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c9093",
   "metadata": {},
   "source": [
    "### Pré Processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5249c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformaVar(row) :\n",
    "    obj = (row['TARGET'], row['__OHE_TARGET[0]'], Vectors.dense([\n",
    "                                                            row['AGE'],\n",
    "                                                            row['PDAYS'],\n",
    "                                                            row['PREVIOUS'],\n",
    "                                                            row['EMP_VAR_RATE'],\n",
    "                                                            row['CONS_PRICE_IDX'],\n",
    "                                                            row['CONS_CONF_IDX'],\n",
    "                                                            row['EURIBOR3M'],\n",
    "                                                            row['EMPLOYED'],\n",
    "                                                            row['CAMPAIGN'],\n",
    "                                                            row['__OHE_JOB[0]'],\n",
    "                                                            row['__OHE_JOB[1]'],\n",
    "                                                            row['__OHE_JOB[2]'],\n",
    "                                                            row['__OHE_JOB[3]'],\n",
    "                                                            row['__OHE_JOB[4]'],\n",
    "                                                            row['__OHE_JOB[5]'],\n",
    "                                                            row['__OHE_JOB[6]'],\n",
    "                                                            row['__OHE_JOB[7]'],\n",
    "                                                            row['__OHE_JOB[8]'],\n",
    "                                                            row['__OHE_JOB[9]'],\n",
    "                                                            row['__OHE_JOB[10]'],\n",
    "                                                            row['__OHE_MARITAL[0]'],\n",
    "                                                            row['__OHE_MARITAL[1]'],\n",
    "                                                            row['__OHE_MARITAL[2]'],\n",
    "                                                            row['__OHE_EDUCATION[0]'],\n",
    "                                                            row['__OHE_EDUCATION[1]'],\n",
    "                                                            row['__OHE_EDUCATION[2]'],\n",
    "                                                            row['__OHE_EDUCATION[3]'],\n",
    "                                                            row['__OHE_EDUCATION[4]'],\n",
    "                                                            row['__OHE_EDUCATION[5]'],\n",
    "                                                            row['__OHE_EDUCATION[6]'],\n",
    "                                                            row['__OHE_DEFAULT[0]'],\n",
    "                                                            row['__OHE_DEFAULT[1]'],\n",
    "                                                            row['__OHE_HOUSING[0]'],\n",
    "                                                            row['__OHE_HOUSING[1]'],\n",
    "                                                            row['__OHE_LOAN[0]'],\n",
    "                                                            row['__OHE_LOAN[1]'],\n",
    "                                                            row['__OHE_CONTACT[0]'],\n",
    "                                                            row['__OHE_MONTH[0]'],\n",
    "                                                            row['__OHE_MONTH[1]'],\n",
    "                                                            row['__OHE_MONTH[2]'],\n",
    "                                                            row['__OHE_MONTH[3]'],\n",
    "                                                            row['__OHE_MONTH[4]'],\n",
    "                                                            row['__OHE_MONTH[5]'],\n",
    "                                                            row['__OHE_MONTH[6]'],\n",
    "                                                            row['__OHE_MONTH[7]'],\n",
    "                                                            row['__OHE_MONTH[8]'],\n",
    "                                                            row['__OHE_DAY_OF_WEEK[0]'],\n",
    "                                                            row['__OHE_DAY_OF_WEEK[1]'],\n",
    "                                                            row['__OHE_DAY_OF_WEEK[2]'],\n",
    "                                                            row['__OHE_DAY_OF_WEEK[3]'],\n",
    "                                                            row['__OHE_POUTCOME[0]'],\n",
    "                                                            row['__OHE_POUTCOME[1]']\n",
    "                                                            ]))\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9b74dc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('no',\n",
       "  1.0,\n",
       "  DenseVector([56.0, 999.0, 0.0, 1.1, 93.994, -36.4, 4.857, 5191.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]))]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_processing = df_res.rdd.map(transformaVar)\n",
    "\n",
    "rdd_processing.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "790af685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+\n",
      "|target|label|            features|\n",
      "+------+-----+--------------------+\n",
      "|    no|  1.0|[56.0,999.0,0.0,1...|\n",
      "|    no|  1.0|[57.0,999.0,0.0,1...|\n",
      "|    no|  1.0|[37.0,999.0,0.0,1...|\n",
      "|    no|  1.0|[40.0,999.0,0.0,1...|\n",
      "|    no|  1.0|[56.0,999.0,0.0,1...|\n",
      "|    no|  1.0|[45.0,999.0,0.0,1...|\n",
      "|    no|  1.0|[59.0,999.0,0.0,1...|\n",
      "|    no|  1.0|[41.0,999.0,0.0,1...|\n",
      "|    no|  1.0|[24.0,999.0,0.0,1...|\n",
      "|    no|  1.0|[25.0,999.0,0.0,1...|\n",
      "+------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[target: string, label: double, features: vector]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processing = spSession.createDataFrame(rdd_processing, [\"target\", \"label\", \"features\"])\n",
    "df_processing.show(10)\n",
    "df_processing.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1800f2",
   "metadata": {},
   "source": [
    "### Escala dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8e00df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RobustScaler, StandardScaler, MinMaxScaler, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0647734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features', outputCol='features_scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ff85f",
   "metadata": {},
   "source": [
    "### Divisão em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e74f207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32007, 8381, 800)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dados de Treino e de Teste\n",
    "(dados_treino, dados_teste, dados_valid) = df_processing.randomSplit([0.78, 0.2, 0.02])\n",
    "\n",
    "dados_treino.count(), dados_teste.count(), dados_valid.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59449c0f",
   "metadata": {},
   "source": [
    "#### Tratamento das classes desbalanceadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "aed67e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|    no|28379|\n",
      "|   yes| 3628|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dados_treino.groupBy('target').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "68d5afd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28379, 3628)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_1 = dados_treino[dados_treino['target'] == 'no']\n",
    "df_target_0 = dados_treino[dados_treino['target'] == 'yes']\n",
    "\n",
    "df_target_1.count(), df_target_0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "394e600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e89be807",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = float(floor(df_target_1.count() / df_target_0.count()))\n",
    "\n",
    "df_target_0_sample = df_target_0.sample(True, fraction, 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "f6f064c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df_target_0_sample.count() - df_target_0.count()\n",
    "\n",
    "df_temp = spSession.createDataFrame(df_target_0_sample.collect()[0:diff])\n",
    "\n",
    "df_temp = df_temp.unionAll(df_target_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c7e27e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino =  df_temp.unionAll(df_target_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "9ed21a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53908"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_treino.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e4b21b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|    no|28379|\n",
      "|   yes|25529|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dados_treino.groupBy('target').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90a1e7",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "373f8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "713fa762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtClassifer = RandomForestClassifier(labelCol = \"label\", featuresCol = \"features\")\n",
    "#dtClassifer = GBTClassifier(labelCol = \"label\", featuresCol = \"features_scaled\")\n",
    "dtClassifer = LinearSVC(labelCol = \"label\", featuresCol = \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91171b6f",
   "metadata": {},
   "source": [
    "### Aplicando Pipeline, Treinando e Prevendo o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "110fac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[scaler, dtClassifer])\n",
    "\n",
    "model = pipeline.fit(dados_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "4b13e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões com dados de teste\n",
    "previsoes = model.transform(dados_teste)\n",
    "\n",
    "#previsoes.select(\"prediction\",\"target\",\"label\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b073f",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e7afc904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7660945495380374"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avaliando a acurácia\n",
    "avaliador = MulticlassClassificationEvaluator(predictionCol = \"prediction\", labelCol = \"label\", metricName = \"f1\")\n",
    "avaliador.evaluate(previsoes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d9f0e5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0| 5336|\n",
      "|  0.0|       1.0|  267|\n",
      "|  1.0|       0.0| 2123|\n",
      "|  0.0|       0.0|  655|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resumindo as previsões - Confusion Matrix\n",
    "previsoes.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf88c89",
   "metadata": {},
   "source": [
    "### Salvando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "409970b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "model.write().overwrite().save('models/modelo_v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b539f",
   "metadata": {},
   "source": [
    "### Carregando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ef695a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d0c52765",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load = PipelineModel.load('models/modelo_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "4abccc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7906905011188439"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_valid = model_load.transform(dados_valid)\n",
    "\n",
    "avaliador = MulticlassClassificationEvaluator(predictionCol = \"prediction\", labelCol = \"label\", metricName = \"f1\")\n",
    "avaliador.evaluate(preds_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "cd08f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|  523|\n",
      "|  0.0|       1.0|   16|\n",
      "|  1.0|       0.0|  187|\n",
      "|  0.0|       0.0|   74|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resumindo as previsões - Confusion Matrix\n",
    "preds_valid.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e044d0d",
   "metadata": {},
   "source": [
    "### Considerações Finais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
