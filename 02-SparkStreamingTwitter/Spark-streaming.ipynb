{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d09e8e",
   "metadata": {},
   "source": [
    "# PySpark e Streaming de Dados\n",
    "\n",
    "\n",
    "## Spark Streaming\n",
    "O Spark Streaming foi lançado em 2013 para permitir que engenheiros de dados e cientistas de dados processassem dados em tempo real de bancos de dados SQL, Flume, Amazon Kinesis etc. Streams discretos, ou DStreams, são abstrações fundamentais aqui, pois representam fluxos de dados divididos em pequenos pedaços (referidos como lotes).\n",
    "\n",
    "A proposta do Spark Stream é analisar dados em tempo real, e não esperar horas para fazer a análise e processamento. \"A vida não acontece em batches\".\n",
    "Streaming de dados não é apenas para projetos altamente especializados. Computação baseada em Streaming está se tornando a regra para empresas orientadas a dados.\n",
    "\n",
    "O Spark Streaming ganhou rápida adoção por causa de seus diferentes recursos de processamento de dados, pois facilitar para os desenvolvedores de big data confiar em uma única estrutura usando uma única estrutura para atender a todos os requisitos de processamento é fácil para os desenvolvedores de big data. Os modelos podem ser treinados offline usando MLlib (biblioteca de aprendizado de máquina do Spark) e, em seguida, usados online para pontuação de dados de streaming usando o Spark Streaming. Alguns modelos podem aprender e pontuar continuamente enquanto os dados de streaming são coletados. Além disso, o Spark SQL permite combinar dados de streaming com uma ampla variedade de fontes de dados estáticos. Por exemplo, o Amazon Redshift pode carregar dados estáticos no Spark e processá-los antes de enviá-los para sistemas downstream.\n",
    "\n",
    "<img src=\"assets/figura01.png\">\n",
    "\n",
    "Uma das principais fontes de dados contínuos são os sensores, da internet das coisas. Existem quatro areas principais que o Spark Streaming vem sendo utilizado:\n",
    "\n",
    "* Streaming ETL;\n",
    "* Detecção de anomalias;\n",
    "* Enriquecimento de dados;\n",
    "* Sessões complexas e aprendizado contínuo.\n",
    "\n",
    "Uma importante vantagem de usar o Spark para Big Data Analytics é a possibilidade de combinar processamento em batch e processamento de streaming em um único sistema.\n",
    "\n",
    "* **Batch**: Você inicia o processamento de um arquivo ou dataset finito, o spark processa as tarefas configuradas e conclui o trabalho.\n",
    "* **Streaming**: Você processa um stream de dados contínuos; a execução não pára até que haja algum erro ou você termine a aplicação manualmente.\n",
    "\n",
    "\n",
    "## Exemplos de Casos de uso do Apache Spark Streaming\n",
    "\n",
    "Existem mais de 3.000 empresas que usam o Spark Streaming, incluindo empresas como Zendesk, Uber, Netflix e Pinterest:\n",
    "\n",
    "1. Para criar análises de telemetria em tempo real, a Uber coleta terabytes de dados de eventos todos os dias de seus usuários móveis. Os dados brutos do evento podem ser convertidos em dados estruturados coletados usando um pipeline ETL contínuo baseado em Kafka, Spark Streaming e HDFS.\n",
    "\n",
    "2. Um pipeline de dados semelhante foi criado para o Pinterest para alimentar dados do Kafka no Spark via Spark Streaming, fornecendo informações imediatas sobre como os pinos interagem globalmente em tempo real. Ele ajuda o Pinterest a melhorar suas recomendações em tempo real, sugerindo Pins relacionados aos usuários enquanto eles navegam no site em busca de lugares para ir, produtos para comprar, receitas para preparar e muito mais.\n",
    "\n",
    "3. Um mecanismo em tempo real que fornece recomendações de filmes para usuários da Netflix é construído com Kafka e Spark Streaming, que a Netflix usa para lidar com bilhões de eventos por dia de várias fontes.\n",
    "\n",
    "\n",
    "\n",
    "## DStreams: Discretized Streams\n",
    "\n",
    "Assim como os RDDs são a base do Apache Spark, os DStreams são a base do Apache Spark Streaming.\n",
    "O dstream é uma sequência de dados que são coletados ao longo do tempo. Internamente, um dstream é representado por uma sequência de RDDs coletados em cada intervalo de tempo. Pode ser criado por diversas fontes, como Kafka, Flume, Twitter, etc..\n",
    "Uma vez que são criados, os dstreams oferecem dois tipos e operações:\n",
    "\n",
    "* **Transformações**: Geram um novo dstream;\n",
    "* **Ações (operações de output)**: Gravam os dados em um sistema de armazenamento ou outra fonte externa. Os DStreams oferecem muitas das operações que podem ser realizadas com os RDDs, mais operações relacionadas ao tempo, como sliding windows.\n",
    "\n",
    "<img src=\"assets/figura02.PNG\">\n",
    "\n",
    "**O que pode ser feito com DStreams:**\n",
    "* Map;\n",
    "* FlatMap;\n",
    "* Filter;\n",
    "* ReduceByKey;\n",
    "* Join;\n",
    "* Window;\n",
    "* Manter o controle de estado dos dados (Stateful Data)\n",
    "\n",
    "**Windowing** (Computação em uma janela de tempo): A cada janela de tempo, um RDD é criado no DStream, podemos querer ver o que acontece em um determinado intervalo de tempo.\n",
    "\n",
    "* **Window length**: Duração da window;\n",
    "* **Sliding interval**: Intervalo entre as windows.\n",
    "\n",
    "Exemplo de uso:\n",
    "\n",
    "> `ssc = StreamingContext(sc, INTERVALO_BATCH);`<br/>\n",
    "> `window(windowDuration: Duration, slideDuration: Duration): DStream[T]`\n",
    "\n",
    "Windowing permite computar os resultados ao longo de períodos de tempo maiores que o batch interval.\n",
    "\n",
    "São três os intervalos de tempo que devemos considerar ao trabalhar com streaming:\n",
    "\n",
    "* Batch interval: Frequência com que os dados são capturados em um DStream;\n",
    "* Frequência com que uma window é aplicada;\n",
    "* Intervalo de tepo capturado para computação e geração de resultados;\n",
    "\n",
    "**Principais mecanismos de tolerância a falhas:**\n",
    "\n",
    "* Todos os dados são replicados para no mínimo 2 worker nodes;\n",
    "* **ssc.checkpoint()**: Um diretório de checkpoint pode ser usado para armazenar o estado do streaming de dados, no caso em que é necessário reiniciar o streaming.\n",
    "* **Falha no Receiver**: Alguns receivers são melhores que outros. Receiver como Twitter, Kafka e Flume não permitem recuperação de dados. Se o receiver falha, os dados do streaming são perdidos. Outros garantem a recuperação dos dados em caso de falhas: HDFS, Directly-consumed Kafka, Pull-based Flume.\n",
    "* **Falha no Driver Context**: Embora os dados sejam replicados para os worker nodes, o DriverContext é executado no node master e este pode ser um ponto único de falha. Podemos usar o checkpoint() para recuperar dados em caso de falhas e usamos a função streamingContext.getOrCreate() para continuar o processamento de onde ele foi interrompido em caso de falha. Em caso de falha no script sendo executado no DriverContext, podemos reiniciar automaticamente o processo de streaming, usando o Zookeeper (no modo supervise). O zookeeper é um cluster manager usado pelo spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30acaed",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "* [1] **PROJECTPRO**. *A Beginners Guide to Spark Streaming Architecture with Example*, 2022. Disponível em <<a target=\"_blank\" href=\"https://www.projectpro.io/article/spark-streaming-example/540\">ProjectPro.com/spark-streaming-guide</a>>\n",
    "\n",
    "\n",
    "* [2] **APACHE SPARK**. *Structured Streaming Programming Guide*, 2022. Disponível em: <<a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">spark.apache.org/structured-streaming-programming-guide.</a>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910075e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exemplos de Streaming de Dados\n",
    "\n",
    "\n",
    "### Exemplo 1: Coletando dados do Twitter via socket e exibindo no console em tempo real\n",
    "\n",
    "Este pode ser considerado um dos exemplos mais simples de uso do Spark Streaming. Neste exemplo nós vamos coletar os dados que virão diretamente da porta 5554. Nessa porta por sua vez estará sendo executada a aplicação responsável por coletar os dados do Twitter em tempo real.\n",
    "\n",
    "Aqui neste projeto nós iremos aplicar os métodos de mapreduce para separar as palavras obtidas em cada tweet. Por fim, as palavras já transformadas serão exibidas no console do Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a3bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Criação de um StreamingContext local com as threads de trabalho e intervalo de batch de 1 segundo\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Criação de um DStream que se conectará ao hostname:port, como por exemplo: localhost:9999\n",
    "lines = ssc.socketTextStream(\"127.0.0.1\", 5554)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ce46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divida cada linha em palavras\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Conte cada palavra em cada lote\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Imprima os primeiros dez elementos de cada RDD gerado neste DStream para o console\n",
    "wordCounts.pprint(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150bc31",
   "metadata": {},
   "source": [
    "Execute a aplicação **tweets-listener.py** no terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f889e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:34:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:34:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:34:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:34:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:35\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inicia o listen\n",
    "ssc.start()\n",
    "\n",
    "# Aguardar o término do listen ou o limite de 120 segundos\n",
    "ssc.awaitTerminationOrTimeout(120)  \n",
    "\n",
    "# Encerra o Listen\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98965fff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exemplo 2: Coletando dados do Twitter via socket e exibindo um gráfico em tempo real\n",
    "\n",
    "Agora iremos aplicar mais de técnicas para que possamos obter informações mais úteis do Streaming de dados. Neste exemplo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "935bc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "from collections import namedtuple\n",
    "\n",
    "# Can only run this once. restart your kernel for any errors.\n",
    "\n",
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)\n",
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5554)\n",
    "\n",
    "lines = socket_stream.window( 100 )\n",
    "\n",
    "fields = (\"text\")\n",
    "\n",
    "# Use Parenthesis for multiple lines or use \\.\n",
    "Tweet = namedtuple( 'Tweet', fields )\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) \n",
    "     .filter( lambda word: word.lower().startswith(\"#\"))  \n",
    "     .map( lambda word: ( word.lower(), 1 ) ) \n",
    "     .reduceByKey( lambda a, b: a + b ) \n",
    "     .map( lambda rec: Tweet( rec[0], rec[1] ) ) \n",
    "     .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) \n",
    "     .limit(100).registerTempTable(\"tweets\") ) ) # Registers to a table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca5573",
   "metadata": {},
   "source": [
    "## Now run TweetListener.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f68aa",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f106b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Only works for Jupyter Notebooks!\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    \n",
    "    time.sleep( 3 )\n",
    "    top_10_tweets = sqlContext.sql( 'Select tag, count from tweets' )\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure( figsize = ( 10, 8 ) )\n",
    "    sns.barplot( x=\"count\", y=\"tag\", data=top_10_df)\n",
    "    plt.show()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95493ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = sqlContext.sql( 'Select * from tweets' )\n",
    "df = tweets.toPandas()\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
