{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row #Converte RDDs em objetos do tipo Row\n",
    "from pyspark.sql.functions import col, isnan, when, count # Encontra a contagem para valores None, Null, Nan, etc.\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removerStopwords(texto):\n",
    "    por = stopwords.words('portuguese')\n",
    "    eng = stopwords.words('english')\n",
    "    spa = stopwords.words('spanish')\n",
    "\n",
    "    stop_words = por + eng + spa\n",
    "    \n",
    "    texto = ' '.join(palavra for palavra in texto.split(' ') if palavra not in stop_words)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6776d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limparTexto(texto):\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    texto = emoji_pattern.sub(r'', texto) # no emoji\n",
    "    \n",
    "    # Remove non-english chars: chinese, arabic, korean, etc...\n",
    "    nonenglish_pattern = re.compile(u'[^\\u0000-\\u05C0\\u2100-\\u214F]+', flags=re.UNICODE)\n",
    "    texto = nonenglish_pattern.sub(r'', texto)\n",
    "    \n",
    "    #Vamos transformar o texto em lowercase, remover textos entre colchetes, links, pontua√ß√µes e palavras que contenham n√∫meros.\n",
    "    texto = str(texto).lower()\n",
    "    texto = re.sub('\\[.*?\\]', '', texto) #Removendo textos entre colchetes\n",
    "    texto = re.sub('<.*?>+', '', texto)  # Remove textos entre <>\n",
    "    texto = re.sub('https?://\\S+|www\\.\\S+', '', texto) #Removendo links\n",
    "    texto = re.sub('[@#]\\S+', '', texto) #Removendo arrobas e hashtags\n",
    "    texto = re.sub('\\w*\\d\\w*', '', texto) #Remove palavras contendo d√≠gitos no meio.\n",
    "        \n",
    "    texto = re.sub(r'[%@#\\t\\n\\r]+', '', texto) #Remove caracteres especiais\n",
    "    texto = re.sub(r'[ ]+', ' ', texto) #Remove mais do que um espa√ßo em branco\n",
    "    \n",
    "    return (texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"127.0.0.1\", 5554)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words_trans = lines.map(lambda x: limparTexto(x)).map(lambda x: removerStopwords(x))\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f889e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4078b0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc51d86",
   "metadata": {},
   "source": [
    "Links: \n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#programming-model\n",
    "\n",
    "https://github.com/syalanuj/youtube/blob/main/spark_streaming_with_python_in_12_minutes/spark_st_run.ipynb\n",
    "\n",
    "https://github.com/Krupique/cursos-datascience-conteudo/blob/main/DSA_DS-02-BigData%20Analytics%20com%20Python%20e%20Spark/05%20-%20Introducao-SparkStreaming.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c83125",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [\n",
    "    '√â necess√°rio muita a√ß√£o',\n",
    "    'I plan nothing on my weekend ÔºÅ#cute #ÂèØÁà± #Ëêå #cat https://t.co/JNbftLiVOM',\n",
    "         '@PAVGOD: Whenever someone asks if Pavlov is a cuddle dog, \\\n",
    "         I try to explain that this is his reaction üòÇ https://t.co/KGrI4pHEXV',\n",
    "         'Cat feed cat https://t.co/1Avv8qhT4J üó£Ô∏è\"Tenim el deure de complir amb el mandat del 52 %  \\\n",
    "         davan√ßar cap a la independ√®ncia'\n",
    "        ]\n",
    "\n",
    "rdd = sc.parallelize(lista)\n",
    "\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc55b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd.flatMap(lambda line: line.split(' '))\n",
    "\n",
    "rdd1.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd.map(lambda x: limparTexto(x)).map(lambda x: removerStopwords(x))\n",
    "\n",
    "rdd3.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98965fff",
   "metadata": {},
   "source": [
    "## Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935bc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "from collections import namedtuple\n",
    "\n",
    "# Can only run this once. restart your kernel for any errors.\n",
    "\n",
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)\n",
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5554)\n",
    "\n",
    "lines = socket_stream.window( 1000 )\n",
    "\n",
    "fields = (\"text\")\n",
    "\n",
    "# Use Parenthesis for multiple lines or use \\.\n",
    "Tweet = namedtuple( 'Tweet', fields )\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) \n",
    "     #.filter( lambda word: word.lower().startswith(\"http\") )  \n",
    "     .map( lambda word: ( word.lower(), 1 ) ) \n",
    "     .reduceByKey( lambda a, b: a + b ) \n",
    "     .map( lambda rec: Tweet( rec[0], rec[1] ) ) \n",
    "     .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) \n",
    "     .limit(100).registerTempTable(\"tweets\") ) ) # Registers to a table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca5573",
   "metadata": {},
   "source": [
    "## Now run TweetListener.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f68aa",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f106b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Only works for Jupyter Notebooks!\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59079a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95493ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = sqlContext.sql( 'Select * from tweets' )\n",
    "df = tweets.toPandas()\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
