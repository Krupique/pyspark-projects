{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a792dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row #Converte RDDs em objetos do tipo Row\n",
    "from pyspark.sql.functions import col, isnan, when, count # Encontra a contagem para valores None, Null, Nan, etc.\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder #Converte strings em valores numéricos\n",
    "from pyspark.ml.linalg import Vectors #Serve para criar um vetor denso\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # Para avaliar o modelo com as métricas de avaliação.\n",
    "from pyspark.ml.feature import RobustScaler, StandardScaler, MinMaxScaler, Normalizer # Métodos para escalas dos dados\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier, LinearSVC # Algoritmos de ML\n",
    "from pyspark.ml import Pipeline # Criação de um Pipeline de execução.\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator # GridSearch e Validação Cruzada\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator # Evaluator para classificação binária\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab09b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos usados\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from operator import add\n",
    "import requests_oauthlib\n",
    "from time import gmtime, strftime\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import ast\n",
    "import json\n",
    "#import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8bbf0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pacote NLTK\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69dc8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequência de update\n",
    "INTERVALO_BATCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac1652c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o StreamingContext\n",
    "ssc = StreamingContext(sc, INTERVALO_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00c341",
   "metadata": {},
   "source": [
    "## Treinando o Classificador de Análise de Sentimento\n",
    "\n",
    "Uma parte essencial da criação de um algoritmo de análise de sentimento (ou qualquer algoritmo de mineração de dados) é ter um conjunto de dados abrangente ou \"Corpus\" para o aprendizado, bem como um conjunto de dados de teste para garantir que a precisão do seu algoritmo atende aos padrões que você espera. Isso também permitirá que você ajuste o seu algoritmo a fim de deduzir melhores (ou mais precisas) características de linguagem natural que você poderia extrair do texto e que vão contribuir para a classificação de sentimento, em vez de usar uma abordagem genérica. Tomaremos como base o dataset de treino fornecido pela Universidade de Michigan, para competições do Kaggle (https://inclass.kaggle.com/c/si650winter11).\n",
    "\n",
    "\n",
    "Esse dataset contém 1,578,627 tweets classificados e cada linha é marcada como: \n",
    "\n",
    "#### 1 para o sentimento positivo \n",
    "#### 0 para o sentimento negativo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9ef0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session - usada quando se trabalha com Dataframes no Spark\n",
    "spSession = SparkSession.builder.master(\"local\").appName(\"tw-session\").config(\"spark.some.config.option\", \"session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "252d76a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o arquivo texto e criando um RDD em memória com Spark\n",
    "rdd = sc.textFile(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "945bf448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE', 'LABEL']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = rdd.first()\n",
    "rdd_body = rdd.filter(lambda x: header not in x)#.map(lambda l: l.split(','))\n",
    "\n",
    "list_columns = header.replace('.', '_').upper().split(',')\n",
    "list_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c56435",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_body = rdd_body.map(lambda x: x.replace(',0', ';0')).map(lambda x: x.replace(',1', ';1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3af3edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ok brokeback mountain is such a horrible movie.;0',\n",
       " 'Brokeback Mountain was so awesome.;1',\n",
       " 'friday hung out with kelsie and we went and saw The Da Vinci Code SUCKED!!!!!;0',\n",
       " 'I am going to start reading the Harry Potter series again because that is one awesome story.;1',\n",
       " 'Is it just me, or does Harry Potter suck?...;0',\n",
       " 'The Da Vinci Code sucked big time.;0',\n",
       " 'I am going to start reading the Harry Potter series again because that is one awesome story.;1',\n",
       " 'For those who are Harry Potter ignorant, the true villains of this movie are awful creatures called dementors.;0',\n",
       " 'Harry Potter dragged Draco Malfoy ’ s trousers down past his hips and sucked him into his throat with vigor, making whimpering noises and panting and groaning around the blonds rock-hard, aching cock...;0',\n",
       " \"So as felicia's mom is cleaning the table, felicia grabs my keys and we dash out like freakin mission impossible.;1\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_body.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdd0d54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TEXT='O', SENTIMENT='k'),\n",
       " Row(TEXT='B', SENTIMENT='r'),\n",
       " Row(TEXT='f', SENTIMENT='r'),\n",
       " Row(TEXT='I', SENTIMENT=' '),\n",
       " Row(TEXT='I', SENTIMENT='s')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_row = rdd_body.map(lambda p: Row(\n",
    "    TEXT = p[0], \n",
    "    SENTIMENT = p[1]\n",
    "))\n",
    "\n",
    "rdd_row.take(10)\n",
    "\n",
    "# Criando um Dataframe\n",
    "rdd_df = spSession.createDataFrame(rdd_row)\n",
    "rdd_df.cache()\n",
    "\n",
    "rdd_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84698ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função separa as colunas em cada linha, cria uma tupla e remove a pontuação.\n",
    "def get_row(line):\n",
    "    row = line.split(';')\n",
    "    \n",
    "    tweet = row[0].strip()\n",
    "    sentimento = int(re.sub('[^\\d]+', '', row[1]))\n",
    "    \n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    #translator = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    #tweet = regex.sub('', tweet)\n",
    "    tweet = tweet.translate(translator)\n",
    "    tweet = tweet.split(' ')\n",
    "    tweet_lower = []\n",
    "    for word in tweet:\n",
    "        tweet_lower.append(word.lower())\n",
    "    return (tweet_lower, sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd057ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplcia a função a cada linha do dataset\n",
    "dataset_treino = rdd_body.map(lambda line: get_row(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75bcd83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['ok', 'brokeback', 'mountain', 'is', 'such', 'a', 'horrible', 'movie'], 0),\n",
       " (['brokeback', 'mountain', 'was', 'so', 'awesome'], 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_treino.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06df2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um objeto SentimentAnalyzer \n",
    "sentiment_analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f465a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém a lista de stopwords em Inglês \n",
    "stopwords_all = []\n",
    "for word in stopwords.words('english'):\n",
    "    stopwords_all.append(word)\n",
    "    stopwords_all.append(word + '_NEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeec3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém 10.000 tweets do dataset de treino e retorna todas as palavras que não são stopwords\n",
    "dataset_treino_amostra = dataset_treino.take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54294dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\n",
    "all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0843371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um unigram e extrai as features\n",
    "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops, top_n = 200)\n",
    "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
    "training_set = sentiment_analyzer.apply_features(dataset_treino_amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "503fe9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.collections.LazyMap"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f40ac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c0e1175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testa o classificador em algumas sentenças\n",
    "test_sentence1 = [(['this', 'program', 'is', 'suck'], '')]\n",
    "test_sentence2 = [(['tough', 'day', 'at', 'work', 'today'], '')]\n",
    "test_sentence3 = [(['good', 'wonderful', 'amazing', 'awesome'], '')]\n",
    "test_set1 = sentiment_analyzer.apply_features(test_sentence1)\n",
    "test_set2 = sentiment_analyzer.apply_features(test_sentence2)\n",
    "test_set3 = sentiment_analyzer.apply_features(test_sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc980429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contains(da)': False,\n",
       " 'contains(vinci)': False,\n",
       " 'contains(code)': False,\n",
       " 'contains(brokeback)': False,\n",
       " 'contains(mountain)': False,\n",
       " 'contains(harry)': False,\n",
       " 'contains(potter)': False,\n",
       " 'contains(love)': False,\n",
       " 'contains(awesome)': False,\n",
       " 'contains(impossible)': False,\n",
       " 'contains(mission)': False,\n",
       " 'contains()': False,\n",
       " 'contains(like)': False,\n",
       " 'contains(movie)': False,\n",
       " 'contains(sucked)': False,\n",
       " 'contains(sucks)': False,\n",
       " 'contains(hate)': False,\n",
       " 'contains(movies)': False,\n",
       " 'contains(stupid)': False,\n",
       " 'contains(suck)': True,\n",
       " 'contains(really)': False,\n",
       " 'contains(much)': False,\n",
       " 'contains(one)': False,\n",
       " 'contains(3)': False,\n",
       " 'contains(loved)': False,\n",
       " 'contains(right)': False,\n",
       " 'contains(series)': False,\n",
       " 'contains(depressing)': False,\n",
       " 'contains(fucking)': False,\n",
       " 'contains(left)': False,\n",
       " 'contains(want)': False,\n",
       " 'contains(horrible)': False,\n",
       " 'contains(harry_NEG)': False,\n",
       " 'contains(potter_NEG)': False,\n",
       " 'contains(start)': False,\n",
       " 'contains(reading)': False,\n",
       " 'contains(terrible)': False,\n",
       " 'contains(oh)': False,\n",
       " 'contains(know)': False,\n",
       " 'contains(would)': False,\n",
       " 'contains(story)': False,\n",
       " 'contains(ok)': False,\n",
       " 'contains(went)': False,\n",
       " 'contains(seen)': False,\n",
       " 'contains(saw)': False,\n",
       " 'contains(book)': False,\n",
       " 'contains(going)': False,\n",
       " 'contains(luv)': False,\n",
       " 'contains(ever)': False,\n",
       " 'contains(dont)': False,\n",
       " 'contains(people)': False,\n",
       " 'contains(also)': False,\n",
       " 'contains(lubb)': False,\n",
       " 'contains(never)': False,\n",
       " 'contains(know_NEG)': False,\n",
       " 'contains(side_NEG)': False,\n",
       " 'contains(read_NEG)': False,\n",
       " 'contains(absolutely)': False,\n",
       " 'contains(whos_NEG)': False,\n",
       " 'contains(right_NEG)': False,\n",
       " 'contains(snuck)': False,\n",
       " 'contains(way)': False,\n",
       " 'contains(beautiful)': False,\n",
       " 'contains(b)': False,\n",
       " 'contains(friday)': False,\n",
       " 'contains(big)': False,\n",
       " 'contains(time)': False,\n",
       " 'contains(’)': False,\n",
       " 'contains(making)': False,\n",
       " 'contains(around)': False,\n",
       " 'contains(care_NEG)': False,\n",
       " 'contains(anyone_NEG)': False,\n",
       " 'contains(says_NEG)': False,\n",
       " 'contains(cant)': False,\n",
       " 'contains(wait_NEG)': False,\n",
       " 'contains(guy)': False,\n",
       " 'contains(think)': False,\n",
       " 'contains(hung)': False,\n",
       " 'contains(kelsie)': False,\n",
       " 'contains(dragged)': False,\n",
       " 'contains(draco)': False,\n",
       " 'contains(malfoy)': False,\n",
       " 'contains(trousers)': False,\n",
       " 'contains(past)': False,\n",
       " 'contains(hips)': False,\n",
       " 'contains(throat)': False,\n",
       " 'contains(vigor)': False,\n",
       " 'contains(whimpering)': False,\n",
       " 'contains(noises)': False,\n",
       " 'contains(panting)': False,\n",
       " 'contains(groaning)': False,\n",
       " 'contains(blonds)': False,\n",
       " 'contains(rockhard)': False,\n",
       " 'contains(aching)': False,\n",
       " 'contains(cock)': False,\n",
       " 'contains(differently_NEG)': False,\n",
       " 'contains(boring)': False,\n",
       " 'contains(watch)': False,\n",
       " 'contains(tom)': False,\n",
       " 'contains(dads)': False,\n",
       " 'contains(retarted)': False,\n",
       " 'contains(gay)': False,\n",
       " 'contains(theres)': False,\n",
       " 'contains(black)': False,\n",
       " 'contains(daniel)': False,\n",
       " 'contains(wotshisface)': False,\n",
       " 'contains(needs)': False,\n",
       " 'contains(slap)': False,\n",
       " 'contains(liked)': False,\n",
       " 'contains(still)': False,\n",
       " 'contains(yeah)': False,\n",
       " 'contains(last)': False,\n",
       " 'contains(thought)': False,\n",
       " 'contains(good)': False,\n",
       " 'contains(thats)': False,\n",
       " 'contains(opinion)': False,\n",
       " 'contains(gonna)': False,\n",
       " 'contains(hoot)': False,\n",
       " 'contains(count)': False,\n",
       " 'contains(catcher)': False,\n",
       " 'contains(tye)': False,\n",
       " 'contains(jane)': False,\n",
       " 'contains(eyre)': False,\n",
       " 'contains(virgin)': False,\n",
       " 'contains(suicides)': False,\n",
       " 'contains(stand)': False,\n",
       " 'contains(film)': False,\n",
       " 'contains(hate_NEG)': False,\n",
       " 'contains(main_NEG)': False,\n",
       " 'contains(anyway)': False,\n",
       " 'contains(first)': False,\n",
       " 'contains(excellent)': False,\n",
       " 'contains(2)': False,\n",
       " 'contains(rocks)': False,\n",
       " 'contains(fun)': False,\n",
       " 'contains(letting)': False,\n",
       " 'contains(make)': False,\n",
       " 'contains(things)': False,\n",
       " 'contains(always)': False,\n",
       " 'contains(jokes)': False,\n",
       " 'contains(dudeee)': False,\n",
       " 'contains(type_NEG)': False,\n",
       " 'contains(person_NEG)': False,\n",
       " 'contains(likes_NEG)': False,\n",
       " 'contains(character_NEG)': False,\n",
       " 'contains(dies_NEG)': False,\n",
       " 'contains(better)': False,\n",
       " 'contains(combining)': False,\n",
       " 'contains(review)': False,\n",
       " 'contains(gary)': False,\n",
       " 'contains(gin)': False,\n",
       " 'contains(zen)': False,\n",
       " 'contains(take)': False,\n",
       " 'contains(friends)': False,\n",
       " 'contains(joining)': False,\n",
       " 'contains(community)': False,\n",
       " 'contains(knows)': False,\n",
       " 'contains(guy_NEG)': False,\n",
       " 'contains(crazy_NEG)': False,\n",
       " 'contains(hates_NEG)': False,\n",
       " 'contains(helped)': False,\n",
       " 'contains(bobbypin)': False,\n",
       " 'contains(insanely)': False,\n",
       " 'contains(cool)': False,\n",
       " 'contains(hat)': False,\n",
       " 'contains(head)': False,\n",
       " 'contains(laughed)': False,\n",
       " 'contains(cowboy)': False,\n",
       " 'contains(wanted)': False,\n",
       " 'contains(desperately)': False,\n",
       " 'contains(lovethe)': False,\n",
       " 'contains(sentry)': False,\n",
       " 'contains(station)': False,\n",
       " 'contains(bonkers)': False,\n",
       " 'contains(quiz)': False,\n",
       " 'contains(bye)': False,\n",
       " 'contains(well)': False,\n",
       " 'contains(mom)': False,\n",
       " 'contains(place)': False,\n",
       " 'contains(serious)': False,\n",
       " 'contains(man)': False,\n",
       " 'contains(worth)': False,\n",
       " 'contains(kirsten)': False,\n",
       " 'contains(leah)': False,\n",
       " 'contains(kate)': False,\n",
       " 'contains(escapades)': False,\n",
       " 'contains(reality)': False,\n",
       " 'contains(said)': False,\n",
       " 'contains(felicias)': False,\n",
       " 'contains(cleaning)': False,\n",
       " 'contains(table)': False,\n",
       " 'contains(felicia)': False,\n",
       " 'contains(grabs)': False,\n",
       " 'contains(keys)': False,\n",
       " 'contains(dash)': False,\n",
       " 'contains(freakin)': False,\n",
       " 'contains(outshines)': False,\n",
       " 'contains(material)': False,\n",
       " 'contains(plain)': False,\n",
       " 'contains(begin)': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89f2b933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(test_set1[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c95c2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b8aaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import configparser\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import socket\n",
    "import json\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68c592e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autenticação do Twitter \n",
    "api_key = 'Olk6wAb5NqOnXztjIs9bR6FaF'\n",
    "api_key_secret = 'vkUwO1OGEjKH5CGlmLsKP1KBTYoESycOGopwAjVTlDqG98Ajzj'\n",
    "access_token = '1228500338912743425-fLQRIKtrHJe9mkvWM7EXToYmtfzzxc'\n",
    "access_token_secret = 'q1cx5rGXqmIqeWX6gBsMGG0ymGzYLTfAe4Eagxin9ohRh'\n",
    "bearer_token = r'AAAAAAAAAAAAAAAAAAAAAHY5hgEAAAAA2FSk9Qi3r1OKDdlXRzhkzox9InI%3DR3U3eL0OI5G8ka9GO1OlXtrZnvfuktbrEJozzVwPz1LssHYWcG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60692ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2FilteredStreamPython\"\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27e2b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_rules():\n",
    "    print('Getting rules...')\n",
    "    response = requests.get(\"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth)\n",
    "    print(json.dumps(response.json()))\n",
    "    rules = response.json()\n",
    "    \n",
    "    #########################################################################\n",
    "    print('\\nDeleting all rules...')\n",
    "    if rules is None or \"data\" not in rules:\n",
    "        return None\n",
    "\n",
    "    ids = list(map(lambda rule: rule[\"id\"], rules[\"data\"]))\n",
    "    payload = {\"delete\": {\"ids\": ids}}\n",
    "    response = requests.post(\"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth, json=payload)\n",
    "    \n",
    "    print(json.dumps(response.json()))\n",
    "    \n",
    "    #########################################################################\n",
    "    print('\\nSetting rules...')\n",
    "    sample_rules = [{'value': 'russia'},{'value': 'war'},{'value': 'putin'}]\n",
    "\n",
    "    payload = {\"add\": sample_rules}\n",
    "    response = requests.post(\"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth, json=payload)\n",
    "    print(json.dumps(response.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c74b9a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting rules...\n",
      "{\"data\": [{\"id\": \"1579447944444526593\", \"value\": \"russia\"}, {\"id\": \"1579447950945689601\", \"value\": \"war\"}, {\"id\": \"1579447955551051777\", \"value\": \"putin\"}], \"meta\": {\"sent\": \"2022-10-10T12:28:19.992Z\", \"result_count\": 3}}\n",
      "\n",
      "Deleting all rules...\n",
      "{\"meta\": {\"sent\": \"2022-10-10T12:28:21.341Z\", \"summary\": {\"deleted\": 3, \"not_deleted\": 0}}}\n",
      "\n",
      "Setting rules...\n",
      "{\"data\": [{\"value\": \"putin\", \"id\": \"1579448750732464129\"}, {\"value\": \"russia\", \"id\": \"1579448750732464131\"}, {\"value\": \"war\", \"id\": \"1579448750732464130\"}], \"meta\": {\"sent\": \"2022-10-10T12:28:22.788Z\", \"summary\": {\"created\": 3, \"not_created\": 0, \"valid\": 3, \"invalid\": 0}}}\n"
     ]
    }
   ],
   "source": [
    "# Gainaing access and connecting to Twitter API using Credentials\n",
    "client = tweepy.Client(bearer_token, api_key, api_key_secret, access_token, access_token_secret)\n",
    "\n",
    "auth = tweepy.OAuth1UserHandler(api_key, api_key_secret, access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "get_set_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e18a6810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.DStream"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream = ssc.queueStream([], default = rdd)\n",
    "type(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb05c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de tweets por update\n",
    "NUM_TWEETS = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "204c1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função conecta ao Twitter e retorna um número específico de Tweets (NUM_TWEETS)\n",
    "def tfunc(t, rdd):\n",
    "    return rdd.flatMap(lambda x: stream_twitter_data())\n",
    "\n",
    "def stream_twitter_data():\n",
    "    response = requests.get(\"https://api.twitter.com/2/tweets/search/stream\", auth=bearer_oauth, stream=True)\n",
    "    print('Status code {}'.format(response.status_code))\n",
    "    count = 0\n",
    "    for line in response.iter_lines():\n",
    "        try:\n",
    "            if count > NUM_TWEETS:\n",
    "                break\n",
    "            post = json.loads(line.decode('utf-8'))\n",
    "            contents = [post['data']['text']]\n",
    "            count += 1\n",
    "            yield str(contents)\n",
    "        except:\n",
    "            result = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "040e39a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.TransformedDStream"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream = stream.transform(tfunc)\n",
    "type(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1845721b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.TransformedDStream"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_stream = stream.map(lambda line: ast.literal_eval(line))\n",
    "type(coord_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24ebb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função classifica os tweets, aplicando as features do modelo criado anteriormente\n",
    "def classifica_tweet(tweet):\n",
    "    sentence = [(tweet, '')]\n",
    "    test_set = sentiment_analyzer.apply_features(sentence)\n",
    "    print(tweet, classifier.classify(test_set[0][0]))\n",
    "    return(tweet, classifier.classify(test_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "326e845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função retorna o texto do Twitter\n",
    "def get_tweet_text(rdd):\n",
    "    for line in rdd:\n",
    "        tweet = line.strip()\n",
    "        translator = str.maketrans({key: None for key in string.punctuation})\n",
    "        tweet = tweet.translate(translator)\n",
    "        tweet = tweet.split(' ')\n",
    "        tweet_lower = []\n",
    "        for word in tweet:\n",
    "            tweet_lower.append(word.lower())\n",
    "    return(classifica_tweet(tweet_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4ab851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista vazia para os resultados\n",
    "resultados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43018aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função salva o resultado dos batches de Tweets junto com o timestamp\n",
    "def output_rdd(rdd):\n",
    "    global resultados\n",
    "    pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))\n",
    "    counts = pairs.reduceByKey(add)\n",
    "    output = []\n",
    "    for count in counts.collect():\n",
    "        output.append(count)\n",
    "    result = [time.strftime(\"%I:%M:%S\"), output]\n",
    "    resultados.append(result)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f4d6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função foreachRDD() aplica uma função a cada RDD to streaming de dados\n",
    "coord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b49e616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming\n",
    "ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = True\n",
    "while cont:\n",
    "    if len(resultados) > 5:\n",
    "        cont = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb99122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
