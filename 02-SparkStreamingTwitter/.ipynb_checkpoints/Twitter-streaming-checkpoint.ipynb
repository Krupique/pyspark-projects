{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4a8aaa",
   "metadata": {},
   "source": [
    "# Análise de Sentimentos de Tweets coletados em tempo real com Spark Streaming e MLlib\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9dd8e5",
   "metadata": {},
   "source": [
    "A análise de sentimentos ajuda os analistas de dados de empresas e organizações a avaliar a opinião pública, realizar pesquisas de mercado, monitorar a reputação de marcas e produtos e compreender as experiências dos seus consumidores ou potenciais clientes.\n",
    "\n",
    "Uma excelente forma de obter dados sobre opiniões, sentimentos e discussões sobre um determinado assunto, é através das redes sociais. Atualmente, o Twitter é a principal rede social para obter informações sobre discussções dos mais variados assuntos e tópicos. A empresa disponibiliza uma API de maneira gratuita para os desenvolvedores que desejam coletar os dados para realizar as suas análises.\n",
    "\n",
    "A quantidade de tweets gerados por dia pode ultrapassar as centenas de milhares, chegando até em milhões de tweets em um único dia. Para lidar com essa grande quantidade de dados, a utilização de frameworks que sejam capazes de realizar o processamento distribuído em clusters ou em nuvens se faz bastante relevante. A principal ferramenta para realizar esse tipo de tarefa é o Spark. O Spark é uma estrutura de computação em cluster de código aberto, construída em torno da velocidade, facilidade de uso e análise de streaming de dados.\n",
    "\n",
    "Com a utilização do Spark e a API do Twitter, este projeto se propõe ao desenvolvimento de um modelo de Machine Learning que com base nos dados históricos, seja capaz de criar um modelo para a classificação do sentimento de um texto como sendo positivo ou negativo. A partir disso, novos dados gerados e coletados em tempo real pela API serão submetidos ao modelo e serão classificados de acordo com o respectivo sentimento.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef6970",
   "metadata": {},
   "source": [
    "## Sumário\n",
    "\n",
    "* <a href=\"#id_1\">  1) Introdução e definição do problema</a>\n",
    "    * <a href=\"#id_1-1\"> 1.1) O que é análise de sentimentos?</a>\n",
    "    * <a href=\"#id_1-2\"> 1.2) Objetivos deste projeto</a>\n",
    "    * <a href=\"#id_1-3\"> 1.3) Metodologia</a>\n",
    "<br/><br/>\n",
    "\n",
    "* <a href=\"#id_2\"> 2) Parte 1 - Criação do modelo</a>\n",
    "    * <a href=\"#id_2-1\"> 2.1) Importação das bibliotecas</a>\n",
    "    * <a href=\"#id_2-2\"> 2.2) Obtendo os dados de treino</a>\n",
    "    * <a href=\"#id_2-3\"> 2.3) Pré Processamento dos dados</a>\n",
    "    * <a href=\"#id_2-4\"> 2.4) Modelagem</a>\n",
    "        * <a href=\"#id_2-4-1\">2.4.1) Obtendo a lista de Stopwords</a>\n",
    "        * <a href=\"#id_2-4-2\">2.4.2) Treinamento do modelo</a>\n",
    "        * <a href=\"#id_2-4-3\">2.4.3) Testando o modelo</a>\n",
    "<br/><br/>\n",
    "\t\n",
    "* <a href=\"#id_3\"> 3) Parte 2 - Coleta de dados do Twitter para Classificação</a>\n",
    "    * <a href=\"#id_3-1\"> 3.1) Obtendo chaves de autentifação e Definindo regras de pesquisa</a>\n",
    "    * <a href=\"#id_3-2\"> 3.2) Pré processamento dos Dados</a>\n",
    "    * <a href=\"#id_3-3\"> 3.3) Coleta e classificação</a>\n",
    "<br/><br/>\n",
    "\n",
    "* <a href=\"#id_4\"> 4) Considerações finais</a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae40ec6",
   "metadata": {},
   "source": [
    "<h2 id=\"id_1\">  1) Introdução e definição do problema</h2>\n",
    "\n",
    "<h3 id=\"id_1-1\"> 1.1) O que é análise de sentimentos?</h3>\n",
    "Uma tendência relativamente mais recente na análise de textos vai além da detecção de tópicos e tenta identificar a emoção por trás de um texto. Isso é chamado de análise de sentimentos, ou também de mineração de opinião e IA de emoção.\n",
    "\n",
    "A análise de sentimentos é uma mineração contextual de um texto que identifica e extrai informações subjetivas no material de origem. Ela ajuda as empresas a entenderem o sentimento social de sua marca, produto ou serviço.\n",
    "\n",
    "Um sistema de análise de sentimentos para conteúdo textual combina o processamento de linguagem natural (PLN) e técnicas de aprendizado de máquina para atribuir pontuações ponderadas de sentimento às sentenças. Com os recentes avanços na aprendizagem de máquina, o uso de técnicas avançadas de inteligência artificial se tornou eficaz para identificar sentimentos de usuários na web.\n",
    "\n",
    "Quando uma empresa deseja entender o que estão falando sobre ela e qual a reputação de seus produtos online, uma das formas de se fazer isso é utilizando machine learning. Nesse sentido, uma das técnicas recomendadas é a análise de sentimentos, que consiste em extrair informações de textos a partir de linguagem natural.O objetivo dessa técnica é classificar sentenças, ou um conjunto de sentenças, como positivas ou negativas. Essa classificação é realizada automaticamente e extrai informações subjetivas de textos, criando conhecimento estruturado que pode ser utilizado por um sistema.\n",
    "\n",
    "Contudo, obter uma grande quantidade de dados em tempo real pode gerar um problema. A dificuldade em termos de capacidade computacional para processar todos estes dados em tempo real. Ao longo dos últimos anos, a principal ferramenta utilizada para processar grandes quantidade de dados em tempo real é o Apache Spark. Em linhas gerais, o Apache Spark é uma estrtutura de código aberto desenvolvida para ser um mecanismo de processamento analítico para aplicações de processamento de dados distribuídos em larga escala e aprendizado de máquina em tempo real, ou seja, para grandes volumes de dados, o chamado Big Data.\n",
    "\n",
    "<h3 id=\"id_1-2\"> 1.2) Objetivos deste projeto</h3>\n",
    "Este projeto tem por objetivo, utilizar a API do Twitter para obter tweets em tempo real de um determinado assunto, aplicar técnicas de MapReduce, transformação de dados com o framework Spark e utilizar o MLlib para classificar tweets como sentimento positivo ou negativo.\n",
    "\n",
    "<h3 id=\"id_1-3\"> 1.3) Metodologia</h3>\n",
    "O projeto foi dividido em duas principais etapas. A primeira parte do projeto compreende a obtenção de um dataset rotulado contendo sentenças e suas respectivas classificações: 1 ou 0. Sendo 1 para sentimento positivo e 0 para sentimento negativo. O dataset foi obtido do repositório de datasets Kaggle e se trata de um conjunto de dados fornecido por um colaborador da Universidade de Michigan. O dataset se chama <a target=\"_blank\" href=\"https://www.kaggle.com/datasets/seesea0203/umich-si650-nlp\">UMICH SI650 NLP</a>, contém 5662 registros e duas colunas. A primeira para o texto e a segunda para o sentimento.\n",
    "\n",
    "Após o carregamento dos dados foi realizado o pré-processamento dos dados. Esta é a principal atividade em um projeto baseado em dados. Os dados aqui se tratam de tweets reais e há vários tratamentos que devem ser realizados para garantir a legibilidade dos dados. Foi aplicado a remoção de caracteres especiais, remoção e de pontuação, remoção de stopwords (palavras irrelevantes para o sentido da frase) e a divisão dos dados em listas de palavras para se adequar ao formato de entrada que o modelo espera.\n",
    "\n",
    "Com os dados limpos e organizados os mesmos foram submetidos ao algoritmo de Machine Learning responsável por fazer a classicação dos tweets e gerar o modelo de classificação. \n",
    "\n",
    "Em seguida foi iniciado a segunda parte do projeto, a etapa de coleta dos dados em tempo real do Twitter, tratamento dos tweets e a classificação com o modelo gerado anteriormente. \n",
    "\n",
    "A primeira atividade para obter dados do Twitter via API é estabelecar conexão com as respecitivas autenticações: api key, api key secret, access token, access token secret e bearer token. Com as chaves de autenticação foi necessário estabelecer conexão, obter as regras de acesso, deletar as antigas regras de acesso, adicionar as novas regras de acesso, para então realizar a coleta de dados.\n",
    "\n",
    "Depois de pronto a conexão com a API do Twitter, os dados vêm são coletados por meio do streaming de dados do Spark (SparkStreaming) e salvos em objetos DSTreams. Os dados são então pré-processados e submetidos ao modelo para realizar a classificação. A coleta de dados termina após o tempo estabelicido pelo usuário ou então após a interrupção da conexão.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7281882",
   "metadata": {},
   "source": [
    "<h2 id=\"id_2\"> 2) Parte 1 - Criação do modelo</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8a51d",
   "metadata": {},
   "source": [
    "<h3 id=\"id_2-1\"> 2.1) Importação das bibliotecas</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab09b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos usados\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from operator import add\n",
    "import requests_oauthlib\n",
    "from time import gmtime, strftime\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import ast\n",
    "import json\n",
    "#import re\n",
    "\n",
    "# Pacote NLTK\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import subjectivity, stopwords\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69dc8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequência de update\n",
    "INTERVALO_BATCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac1652c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o StreamingContext\n",
    "ssc = StreamingContext(sc, INTERVALO_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ef0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session - usada quando se trabalha com Dataframes no Spark\n",
    "spSession = SparkSession.builder.master(\"local\").appName(\"tw-session\").config(\"spark.some.config.option\", \"session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00c341",
   "metadata": {},
   "source": [
    "<h3 id=\"id_2-2\"> 2.2) Obtendo os dados de treino</h3>\n",
    "\n",
    "Uma parte essencial da criação de um algoritmo de análise de sentimento (ou qualquer algoritmo de mineração de dados) é ter um conjunto de dados abrangente ou \"Corpus\" para o aprendizado, bem como um conjunto de dados de teste para garantir que a precisão do seu algoritmo atende aos padrões que você espera. Isso também permitirá que você ajuste o seu algoritmo a fim de deduzir melhores (ou mais precisas) características de linguagem natural que você poderia extrair do texto e que vão contribuir para a classificação de sentimento, em vez de usar uma abordagem genérica. Tomaremos como base o dataset de treino fornecido pela Universidade de Michigan, para competições do Kaggle <a target=\"_blank\" href=\"https://www.kaggle.com/datasets/seesea0203/umich-si650-nlp\">UMICH SI650 NLP</a>.\n",
    "\n",
    "Cada linha do Dataset é marcada como:\n",
    "* **1**: para o sentimento positivo \n",
    "* **0**: para o sentimento negativo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "252d76a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o arquivo texto e criando um RDD em memória com Spark\n",
    "rdd = sc.textFile(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945bf448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE', 'LABEL']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removendo o cabeçalho do arquivo\n",
    "header = rdd.first()\n",
    "rdd_body = rdd.filter(lambda x: header not in x)#.map(lambda l: l.split(','))\n",
    "\n",
    "list_columns = header.replace('.', '_').upper().split(',')\n",
    "list_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e40716",
   "metadata": {},
   "source": [
    "<h3 id=\"id_2-3\"> 2.3) Pré Processamento dos dados</h3>\n",
    "\n",
    "Agora nós vamos trabalhar para tratar os dados de forma a facilitar e otimizar para que o modelo possa realizar a classificação. O pré-processamento é um conjunto de atividades que envolvem preparação, organização e estruturação dos dados. Trata-se de uma etapa fundamental que precede a realização de análises e predições. Essa etapa é muito importante para reduzir as dimensões do problema e também será determinante para a qualidade final dos dados que serão treinados com o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54c56435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ok brokeback mountain is such a horrible movie.;0',\n",
       " 'Brokeback Mountain was so awesome.;1',\n",
       " 'friday hung out with kelsie and we went and saw The Da Vinci Code SUCKED!!!!!;0',\n",
       " 'I am going to start reading the Harry Potter series again because that is one awesome story.;1',\n",
       " 'Is it just me, or does Harry Potter suck?...;0',\n",
       " 'The Da Vinci Code sucked big time.;0',\n",
       " 'I am going to start reading the Harry Potter series again because that is one awesome story.;1',\n",
       " 'For those who are Harry Potter ignorant, the true villains of this movie are awful creatures called dementors.;0',\n",
       " 'Harry Potter dragged Draco Malfoy ’ s trousers down past his hips and sucked him into his throat with vigor, making whimpering noises and panting and groaning around the blonds rock-hard, aching cock...;0',\n",
       " \"So as felicia's mom is cleaning the table, felicia grabs my keys and we dash out like freakin mission impossible.;1\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Substituindo as ocorrências de ,0 e ,1 por ;0 e ;1 para que possamos saber exatamente qual é o delimitador das colunas.\n",
    "rdd_body = rdd_body.map(lambda x: x.replace(',0', ';0')).map(lambda x: x.replace(',1', ';1'))\n",
    "rdd_body.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84698ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função separa as colunas em cada linha, cria uma tupla e remove a pontuação.\n",
    "def get_row(line):\n",
    "    row = line.split(';') # Separa a linha em duas colunas.\n",
    "    \n",
    "    tweet = row[0].strip() # Coluna 1 para o texto.\n",
    "    sentimento = int(re.sub('[^\\d]+', '', row[1])) # Coluna 2 para o dígito corresponde a classificação.\n",
    "    \n",
    "    translator = str.maketrans({key: None for key in string.punctuation}) # Remove pontuação\n",
    "    tweet = tweet.translate(translator)\n",
    "    tweet = tweet.split(' ') # Separa a frase em lista de palavras\n",
    "    tweet_lower = []\n",
    "    for word in tweet:\n",
    "        tweet_lower.append(word.lower()) # Converte todas as palavras para lowercase.\n",
    "    return (tweet_lower, sentimento) # Retorna uma tupla com a lista de palavras e com o sentimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd057ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica a função para cada linha do dataset\n",
    "dataset_treino = rdd_body.map(lambda line: get_row(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75bcd83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['ok', 'brokeback', 'mountain', 'is', 'such', 'a', 'horrible', 'movie'], 0),\n",
       " (['brokeback', 'mountain', 'was', 'so', 'awesome'], 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exibindo dois registros\n",
    "dataset_treino.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b96088",
   "metadata": {},
   "source": [
    "<h3 id=\"id_2-4\"> 2.4) Modelagem</h3>\n",
    "\n",
    "Agora que passamos pela etapa de pré-processamento, podemos entrar na modelagem propriamente dita.  Com os dados prontos, finalmente podemos escolher o modelo que iremos aplicar em nossos dados. Para a identificação do sentimento, iremos utilizar uma biblioteca pré-construída da biblioteca NLTK que pontia os dados de texto com base no sentimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06df2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um objeto SentimentAnalyzer \n",
    "sentiment_analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf09a615",
   "metadata": {},
   "source": [
    "<h4 id=\"id_2-4-1\">2.4.1) Obtendo a lista de Stopwords</h4>\n",
    "\n",
    "Stopwords são palavras comuns que normalmente não contribuem para o significado de uma frase, pelo menos com relação ao propósito da informação e do processamento da linguagem natural. São palavras como \"The\" e \"a\" ((em inglês) ou \"O/A\" e \"Um/Uma\" ((em português). Muitos mecanismos de busca filtram estas palavras (stopwords), como forma de economizar espaço em seus índices de pesquisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f465a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém a lista de stopwords em Inglês \n",
    "stopwords_all = []\n",
    "for word in stopwords.words('english'):\n",
    "    stopwords_all.append(word)\n",
    "    stopwords_all.append(word + '_NEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeec3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém 10.000 tweets do dataset de treino e retorna todas as palavras que não são stopwords\n",
    "dataset_treino_amostra = dataset_treino.take(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54294dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\n",
    "all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0843371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um unigram e extrai as features\n",
    "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops, top_n = 200)\n",
    "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
    "training_set = sentiment_analyzer.apply_features(dataset_treino_amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "503fe9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.collections.LazyMap"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d0bcf",
   "metadata": {},
   "source": [
    "<h4 id=\"id_2-4-2\">2.4.2) Treinamento do modelo</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f40ac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a166f70",
   "metadata": {},
   "source": [
    "<h4 id=\"id_2-4-3\">2.4.3) Testando o modelo</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c0e1175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testa o classificador em algumas sentenças\n",
    "test_sentence1 = [(['this', 'program', 'is', 'amazing'], '')]\n",
    "test_sentence2 = [(['tough', 'day', 'at', 'work', 'today'], '')]\n",
    "test_sentence3 = [(['good', 'wonderful', 'amazing', 'awesome'], '')]\n",
    "test_set1 = sentiment_analyzer.apply_features(test_sentence1)\n",
    "test_set2 = sentiment_analyzer.apply_features(test_sentence2)\n",
    "test_set3 = sentiment_analyzer.apply_features(test_sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89f2b933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(test_set1[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c95c2d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956bb0d0",
   "metadata": {},
   "source": [
    "<h2 id=\"id_3\"> 3) Parte 2 - Coleta de dados do Twitter para Classificação</h2>\n",
    "\n",
    "Nesta etapa iremos realizar a coleta dos dados do Twitter em tempo real. Há várias etapas que devemos seguir para conseguir obter os dados. \n",
    "\n",
    "Como pré-requisito básico, você precisa ter uma conta de desenvolvedorno Twitter. Daí então criar uma app e gerar as chaves de autenticação: api key, api key secret, access token, access token secret  e bearer token.\n",
    "\n",
    "Com as chaves em mãos, é necessário realizar a etapa de autenticação com os métodos de autentication. Em seguida, devemos obter as regras antigas de pesquisa (caso você já tenha contectado antes), excluí-las e então definir as novas regras de pesquisa.\n",
    "\n",
    "\n",
    "> NOTA: Este notebook funciona de maneira independente. Diferentemente dos exemplos que eu usei no tutorial sobre como utilizar o Spark Streaming via Socket, este notebook não necessita do app `tweets-listener.py` para ser executado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b8aaf77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conf/config.ini']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import configparser\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import socket\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Nestas configs estão as chaves da api do Twiterr\n",
    "# Você vai encontrar um arquivo chamado config_template.ini no diretório conf. renomeie para config.ini e insira suas credenciais do Twitter\n",
    "config = configparser.ConfigParser()\n",
    "config.read('conf/config.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149d457",
   "metadata": {},
   "source": [
    "<h3 id=\"id_3-1\"> 3.1) Obtendo chaves de autentifação e Definindo regras de pesquisa</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bb69629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo as credenciais\n",
    "api_key = config['twitter']['api_key']\n",
    "api_key_secret = config['twitter']['api_key_secret']\n",
    "access_token = config['twitter']['access_token']\n",
    "access_token_secret = config['twitter']['access_token_secret']\n",
    "\n",
    "bearer_token = r'AAAAAAAAAAAAAAAAAAAAAHY5hgEAAAAA2FSk9Qi3r1OKDdlXRzhkzox9InI%3DR3U3eL0OI5G8ka9GO1OlXtrZnvfuktbrEJozzVwPz1LssHYWcG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60692ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autenticação utilizando o Bearer Token\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2FilteredStreamPython\"\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27e2b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_rules():\n",
    "    print('Getting rules...') # Obtendo regras atuais\n",
    "    response = requests.get(\"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth)\n",
    "    print(json.dumps(response.json()))\n",
    "    rules = response.json()\n",
    "    print(f'Status {response.status_code}')\n",
    "    \n",
    "    #########################################################################\n",
    "    print('\\nDeleting all rules...') # Deletando todas as regras\n",
    "    if rules is None or \"data\" not in rules:\n",
    "        return None\n",
    "\n",
    "    ids = list(map(lambda rule: rule[\"id\"], rules[\"data\"]))\n",
    "    payload = {\"delete\": {\"ids\": ids}}\n",
    "    response = requests.post(\"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth, json=payload)\n",
    "    \n",
    "    print(json.dumps(response.json()))\n",
    "    print(f'Status {response.status_code}')\n",
    "    \n",
    "    #########################################################################\n",
    "    print('\\nSetting rules...') # Setando as novas regras\n",
    "    sample_rules = [{'value': 'russia'},{'value': 'war'},{'value': 'putin'}]\n",
    "\n",
    "    payload = {\"add\": sample_rules}\n",
    "    response = requests.post(\"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth, json=payload)\n",
    "    print(json.dumps(response.json()))\n",
    "    print(f'Status {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo acesso e conectando a API do Twitter usando credenciais\n",
    "client = tweepy.Client(bearer_token, api_key, api_key_secret, access_token, access_token_secret)\n",
    "\n",
    "# Realizando autenticação\n",
    "auth = tweepy.OAuth1UserHandler(api_key, api_key_secret, access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "get_set_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec890fed",
   "metadata": {},
   "source": [
    "<h3 id=\"id_3-2\"> 3.2) Pré processamento dos Dados</h3>\n",
    "\n",
    "Foi realizado a autenticação e definido as regras de pesquisas. Agora vamos definir como o nosso sistema vai receber e tratar estes dados. Afinal, os tweets também estarão \"poluídos\" com stopwords, gírias, hashtags, menções e pontuações. Portanto, devemos tratar da mesma forma que foi tratado anteriormente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de uma fila de fluxo\n",
    "stream = ssc.queueStream([], default = rdd)\n",
    "type(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de tweets por update\n",
    "NUM_TWEETS = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função conecta ao Twitter e retorna um número específico de Tweets (NUM_TWEETS)\n",
    "def tfunc(t, rdd):\n",
    "    return rdd.flatMap(lambda x: stream_twitter_data())\n",
    "\n",
    "def stream_twitter_data():\n",
    "    # Faz a requisição do tweet na API.\n",
    "    response = requests.get(\"https://api.twitter.com/2/tweets/search/stream\", auth=bearer_oauth, stream=True)\n",
    "    # Exibe o status de retorno: 200 e 201 para sucesso.\n",
    "    print('Status code {}'.format(response.status_code))\n",
    "    count = 0\n",
    "    # Faz a iteração para cada linha de tweet obtido.\n",
    "    # Se a quantidade de registros for maior que a quantidade permitida por update, então retorna os tweets\n",
    "    for line in response.iter_lines():\n",
    "        try:\n",
    "            if count > NUM_TWEETS:\n",
    "                break\n",
    "            post = json.loads(line.decode('utf-8'))\n",
    "            contents = [post['data']['text']]\n",
    "            count += 1\n",
    "            yield str(contents)\n",
    "        except:\n",
    "            result = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica a transformação\n",
    "stream = stream.transform(tfunc)\n",
    "type(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_stream = stream.map(lambda line: ast.literal_eval(line))\n",
    "type(coord_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função classifica os tweets, aplicando as features do modelo criado anteriormente\n",
    "def classifica_tweet(tweet):\n",
    "    sentence = [(tweet, '')]\n",
    "    test_set = sentiment_analyzer.apply_features(sentence)\n",
    "    print(tweet, classifier.classify(test_set[0][0]))\n",
    "    return(tweet, classifier.classify(test_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função retorna o texto do Twitter\n",
    "def get_tweet_text(rdd):\n",
    "    for line in rdd:\n",
    "        tweet = line.strip()\n",
    "        translator = str.maketrans({key: None for key in string.punctuation})\n",
    "        tweet = tweet.translate(translator)\n",
    "        tweet = tweet.split(' ')\n",
    "        tweet_lower = []\n",
    "        for word in tweet:\n",
    "            tweet_lower.append(word.lower())\n",
    "    return(classifica_tweet(tweet_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista vazia para os resultados\n",
    "resultados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43018aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função salva o resultado dos batches de Tweets junto com o timestamp\n",
    "def output_rdd(rdd):\n",
    "    global resultados\n",
    "    pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))\n",
    "    counts = pairs.reduceByKey(add)\n",
    "    output = []\n",
    "    for count in counts.collect():\n",
    "        output.append(count)\n",
    "    result = [time.strftime(\"%I:%M:%S\"), output]\n",
    "    resultados.append(result)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função foreachRDD() aplica uma função a cada RDD to streaming de dados\n",
    "coord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930cc39",
   "metadata": {},
   "source": [
    "<h3 id=\"id_3-3\"> 3.3) Coleta e classificação</h3>\n",
    "\n",
    "Tudo pronto. Agora é só iniciar a coleta e visualizar os resultados sendo classificados em tempo real. Você pode utilizar qualquer uma das duas células seguintes para realizar o inicio da coleta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia a listen\n",
    "ssc.start()\n",
    "\n",
    "# Aguarda a interrupção ou o tempo definido em segudos. (120s) por exemplo.\n",
    "ssc.awaitTerminationOrTimeout(120) \n",
    "\n",
    "# Para o listen\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enquanto a quantidade de resultados obtidos for menor ou igual a quantidade definida, \n",
    "# então coleta e classifica os tweets, salvando na lista de resultados.\n",
    "\n",
    "cont = True\n",
    "while cont:\n",
    "    if len(resultados) > 5:\n",
    "        cont = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a600a7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fda5f7",
   "metadata": {},
   "source": [
    "<h2 id=\"id_4\"> 4) Considerações finais</h2>\n",
    "\n",
    "Este projeto se propôs a realização de um classificador de sentimentos em tempo real relativamente simples utilizando um dos veículos de obter opiniões sobre qualquer assunto, que é o Twitter. Utilizamos o principal framework para análise de Big Data, o Apache Spark, que não é dos frameworks mais simples de se aprender, mas é extremamente poderoso e escalável, e é sem dúvidas uma ferramenta muito importe para compor a carteira de conhecimento dos cientistas de dados.\n",
    "\n",
    "Projetos desse tipo são valiosíssimos para as organizações e empresas. Citando alguns exemplos de aplicação prática:\n",
    "* Obter opiniões sobre determinado assunto de interesse da empresa;\n",
    "* Entender o sentimento das pessoas quanto a um produto X;\n",
    "* Estudar o comportamento das pessoas frente a um evento como por exemplo: as eleições política;\n",
    "\n",
    "\n",
    "**Agradecimentos**\n",
    "\n",
    "Para quem chegou até aqui, muito obrigado por acompanhar este conteúdo. Me coloco a disposição para esclarecer eventuais dúvidas. Segue o contato das minhas redes:\n",
    "\n",
    "* Email: **<a target=\"_blank\" href=\"mailto:krupck@outlook.com\">krupck@outlook.com</a>**\n",
    "* Linkedin: **<a target=\"_blank\" href=\"https://www.linkedin.com/in/henrique-krupck/\">henrique-krupck</a>**\n",
    "\n",
    "\n",
    "At.te,\n",
    "\n",
    "Henrique K.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bf6cc",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
