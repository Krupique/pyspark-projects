{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d09e8e",
   "metadata": {},
   "source": [
    "# PySpark e Streaming de Dados\n",
    "\n",
    "\n",
    "## Spark Streaming\n",
    "A proposta do Spark Stream é analisar dados em tempo real, e não esperar horas para fazer a análise e processamento. \"A vida não acontece em batches\".\n",
    "Streaming de dados não é apenas para projetos altamente especializados. Computação baseada em Streaming está se tornando a regra para empresas orientadas a dados.\n",
    "Uma das principais fontes de dados contínuos são os sensores, da internet das coisas. Existem quatro areas principais que o Spark Streaming vem sendo utilizado:\n",
    "\n",
    "* Streaming ETL;\n",
    "* Detecção de anomalias;\n",
    "* Enriquecimento de dados;\n",
    "* Sessões complexas e aprendizado contínuo.\n",
    "\n",
    "Uma importante vantagem de usar o Spark para Big Data Analytics é a possibilidade de combinar processamento em batch e processamento de streaming em um único sistema.\n",
    "\n",
    "* **Batch**: Você inicia o processamento de um arquivo ou dataset finito, o spark processa as tarefas configuradas e conclui o trabalho.\n",
    "* **Streaming**: Você processa um stream de dados contínuos; a execução não pára até que haja algum erro ou você termine a aplicação manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30acaed",
   "metadata": {},
   "source": [
    "Refs: https://www.projectpro.io/article/spark-streaming-example/540\n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a525733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row #Converte RDDs em objetos do tipo Row\n",
    "from pyspark.sql.functions import col, isnan, when, count # Encontra a contagem para valores None, Null, Nan, etc.\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Módulos usados\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from operator import add\n",
    "import requests_oauthlib\n",
    "from time import gmtime, strftime\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import ast\n",
    "import json\n",
    "#import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a3bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"127.0.0.1\", 5554)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ce46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []\n",
    "#words_trans = lines.map(lambda x: limparTexto(x)).map(lambda x: removerStopwords(x))\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f889e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:34:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:34:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:34:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:34:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:35:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-10-11 09:36:35\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTerminationOrTimeout(120)  # Wait for the computation to terminate\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc51d86",
   "metadata": {},
   "source": [
    "Links: \n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#programming-model\n",
    "\n",
    "https://github.com/syalanuj/youtube/blob/main/spark_streaming_with_python_in_12_minutes/spark_st_run.ipynb\n",
    "\n",
    "https://github.com/Krupique/cursos-datascience-conteudo/blob/main/DSA_DS-02-BigData%20Analytics%20com%20Python%20e%20Spark/05%20-%20Introducao-SparkStreaming.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98965fff",
   "metadata": {},
   "source": [
    "## Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "935bc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "from collections import namedtuple\n",
    "\n",
    "# Can only run this once. restart your kernel for any errors.\n",
    "\n",
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)\n",
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5554)\n",
    "\n",
    "lines = socket_stream.window( 100 )\n",
    "\n",
    "fields = (\"text\")\n",
    "\n",
    "# Use Parenthesis for multiple lines or use \\.\n",
    "Tweet = namedtuple( 'Tweet', fields )\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) \n",
    "     .filter( lambda word: word.lower().startswith(\"#\"))  \n",
    "     .map( lambda word: ( word.lower(), 1 ) ) \n",
    "     .reduceByKey( lambda a, b: a + b ) \n",
    "     .map( lambda rec: Tweet( rec[0], rec[1] ) ) \n",
    "     .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) \n",
    "     .limit(100).registerTempTable(\"tweets\") ) ) # Registers to a table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca5573",
   "metadata": {},
   "source": [
    "## Now run TweetListener.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f68aa",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f106b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Only works for Jupyter Notebooks!\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    \n",
    "    time.sleep( 3 )\n",
    "    top_10_tweets = sqlContext.sql( 'Select tag, count from tweets' )\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure( figsize = ( 10, 8 ) )\n",
    "    sns.barplot( x=\"count\", y=\"tag\", data=top_10_df)\n",
    "    plt.show()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95493ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = sqlContext.sql( 'Select * from tweets' )\n",
    "df = tweets.toPandas()\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
