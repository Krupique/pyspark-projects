{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4a8aaa",
   "metadata": {},
   "source": [
    "# Análise de Sentimentos de Tweets coletados em tempo real com Spark Streaming e MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef6970",
   "metadata": {},
   "source": [
    "## Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae40ec6",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "## 1) Definição do problema\n",
    "### 1.1) O que é análise de sentimentos?\n",
    "Uma tendência relativamente mais recente na análise de textos vai além da detecção de tópicos e tenta identificar a emoção por trás de um texto. Isso é chamado de análise de sentimentos, ou também de mineração de opinião e IA de emoção.\n",
    "\n",
    "A análise de sentimentos é uma mineração contextual de um texto que identifica e extrai informações subjetivas no material de origem. Ela ajuda as empresas a entenderem o sentimento social de sua marca, produto ou serviço.\n",
    "\n",
    "Um sistema de análise de sentimentos para conteúdo textual combina o processamento de linguagem natural (PLN) e técnicas de aprendizado de máquina para atribuir pontuações ponderadas de sentimento às sentenças. Com os recentes avanços na aprendizagem de máquina, o uso de técnicas avançadas de inteligência artificial se tornou eficaz para identificar sentimentos de usuários na web.\n",
    "\n",
    "Quando uma empresa deseja entender o que estão falando sobre ela e qual a reputação de seus produtos online, uma das formas de se fazer isso é utilizando machine learning. Nesse sentido, uma das técnicas recomendadas é a análise de sentimentos, que consiste em extrair informações de textos a partir de linguagem natural.O objetivo dessa técnica é classificar sentenças, ou um conjunto de sentenças, como positivas ou negativas. Essa classificação é realizada automaticamente e extrai informações subjetivas de textos, criando conhecimento estruturado que pode ser utilizado por um sistema.\n",
    "\n",
    "Contudo, obter uma grande quantidade de dados em tempo real pode gerar um problema. A dificuldade em termos de capacidade computacional para processar todos estes dados em tempo real. Ao longo dos últimos anos, a principal ferramenta utilizada para processar grandes quantidade de dados em tempo real é o Apache Spark. Em linhas gerais, o Apache Spark é uma estrtutura de código aberto desenvolvida para ser um mecanismo de processamento analítico para aplicações de processamento de dados distribuídos em larga escala e aprendizado de máquina em tempo real, ou seja, para grandes volumes de dados, o chamado Big Data.\n",
    "\n",
    "### 1.2) Objetivos deste projeto\n",
    "Este projeto tem por objetivo, utilizar a API do Twitter para obter tweets em tempo real de um determinado assunto, aplicar técnicas de MapReduce, transformação de dados com o framework Spark e utilizar o MLlib para classificar tweets como sentimento positivo ou negativo.\n",
    "\n",
    "### 1.3) Metodologia\n",
    "O projeto foi dividido em duas principais etapas. A primeira parte do projeto compreende a obtenção de um dataset rotulado contendo sentenças e suas respectivas classificações: 1 ou 0. Sendo 1 para sentimento positivo e 0 para sentimento negativo. O dataset foi obtido do repositório de datasets Kaggle e se trata de um conjunto de dados fornecido por um colaborador da Universidade de Michigan. O dataset se chama <a target=\"_blank\" href=\"https://www.kaggle.com/datasets/seesea0203/umich-si650-nlp\">UMICH SI650 NLP</a>, contém 5662 registros e duas colunas. A primeira para o texto e a segunda para o sentimento.\n",
    "\n",
    "Após o carregamento dos dados foi realizado o pré-processamento dos dados. Esta é a principal atividade em um projeto baseado em dados. Os dados aqui se tratam de tweets reais e há vários tratamentos que devem ser realizados para garantir a legibilidade dos dados. Foi aplicado a remoção de caracteres especiais, remoção e de pontuação, remoção de stopwords (palavras irrelevantes para o sentido da frase) e a divisão dos dados em listas de palavras para se adequar ao formato de entrada que o modelo espera.\n",
    "\n",
    "Com os dados limpos e organizados os mesmos foram submetidos ao algoritmo de Machine Learning responsável por fazer a classicação dos tweets e gerar o modelo de classificação. \n",
    "\n",
    "Em seguida foi iniciado a segunda parte do projeto, a etapa de coleta dos dados em tempo real do Twitter, tratamento dos tweets e a classificação com o modelo gerado anteriormente. \n",
    "\n",
    "A primeira atividade para obter dados do Twitter via API é estabelecar conexão com as respecitivas autenticações: api key, api key secret, access token, access token secret e bearer token. Com as chaves de autenticação foi necessário estabelecer conexão, obter as regras de acesso, deletar as antigas regras de acesso, adicionar as novas regras de acesso, para então realizar a coleta de dados.\n",
    "\n",
    "Depois de pronto a conexão com a API do Twitter, os dados vêm são coletados por meio do streaming de dados do Spark (SparkStreaming) e salvos em objetos DSTreams. Os dados são então pré-processados e submetidos ao modelo para realizar a classificação. A coleta de dados termina após o tempo estabelicido pelo usuário ou então após a interrupção da conexão.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a792dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row #Converte RDDs em objetos do tipo Row\n",
    "from pyspark.sql.functions import col, isnan, when, count # Encontra a contagem para valores None, Null, Nan, etc.\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder #Converte strings em valores numéricos\n",
    "from pyspark.ml.linalg import Vectors #Serve para criar um vetor denso\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # Para avaliar o modelo com as métricas de avaliação.\n",
    "from pyspark.ml.feature import RobustScaler, StandardScaler, MinMaxScaler, Normalizer # Métodos para escalas dos dados\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier, LinearSVC # Algoritmos de ML\n",
    "from pyspark.ml import Pipeline # Criação de um Pipeline de execução.\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator # GridSearch e Validação Cruzada\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator # Evaluator para classificação binária\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab09b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos usados\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from operator import add\n",
    "import requests_oauthlib\n",
    "from time import gmtime, strftime\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import ast\n",
    "import json\n",
    "#import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bbf0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pacote NLTK\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequência de update\n",
    "INTERVALO_BATCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1652c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o StreamingContext\n",
    "ssc = StreamingContext(sc, INTERVALO_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00c341",
   "metadata": {},
   "source": [
    "## Treinando o Classificador de Análise de Sentimento\n",
    "\n",
    "Uma parte essencial da criação de um algoritmo de análise de sentimento (ou qualquer algoritmo de mineração de dados) é ter um conjunto de dados abrangente ou \"Corpus\" para o aprendizado, bem como um conjunto de dados de teste para garantir que a precisão do seu algoritmo atende aos padrões que você espera. Isso também permitirá que você ajuste o seu algoritmo a fim de deduzir melhores (ou mais precisas) características de linguagem natural que você poderia extrair do texto e que vão contribuir para a classificação de sentimento, em vez de usar uma abordagem genérica. Tomaremos como base o dataset de treino fornecido pela Universidade de Michigan, para competições do Kaggle (https://inclass.kaggle.com/c/si650winter11).\n",
    "\n",
    "\n",
    "Esse dataset contém 1,578,627 tweets classificados e cada linha é marcada como: \n",
    "\n",
    "#### 1 para o sentimento positivo \n",
    "#### 0 para o sentimento negativo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf91c9",
   "metadata": {},
   "source": [
    "### Obtendo dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session - usada quando se trabalha com Dataframes no Spark\n",
    "spSession = SparkSession.builder.master(\"local\").appName(\"tw-session\").config(\"spark.some.config.option\", \"session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d76a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o arquivo texto e criando um RDD em memória com Spark\n",
    "rdd = sc.textFile(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945bf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = rdd.first()\n",
    "rdd_body = rdd.filter(lambda x: header not in x)#.map(lambda l: l.split(','))\n",
    "\n",
    "list_columns = header.replace('.', '_').upper().split(',')\n",
    "list_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c56435",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_body = rdd_body.map(lambda x: x.replace(',0', ';0')).map(lambda x: x.replace(',1', ';1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_body.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e40716",
   "metadata": {},
   "source": [
    "### Pré Processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84698ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função separa as colunas em cada linha, cria uma tupla e remove a pontuação.\n",
    "def get_row(line):\n",
    "    row = line.split(';')\n",
    "    \n",
    "    tweet = row[0].strip()\n",
    "    sentimento = int(re.sub('[^\\d]+', '', row[1]))\n",
    "    \n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    #translator = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    #tweet = regex.sub('', tweet)\n",
    "    tweet = tweet.translate(translator)\n",
    "    tweet = tweet.split(' ')\n",
    "    tweet_lower = []\n",
    "    for word in tweet:\n",
    "        tweet_lower.append(word.lower())\n",
    "    return (tweet_lower, sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd057ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplcia a função a cada linha do dataset\n",
    "dataset_treino = rdd_body.map(lambda line: get_row(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bcd83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_treino.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b96088",
   "metadata": {},
   "source": [
    "### Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um objeto SentimentAnalyzer \n",
    "sentiment_analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f465a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém a lista de stopwords em Inglês \n",
    "stopwords_all = []\n",
    "for word in stopwords.words('english'):\n",
    "    stopwords_all.append(word)\n",
    "    stopwords_all.append(word + '_NEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém 10.000 tweets do dataset de treino e retorna todas as palavras que não são stopwords\n",
    "dataset_treino_amostra = dataset_treino.take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54294dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\n",
    "all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0843371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um unigram e extrai as features\n",
    "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops, top_n = 200)\n",
    "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
    "training_set = sentiment_analyzer.apply_features(dataset_treino_amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503fe9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e1175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testa o classificador em algumas sentenças\n",
    "test_sentence1 = [(['this', 'program', 'is', 'suck'], '')]\n",
    "test_sentence2 = [(['tough', 'day', 'at', 'work', 'today'], '')]\n",
    "test_sentence3 = [(['good', 'wonderful', 'amazing', 'awesome'], '')]\n",
    "test_set1 = sentiment_analyzer.apply_features(test_sentence1)\n",
    "test_set2 = sentiment_analyzer.apply_features(test_sentence2)\n",
    "test_set3 = sentiment_analyzer.apply_features(test_sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc980429",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify(test_set1[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c95c2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956bb0d0",
   "metadata": {},
   "source": [
    "## Coleta de dados do Twitter para Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8aaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import configparser\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import socket\n",
    "import json\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149d457",
   "metadata": {},
   "source": [
    "### Obtendo chaves de autentifação e Definindo regras de pesquisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c592e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autenticação do Twitter \n",
    "api_key = 'Olk6wAb5NqOnXztjIs9bR6FaF'\n",
    "api_key_secret = 'vkUwO1OGEjKH5CGlmLsKP1KBTYoESycOGopwAjVTlDqG98Ajzj'\n",
    "access_token = '1228500338912743425-fLQRIKtrHJe9mkvWM7EXToYmtfzzxc'\n",
    "access_token_secret = 'q1cx5rGXqmIqeWX6gBsMGG0ymGzYLTfAe4Eagxin9ohRh'\n",
    "bearer_token = r'AAAAAAAAAAAAAAAAAAAAAHY5hgEAAAAA2FSk9Qi3r1OKDdlXRzhkzox9InI%3DR3U3eL0OI5G8ka9GO1OlXtrZnvfuktbrEJozzVwPz1LssHYWcG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60692ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2FilteredStreamPython\"\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_rules():\n",
    "    print('Getting rules...')\n",
    "    response = requests.get(\"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth)\n",
    "    print(json.dumps(response.json()))\n",
    "    rules = response.json()\n",
    "    print(f'Status {response.status_code}')\n",
    "    \n",
    "    #########################################################################\n",
    "    print('\\nDeleting all rules...')\n",
    "    if rules is None or \"data\" not in rules:\n",
    "        return None\n",
    "\n",
    "    ids = list(map(lambda rule: rule[\"id\"], rules[\"data\"]))\n",
    "    payload = {\"delete\": {\"ids\": ids}}\n",
    "    response = requests.post(\"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth, json=payload)\n",
    "    \n",
    "    print(json.dumps(response.json()))\n",
    "    print(f'Status {response.status_code}')\n",
    "    \n",
    "    #########################################################################\n",
    "    print('\\nSetting rules...')\n",
    "    sample_rules = [{'value': 'russia'},{'value': 'war'},{'value': 'putin'}]\n",
    "\n",
    "    payload = {\"add\": sample_rules}\n",
    "    response = requests.post(\"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth, json=payload)\n",
    "    print(json.dumps(response.json()))\n",
    "    print(f'Status {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gainaing access and connecting to Twitter API using Credentials\n",
    "client = tweepy.Client(bearer_token, api_key, api_key_secret, access_token, access_token_secret)\n",
    "\n",
    "auth = tweepy.OAuth1UserHandler(api_key, api_key_secret, access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "get_set_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec890fed",
   "metadata": {},
   "source": [
    "### Pré processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ssc.queueStream([], default = rdd)\n",
    "type(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de tweets por update\n",
    "NUM_TWEETS = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função conecta ao Twitter e retorna um número específico de Tweets (NUM_TWEETS)\n",
    "def tfunc(t, rdd):\n",
    "    return rdd.flatMap(lambda x: stream_twitter_data())\n",
    "\n",
    "def stream_twitter_data():\n",
    "    response = requests.get(\"https://api.twitter.com/2/tweets/search/stream\", auth=bearer_oauth, stream=True)\n",
    "    print('Status code {}'.format(response.status_code))\n",
    "    count = 0\n",
    "    for line in response.iter_lines():\n",
    "        try:\n",
    "            if count > NUM_TWEETS:\n",
    "                break\n",
    "            post = json.loads(line.decode('utf-8'))\n",
    "            contents = [post['data']['text']]\n",
    "            count += 1\n",
    "            yield str(contents)\n",
    "        except:\n",
    "            result = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = stream.transform(tfunc)\n",
    "type(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_stream = stream.map(lambda line: ast.literal_eval(line))\n",
    "type(coord_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função classifica os tweets, aplicando as features do modelo criado anteriormente\n",
    "def classifica_tweet(tweet):\n",
    "    sentence = [(tweet, '')]\n",
    "    test_set = sentiment_analyzer.apply_features(sentence)\n",
    "    print(tweet, classifier.classify(test_set[0][0]))\n",
    "    return(tweet, classifier.classify(test_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função retorna o texto do Twitter\n",
    "def get_tweet_text(rdd):\n",
    "    for line in rdd:\n",
    "        tweet = line.strip()\n",
    "        translator = str.maketrans({key: None for key in string.punctuation})\n",
    "        tweet = tweet.translate(translator)\n",
    "        tweet = tweet.split(' ')\n",
    "        tweet_lower = []\n",
    "        for word in tweet:\n",
    "            tweet_lower.append(word.lower())\n",
    "    return(classifica_tweet(tweet_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista vazia para os resultados\n",
    "resultados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43018aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função salva o resultado dos batches de Tweets junto com o timestamp\n",
    "def output_rdd(rdd):\n",
    "    global resultados\n",
    "    pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))\n",
    "    counts = pairs.reduceByKey(add)\n",
    "    output = []\n",
    "    for count in counts.collect():\n",
    "        output.append(count)\n",
    "    result = [time.strftime(\"%I:%M:%S\"), output]\n",
    "    resultados.append(result)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função foreachRDD() aplica uma função a cada RDD to streaming de dados\n",
    "coord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930cc39",
   "metadata": {},
   "source": [
    "### Coleta e classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming\n",
    "ssc.start() # Start the computation\n",
    "ssc.awaitTerminationOrTimeout(120) # Wait for the computation to terminate\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = True\n",
    "while cont:\n",
    "    if len(resultados) > 5:\n",
    "        cont = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb99122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "636bf6cc",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4f8ab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b081a66ee97bd2b6a16f43955f1d810b7ea816d6eaeb65e157ef9e038445f0c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
